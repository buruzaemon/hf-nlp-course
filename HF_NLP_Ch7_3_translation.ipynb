{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8b812f56-f43f-4853-998d-05b0bed889ee",
   "metadata": {},
   "source": [
    "# NLP Course\n",
    "\n",
    "Please see the [Hugging Face NLP Course page](https://huggingface.co/learn/nlp-course/chapter0/1?fw=pt)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a30537a6-daae-4907-ac62-0f41a87f43be",
   "metadata": {},
   "source": [
    "## Translation\n",
    "\n",
    "### Try it out: extra credit work at end of this section\n",
    "\n",
    "Please see [Translation](https://huggingface.co/learn/nlp-course/chapter7/4?fw=pt#translation), 7. Main NLP Tasks, in the 🤗 NLP Course."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aec2d1e-aa38-4efa-ab47-3c61e73b7977",
   "metadata": {},
   "source": [
    "### Preparing the data\n",
    "\n",
    "> To fine-tune or train a translation model from scratch, we will need a dataset suitable for the task. As mentioned previously, we’ll use the [KDE4](https://huggingface.co/datasets/Helsinki-NLP/kde4) dataset in this section, but you can adapt the code to use your own data quite easily, as long as you have pairs of sentences in the two languages you want to translate from and into. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4bf3f8a9-5939-4679-a1ae-0b3f5e374986",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'translation'],\n",
       "        num_rows: 131429\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "raw_datasets = load_dataset(\"kde4\", lang1=\"en\", lang2=\"ja\")\n",
    "raw_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3c4a0a5f-df8a-4b1a-9342-de59b43b6bfa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'translation'],\n",
       "        num_rows: 118286\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'translation'],\n",
       "        num_rows: 13143\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_datasets = raw_datasets[\"train\"].train_test_split(train_size=0.9, seed=20)\n",
    "split_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45083339-cd1e-4237-8b59-9204b678cee6",
   "metadata": {},
   "source": [
    "> We can rename the \"test\" key to \"validation\" like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9bca04e6-675c-4dd8-b817-750b0898cd63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'translation'],\n",
       "        num_rows: 118286\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'translation'],\n",
       "        num_rows: 13143\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_datasets[\"validation\"] = split_datasets.pop(\"test\")\n",
    "split_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e693e6be-7bb3-4996-b382-b05ab4341a10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'en': 'Specifies whether to use a logarithmic instead of a linear gradient for the Kalzium Mass Gradient feature',\n",
       " 'ja': 'KalziumMassGradientType クラスに線形グラディエントでなはく対数グラディエントを使うかどうかを指定します。'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_datasets[\"train\"][99][\"translation\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7998688a-cfe7-450b-8a92-b9580692f548",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'translation_text': '持 っ て い る 類 , 細か な 糸 を 生 じ させ る なら ば ,'}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "model_checkpoint = \"Helsinki-NLP/opus-mt-en-jap\"\n",
    "#                                           ^^^\n",
    "\n",
    "translator = pipeline(\"translation\", model=model_checkpoint)\n",
    "translator(\"Default to expanded threads\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fd116e9a-75f1-4196-8c5a-85c029887e04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'translation_text': 'どう か , かぎ を 開 い て ほし い .'}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translator(\"Please press the Enter key\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cf35938a-37fe-4647-a684-9fd55575d5f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'en': \"Neighbors' Loved Radio\", 'ja': 'ご近所さんのお気に入りラジオ'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_datasets[\"train\"][172][\"translation\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d2586e9b-9c01-4ea3-9f05-a7b452b3ebb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'translation_text': '\" わたし たち の 王 クレセテ , すなわち , 今 に 至 る まで 出帆 し た . この 人 が あ る の は , 異な る こと で は な い \" と 言 っ て い た .'}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translator(\n",
    "    \"Unable to import %1 using the OFX importer plugin. This file is not the correct format.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2c75c09-0e94-4afe-9a21-d83ff8bb1fa7",
   "metadata": {},
   "source": [
    "#### Processing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b65d87b7-b4ec-46fa-9ede-55ca99ef1f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "#model_checkpoint = \"Helsinki-NLP/opus-mt-en-fr\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31fd9ee1-fe31-43f5-9f22-33d9e8431d32",
   "metadata": {},
   "source": [
    "> As we can see, the output contains the input IDs associated with the English sentence, while the IDs associated with the Japanese one are stored in the labels field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c86d4213-b31c-44e0-81d4-191c242a2edd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': '42455', 'translation': {'en': 'Unable to start backend.', 'ja': 'バックエンドを開始できません。'}}\n"
     ]
    }
   ],
   "source": [
    "idx = 13\n",
    "\n",
    "print(split_datasets[\"train\"][idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c5ccafeb-0593-4b81-b013-b3bf2a910344",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [11641, 2766, 14, 36214, 749, 1, 4, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1], 'labels': [7155, 2948, 1160, 15702, 5587, 98, 1138, 15796, 222, 336, 2842, 1, 0]}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_sentence = split_datasets[\"train\"][idx][\"translation\"][\"en\"]\n",
    "ja_sentence = split_datasets[\"train\"][idx][\"translation\"][\"ja\"]\n",
    "\n",
    "inputs = tokenizer(en_sentence, text_target=ja_sentence)\n",
    "inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8246027-8f50-44f3-b625-5d6350b17fb1",
   "metadata": {},
   "source": [
    "> If you forget to indicate that you are tokenizing labels, they will be tokenized by the input tokenizer, which in the case of a Marian model is not going to go well at all"
   ]
  },
  {
   "cell_type": "raw",
   "id": "899889a9-b716-405c-96b0-a76598e992f3",
   "metadata": {},
   "source": [
    "wrong_targets = tokenizer(ja_sentence)\n",
    "\n",
    "print(tokenizer.convert_ids_to_tokens(wrong_targets[\"input_ids\"]))\n",
    "print(tokenizer.convert_ids_to_tokens(inputs[\"labels\"]))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f71a62b9-0831-413b-b016-83fb638bde01",
   "metadata": {},
   "source": [
    "wrong_targets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "462f9080-710a-4aad-9497-6fef9a2e045f",
   "metadata": {},
   "source": [
    "##### NOTE `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` "
   ]
  },
  {
   "cell_type": "raw",
   "id": "1877c8cb-2ff5-4bc1-a981-3e06d56ca55d",
   "metadata": {},
   "source": [
    "with tokenizer.as_target_tokenizer():\n",
    "    good_targets = tokenizer(ja_sentence)\n",
    "\n",
    "print(tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"]))\n",
    "print(tokenizer.convert_ids_to_tokens(inputs[\"labels\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "455eb737-39ce-4414-bb82-25ab7e8f0a5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁Un', 'able', '▁to', '▁start', '▁back', '<unk>', '.', '</s>']\n",
      "['▁バ', 'ッ', 'ク', 'エン', 'ド', 'を', '開', '始', 'で', 'き', 'ません', '<unk>', '</s>']\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"]))\n",
    "print(tokenizer.convert_ids_to_tokens(inputs[\"labels\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2b10d831-e16c-4c3c-b088-981793decc6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 128\n",
    "\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    inputs = [ex[\"en\"] for ex in examples[\"translation\"]]\n",
    "    targets = [ex[\"ja\"] for ex in examples[\"translation\"]]\n",
    "    model_inputs = tokenizer(\n",
    "        inputs, text_target=targets, max_length=max_length, truncation=True\n",
    "    )\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8871dc79-83b3-4001-8fb1-e1900262cafc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 118286\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 13143\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets = split_datasets.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    remove_columns=split_datasets[\"train\"].column_names,\n",
    ")\n",
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25d19a99-16fd-4080-b46a-1687891448a0",
   "metadata": {},
   "source": [
    "### Fine-tuning the model with the Trainer API\n",
    "\n",
    "> The actual code using the Trainer will be the same as before, with just one little change: we use a [`Seq2SeqTrainer`](https://huggingface.co/docs/transformers/main_classes/trainer#transformers.Seq2SeqTrainer) here, which is a subclass of `Trainer` that will allow us to properly deal with the evaluation, using the `generate()` method to predict outputs from the inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ffca9497-c16e-4a64-ad11-8fbccb46422c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1832371a-33f3-4e46-94d7-10f097c58910",
   "metadata": {},
   "source": [
    "#### Data collation\n",
    "\n",
    "> This is all done by a [`DataCollatorForSeq2Seq`](https://huggingface.co/docs/transformers/main_classes/data_collator#transformers.DataCollatorForSeq2Seq). Like the `DataCollatorWithPadding`, it takes the tokenizer used to preprocess the inputs, but it also takes the model. This is because this data collator will also be responsible for preparing the decoder input IDs, which are shifted versions of the labels with a special token at the beginning. Since this shift is done slightly differently for different architectures, the `DataCollatorForSeq2Seq` needs to know the model object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3a043e98-4860-42d0-9a71-86c4484290dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForSeq2Seq\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9f6c7e18-9ee0-43da-89b1-c5ca7648a80a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'attention_mask', 'labels', 'decoder_input_ids'])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = data_collator([tokenized_datasets[\"train\"][i] for i in range(1, 3)])\n",
    "batch.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ba54f802-ca65-4cff-b70a-71c7e1290cd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[16465,  2156, 22699, 22784,  3407,  7323,     1, 32478,    31,  2315,\n",
       "         45185, 19121,    29, 20801,     0],\n",
       "        [45630,    76, 17084,     4,     4,     4,     0, 46275, 46275, 46275,\n",
       "         46275, 46275, 46275, 46275, 46275]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]]), 'labels': tensor([[    6,     1,     6,     1, 15508,  3983, 27591,  1696,  1852, 41294,\n",
       "           587,  3443,     1,     0],\n",
       "        [ 7155, 27591,  1781, 13128,  1060,     4,     4,     4,     0,  -100,\n",
       "          -100,  -100,  -100,  -100]]), 'decoder_input_ids': tensor([[46275,     6,     1,     6,     1, 15508,  3983, 27591,  1696,  1852,\n",
       "         41294,   587,  3443,     1],\n",
       "        [46275,  7155, 27591,  1781, 13128,  1060,     4,     4,     4,     0,\n",
       "         46275, 46275, 46275, 46275]])}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a7c0481a-d2eb-4320-b509-f415bc7b7485",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    6,     1,     6,     1, 15508,  3983, 27591,  1696,  1852, 41294,\n",
       "           587,  3443,     1,     0],\n",
       "        [ 7155, 27591,  1781, 13128,  1060,     4,     4,     4,     0,  -100,\n",
       "          -100,  -100,  -100,  -100]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[\"labels\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "164aa5ae-bedf-45d7-92ef-ccfa6c22c3aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[46275,     6,     1,     6,     1, 15508,  3983, 27591,  1696,  1852,\n",
       "         41294,   587,  3443,     1],\n",
       "        [46275,  7155, 27591,  1781, 13128,  1060,     4,     4,     4,     0,\n",
       "         46275, 46275, 46275, 46275]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[\"decoder_input_ids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "05d802c0-401e-466b-848f-a7442489b3c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6, 1, 6, 1, 15508, 3983, 27591, 1696, 1852, 41294, 587, 3443, 1, 0]\n",
      "[7155, 27591, 1781, 13128, 1060, 4, 4, 4, 0]\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, 3):\n",
    "    print(tokenized_datasets[\"train\"][i][\"labels\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3d4d0f8-2136-4edc-a5b5-e397d61d5b41",
   "metadata": {},
   "source": [
    "#### Metrics\n",
    "\n",
    "> One weakness with BLEU is that it expects the text to already be tokenized, which makes it difficult to compare scores between models that use different tokenizers. So instead, the most commonly used metric for benchmarking translation models today is SacreBLEU, which addresses this weakness (and others) by standardizing the tokenization step. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ca318da5-49fd-4ed9-be9a-4ce0d9075a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"sacrebleu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "812d38a5-7ff3-47c8-b272-74b6ffacc702",
   "metadata": {},
   "source": [
    "> The score can go from 0 to 100, and higher is better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d011e057-f8a6-4851-912b-75b0de713ef7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 46.750469682990165,\n",
       " 'counts': [11, 6, 4, 3],\n",
       " 'totals': [12, 11, 10, 9],\n",
       " 'precisions': [91.66666666666667,\n",
       "  54.54545454545455,\n",
       "  40.0,\n",
       "  33.333333333333336],\n",
       " 'bp': 0.9200444146293233,\n",
       " 'sys_len': 12,\n",
       " 'ref_len': 13}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = [\n",
    "    \"This plugin lets you translate web pages between several languages automatically.\"\n",
    "]\n",
    "\n",
    "references = [\n",
    "    [\n",
    "        \"This plugin allows you to automatically translate web pages between several languages.\"\n",
    "    ]\n",
    "]\n",
    "\n",
    "metric.compute(predictions=predictions, references=references)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "da4d14fc-6201-4976-953b-e59e352af7ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 1.683602693167689,\n",
       " 'counts': [1, 0, 0, 0],\n",
       " 'totals': [4, 3, 2, 1],\n",
       " 'precisions': [25.0, 16.666666666666668, 12.5, 12.5],\n",
       " 'bp': 0.10539922456186433,\n",
       " 'sys_len': 4,\n",
       " 'ref_len': 13}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = [\"This This This This\"]\n",
    "\n",
    "references = [\n",
    "    [\n",
    "        \"This plugin allows you to automatically translate web pages between several languages.\"\n",
    "    ]\n",
    "]\n",
    "\n",
    "metric.compute(predictions=predictions, references=references)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58729be9-17b4-451f-b87a-8b2bdeeb8422",
   "metadata": {},
   "source": [
    "> To get from the model outputs to texts the metric can use, we will use the [`tokenizer.batch_decode()`](https://huggingface.co/docs/transformers/v4.49.0/en/main_classes/tokenizer#transformers.PreTrainedTokenizer.batch_decode) method. We just have to clean up all the `-100`s in the labels (the tokenizer will automatically do the same for the padding token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2b90a2cb-686d-40e9-88c8-5ceb636e3cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    preds, labels = eval_preds\n",
    "    # In case the model returns more than the prediction logits\n",
    "    if isinstance(preds, tuple):\n",
    "        preds = preds[0]\n",
    "\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "\n",
    "    # Replace -100s in the labels as we can't decode them\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    # Some simple post-processing\n",
    "    decoded_preds = [pred.strip() for pred in decoded_preds]\n",
    "    decoded_labels = [[label.strip()] for label in decoded_labels]\n",
    "\n",
    "    result = metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "    return {\"bleu\": result[\"score\"]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80a42669-da83-419b-be62-a03a994a0a24",
   "metadata": {},
   "source": [
    "### Fine-tuning the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "64f11d37-ce61-4566-9d64-1ff5db620304",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c571ef5f9f604387a3b5e82b2fbce6ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9ae0bc7d-4bda-4606-a26a-83573b4a0ed9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'buruzaemon/marian-finetuned-kde4-en-to-ja-accelerate'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#from huggingface_hub import Repository, get_full_repo_name\n",
    "from huggingface_hub import get_full_repo_name\n",
    "\n",
    "model_name = \"marian-finetuned-kde4-en-to-ja-accelerate\"\n",
    "\n",
    "repo_name = get_full_repo_name(model_name)\n",
    "repo_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "97cf22af-ad72-4394-b646-16147dd5feb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Seq2SeqTrainingArguments\n",
    "\n",
    "args = Seq2SeqTrainingArguments(\n",
    "    model_name,\n",
    "    eval_strategy=\"no\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=64,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=3,\n",
    "    num_train_epochs=3,\n",
    "    predict_with_generate=True,\n",
    "    fp16=True,\n",
    "    push_to_hub=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2fd20388-0032-4613-bd62-8e56ff72a9fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Seq2SeqTrainer\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    processing_class=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0fc96e6e-618b-4d4e-9206-c141cc37c89e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='206' max='206' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [206/206 15:40]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "That's 100 lines that end in a tokenized period ('.')\n",
      "It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 18min 37s, sys: 3.92 s, total: 18min 41s\n",
      "Wall time: 17min 1s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 10.40902042388916,\n",
       " 'eval_model_preparation_time': 0.0038,\n",
       " 'eval_bleu': 0.00817271844352432,\n",
       " 'eval_runtime': 1021.0567,\n",
       " 'eval_samples_per_second': 12.872,\n",
       " 'eval_steps_per_second': 0.202}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "trainer.evaluate(max_length=max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "33ead6b1-d048-4a26-ba4d-978cdfb18f93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='11091' max='11091' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [11091/11091 27:32, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>4.448400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>3.249200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>2.917400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>2.696000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>2.555300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>2.445300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>2.356200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>2.278600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>2.175800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>2.114500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>2.083400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>2.061800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>2.015900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>2.008800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>1.965600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>1.904200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>1.908500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>1.882900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>1.877400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>1.859400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10500</td>\n",
       "      <td>1.857400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>1.842000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/learn-hf/lib/python3.9/site-packages/transformers/modeling_utils.py:2810: UserWarning: Moving the following attributes in the config to the generation config: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[46275]]}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 24min 58s, sys: 2min 32s, total: 27min 31s\n",
      "Wall time: 27min 35s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=11091, training_loss=2.2918752639848927, metrics={'train_runtime': 1652.9533, 'train_samples_per_second': 214.681, 'train_steps_per_second': 6.71, 'total_flos': 6336352287719424.0, 'train_loss': 2.2918752639848927, 'epoch': 3.0})"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e85ed520-05b6-4e13-a7da-8982f93e706c",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d5777af-21f0-48dd-ba2a-8653fd646ac6",
   "metadata": {},
   "source": [
    "### A custom training loop\n",
    "\n",
    ">  First we’ll build the [`DataLoader`](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader)s from our datasets, after setting the datasets to the \"torch\" format so we get PyTorch tensors..\n",
    "\n",
    "Please also see the API documentation page for [`torch.utils.data`](https://pytorch.org/docs/stable/data.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f689f6da-e634-4891-8ade-6a3205d6700f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "tokenized_datasets.set_format(\"torch\")\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    tokenized_datasets[\"train\"],\n",
    "    shuffle=True,\n",
    "    collate_fn=data_collator,\n",
    "    batch_size=8,\n",
    ")\n",
    "\n",
    "eval_dataloader = DataLoader(\n",
    "    tokenized_datasets[\"validation\"], \n",
    "    shuffle=True,    \n",
    "    collate_fn=data_collator, \n",
    "    batch_size=8\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f2de3e8-17af-43cc-9fcc-ef477d40a92f",
   "metadata": {},
   "source": [
    "> Next we reinstantiate our model, to make sure we're not continuing the fine-tuning from before but starting from the pretrained model again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "854b0e40-9e1a-49e9-a7b0-711043eee144",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7b3615f-b361-43b4-99a2-85104339d803",
   "metadata": {},
   "source": [
    "> Then we will need an optimizer...\n",
    "\n",
    "##### Question: why the [`transformers.AdamW`](https://huggingface.co/docs/transformers/v4.49.0/en/main_classes/optimizer_schedules#transformers.AdamW) instead of [`torch.optim.AdamW`](https://pytorch.org/docs/stable/generated/torch.optim.AdamW.html#torch.optim.AdamW) implementation??\n",
    "\n",
    "When attempting to use the `transformers.AdamW` optimizer, we see the following warning:\n",
    "\n",
    "    FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
    "\n",
    "... so, yeah, the recommended optimizer is from `torch.optim`."
   ]
  },
  {
   "cell_type": "raw",
   "id": "1d4ac2d7-43bb-4547-96de-2755b9857098",
   "metadata": {},
   "source": [
    "from transformers import AdamW\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2d0c7f71-230b-45e7-8c40-45dcdd4c4b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import AdamW\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2b8649d-c73e-419a-8a66-9b1e9d3d75e8",
   "metadata": {},
   "source": [
    "> Once we have all those objects, we can send them to the `accelerator.prepare()` method...\n",
    "\n",
    "##### TODO: check out that [Accelerate documentation](https://huggingface.co/docs/accelerate/index) and go through [that short tutorial](https://huggingface.co/docs/accelerate/basic_tutorials/overview)..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1fab4236-da5e-481c-8135-e24886b2f8c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from accelerate import Accelerator\n",
    "\n",
    "accelerator = Accelerator()\n",
    "\n",
    "model, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(\n",
    "    model, optimizer, train_dataloader, eval_dataloader\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecd64815-3b34-4da3-91a7-539c97bb1c36",
   "metadata": {},
   "source": [
    "> Now that we have sent our `train_dataloader` to `accelerator.prepare()`, we can use its length to compute the number of training steps. <span style=\"background-color:#33ffff\">Remember we should always do this after preparing the dataloader, as that method will change the length of the `DataLoader`</span>. We use a classic linear schedule from the learning rate to 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8b0d8d95-0f19-4492-904d-7cae5c806201",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import get_scheduler\n",
    "\n",
    "num_train_epochs = 3\n",
    "\n",
    "num_update_steps_per_epoch = len(train_dataloader)\n",
    "\n",
    "num_training_steps = num_train_epochs * num_update_steps_per_epoch\n",
    "\n",
    "lr_scheduler = get_scheduler(\n",
    "    \"linear\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=num_training_steps,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8a92fe9f-6a92-4e0e-9529-d62a0707e7f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def postprocess(predictions, labels):\n",
    "    predictions = predictions.cpu().numpy()\n",
    "    labels = labels.cpu().numpy()\n",
    "\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "\n",
    "    # Replace -100 in the labels as we can't decode them.\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    # Some simple post-processing\n",
    "    decoded_preds = [pred.strip() for pred in decoded_preds]\n",
    "    decoded_labels = [[label.strip()] for label in decoded_labels]\n",
    "    return decoded_preds, decoded_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "dda980df-a1a3-4910-a943-c3b6f0bf650b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import HfApi\n",
    "\n",
    "output_dir = model_name\n",
    "\n",
    "api = HfApi()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0822b13d-4c7e-41c2-86c3-0b162783ca19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59bd14b75ee84c7abe267057f9664853",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/44358 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/learn-hf/lib/python3.9/site-packages/transformers/data/data_collator.py:740: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /opt/conda/conda-bld/pytorch_1729647380992/work/torch/csrc/utils/tensor_new.cpp:278.)\n",
      "  batch[\"labels\"] = torch.tensor(batch[\"labels\"], dtype=torch.int64)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c835c9d7ef4443d8254aa4a6a106237",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1643 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, BLEU score: 14.03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/learn-hf/lib/python3.9/site-packages/transformers/modeling_utils.py:2810: UserWarning: Moving the following attributes in the config to the generation config: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[46275]]}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b64cb4eebe740f49689406487bc4f93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/272M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25fcb2fc47564e3c809c6000d74981cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1643 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, BLEU score: 17.28\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "963f3872b99b4d52896945d824aaf79f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/272M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55d2d012347344b59d06befc5e25fbd5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1643 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2, BLEU score: 18.62\n",
      "CPU times: user 2h 50min 58s, sys: 1min 21s, total: 2h 52min 19s\n",
      "Wall time: 2h 11min 42s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61a3ba32640743ccb1277046c4f2fe68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/272M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "import torch\n",
    "\n",
    "progress_bar = tqdm(range(num_training_steps))\n",
    "\n",
    "for epoch in range(num_train_epochs):\n",
    "    # Training\n",
    "    model.train()\n",
    "    for batch in train_dataloader:\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        accelerator.backward(loss)\n",
    "\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        progress_bar.update(1)\n",
    "\n",
    "    # Evaluation\n",
    "    model.eval()\n",
    "    for batch in tqdm(eval_dataloader):\n",
    "        with torch.no_grad():\n",
    "            generated_tokens = accelerator.unwrap_model(model).generate(\n",
    "                batch[\"input_ids\"],\n",
    "                attention_mask=batch[\"attention_mask\"],\n",
    "                max_length=128,\n",
    "            )\n",
    "        #print(f\"??? type(batch['labels']) is???: {type(batch['labels'])}\")\n",
    "        labels = batch[\"labels\"]\n",
    "\n",
    "        # Necessary to pad predictions and labels for being gathered\n",
    "        generated_tokens = accelerator.pad_across_processes(\n",
    "            generated_tokens, dim=1, pad_index=tokenizer.pad_token_id\n",
    "        )\n",
    "        labels = accelerator.pad_across_processes(labels, dim=1, pad_index=-100)\n",
    "\n",
    "        predictions_gathered = accelerator.gather(generated_tokens)\n",
    "        labels_gathered = accelerator.gather(labels)\n",
    "\n",
    "        decoded_preds, decoded_labels = postprocess(predictions_gathered, labels_gathered)\n",
    "        metric.add_batch(predictions=decoded_preds, references=decoded_labels)\n",
    "\n",
    "    results = metric.compute()\n",
    "    print(f\"epoch {epoch}, BLEU score: {results['score']:.2f}\")\n",
    "\n",
    "    # Save and upload\n",
    "    accelerator.wait_for_everyone()\n",
    "    unwrapped_model = accelerator.unwrap_model(model)\n",
    "    unwrapped_model.save_pretrained(output_dir, save_function=accelerator.save)\n",
    "    if accelerator.is_main_process:\n",
    "        tokenizer.save_pretrained(output_dir)\n",
    "        #repo.push_to_hub(\n",
    "        #    commit_message=f\"Training in progress epoch {epoch}\", blocking=False\n",
    "        #)\n",
    "        future = api.upload_folder( # Upload in the background (non-blocking action)\n",
    "            repo_id=repo_name,\n",
    "            folder_path=output_dir,\n",
    "            run_as_future=True,\n",
    "            commit_message=f\"Training in progress epoch {epoch}\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39673324-e4e3-493c-8162-bc9ff02235d4",
   "metadata": {},
   "source": [
    "#### Using the fine-tuned model\n",
    "\n",
    "Let's try using our newly fine-tuned translation model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "dde67b87-5784-4c57-84dd-d56f7dcbdc9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = \"Next, press the Enter key.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a63cb86e-0a76-4071-92ca-7826843ba243",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6aad0baa0444d3086478a9c99f6dc35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/272M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'translation_text': '次のキーを押してキーを押してください'}]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Replace this with your own checkpoint\n",
    "model_checkpoint = repo_name\n",
    "\n",
    "our_finetuned_translator = pipeline(\"translation\", model=model_checkpoint)\n",
    "our_finetuned_translator(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfdc25a8-9fc1-4787-a494-22a1ec3f0b3b",
   "metadata": {},
   "source": [
    "##### Now let's compare with the original model!\n",
    "\n",
    "Recall:\n",
    "* source languages: en\n",
    "* target languages: jap (sic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "aeaabc0e-7235-4d69-a010-1dd13f04095d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'translation_text': 'その 次 に , かぎ の 開き を する つもり で あ っ た .'}]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "the_original_translator = pipeline(\"translation\", model=\"Helsinki-NLP/opus-mt-en-jap\")\n",
    "the_original_translator(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa43d022-a5f4-4089-8574-7c9473948c55",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
