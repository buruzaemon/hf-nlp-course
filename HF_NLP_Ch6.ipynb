{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c523620f-85ce-4308-ac40-51d7790c44fe",
   "metadata": {},
   "source": [
    "# NLP Course\n",
    "\n",
    "Please see the [Hugging Face NLP Course page](https://huggingface.co/learn/nlp-course/chapter0/1?fw=pt)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25289ec0-bd3b-4928-9f5f-853a3273d8f8",
   "metadata": {},
   "source": [
    "## 6. The ðŸ¤— Tokenizers library"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2df83c6d-d216-462c-b0ee-e257a4dbbe9d",
   "metadata": {},
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54b8a564-4d1f-4466-9e94-1f0d45ef0421",
   "metadata": {},
   "source": [
    "#### Assembling a corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9dc3e72-41f3-4227-a8dd-18883ad254aa",
   "metadata": {},
   "source": [
    "\n",
    "> The repository for code_search_net contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/code_search_net.\n",
    "> You can avoid this prompt in future by passing the argument `trust_remote_code=True`.\n",
    ">\n",
    "> Do you wish to run the custom code? [y/N]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cac1ce98-cb82-4113-a964-7e8b8ca20427",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['repository_name', 'func_path_in_repository', 'func_name', 'whole_func_string', 'language', 'func_code_string', 'func_code_tokens', 'func_documentation_string', 'func_documentation_tokens', 'split_name', 'func_code_url'],\n",
       "        num_rows: 412178\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['repository_name', 'func_path_in_repository', 'func_name', 'whole_func_string', 'language', 'func_code_string', 'func_code_tokens', 'func_documentation_string', 'func_documentation_tokens', 'split_name', 'func_code_url'],\n",
       "        num_rows: 22176\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['repository_name', 'func_path_in_repository', 'func_name', 'whole_func_string', 'language', 'func_code_string', 'func_code_tokens', 'func_documentation_string', 'func_documentation_tokens', 'split_name', 'func_code_url'],\n",
       "        num_rows: 23107\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# This can take a few minutes to load, so grab a coffee or tea while you wait!\n",
    "raw_datasets = load_dataset(\"code_search_net\", \"python\", trust_remote_code=True)\n",
    "raw_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "040623d3-8a75-469a-80fd-3100e2e62f33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['repository_name', 'func_path_in_repository', 'func_name', 'whole_func_string', 'language', 'func_code_string', 'func_code_tokens', 'func_documentation_string', 'func_documentation_tokens', 'split_name', 'func_code_url'],\n",
       "    num_rows: 412178\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4d1576ce-9a29-4cf2-b5cd-6e6e5116c4e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def _compress_for_distribute(max_vol, plan, **kwargs):\n",
      "    \"\"\"\n",
      "    Combines as many dispenses as can fit within the maximum volume\n",
      "    \"\"\"\n",
      "    source = None\n",
      "    new_source = None\n",
      "    a_vol = 0\n",
      "    temp_dispenses = []\n",
      "    new_transfer_plan = []\n",
      "    disposal_vol = kwargs.get('disposal_vol', 0)\n",
      "    max_vol = max_vol - disposal_vol\n",
      "\n",
      "    def _append_dispenses():\n",
      "        nonlocal a_vol, temp_dispenses, new_transfer_plan, source\n",
      "        if not temp_dispenses:\n",
      "            return\n",
      "        added_volume = 0\n",
      "        if len(temp_dispenses) > 1:\n",
      "            added_volume = disposal_vol\n",
      "        new_transfer_plan.append({\n",
      "            'aspirate': {\n",
      "                'location': source,\n",
      "                'volume': a_vol + added_volume\n",
      "            }\n",
      "        })\n",
      "        for d in temp_dispenses:\n",
      "            new_transfer_plan.append({\n",
      "                'dispense': {\n",
      "                    'location': d['location'],\n",
      "                    'volume': d['volume']\n",
      "                }\n",
      "            })\n",
      "        a_vol = 0\n",
      "        temp_dispenses = []\n",
      "\n",
      "    for p in plan:\n",
      "        this_vol = p['aspirate']['volume']\n",
      "        new_source = p['aspirate']['location']\n",
      "        if (new_source is not source) or (this_vol + a_vol > max_vol):\n",
      "            _append_dispenses()\n",
      "        source = new_source\n",
      "        a_vol += this_vol\n",
      "        temp_dispenses.append(p['dispense'])\n",
      "    _append_dispenses()\n",
      "    return new_transfer_plan\n"
     ]
    }
   ],
   "source": [
    "print(raw_datasets[\"train\"][123456][\"whole_func_string\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "524a58f9-f8cc-4b41-961b-450771cb2ff2",
   "metadata": {},
   "source": [
    "> Using a Python generator, we can avoid Python loading anything into memory\n",
    "> until itâ€™s actually necessary. To create such a generator, \n",
    "> <span style=\"background-color:#33ffff;\">you just to need to replace the brackets with parentheses</span>"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b6e7b715-0908-47c5-aaca-d3fa7dc5fdaf",
   "metadata": {},
   "source": [
    "training_corpus = (\n",
    "    raw_datasets[\"train\"][i : i + 1000][\"whole_func_string\"]\n",
    "    for i in range(0, len(raw_datasets[\"train\"]), 1000)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bf03a776-d0e5-447a-86e9-435eebbe2047",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "gen = (i for i in range(10))\n",
    "print(list(gen))\n",
    "print(list(gen))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ab175cc9-8da9-4293-bfe8-2c4a02081f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_training_corpus():\n",
    "    return (\n",
    "        raw_datasets[\"train\"][i : i + 1000][\"whole_func_string\"]\n",
    "        for i in range(0, len(raw_datasets[\"train\"]), 1000)\n",
    "    )\n",
    "\n",
    "\n",
    "training_corpus = get_training_corpus()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ca8a732-a3ae-4d91-95a3-d4f0b5776989",
   "metadata": {},
   "source": [
    "> You can also define your generator inside a for loop by using the yield statement...\n",
    "> which will produce the exact same generator as before,\n",
    "> <span style=\"background-color:#33ffff\">but allows you to use more complex logic than you can in a list comprehension</span>"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8f6020b9-3911-41a8-829c-79e1b0ab977c",
   "metadata": {},
   "source": [
    "def get_training_corpus():\n",
    "    dataset = raw_datasets[\"train\"]\n",
    "    for start_idx in range(0, len(dataset), 1000):\n",
    "        samples = dataset[start_idx : start_idx + 1000]\n",
    "        yield samples[\"whole_func_string\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d0a9f49-6784-43f0-90dd-c8c25356f92d",
   "metadata": {},
   "source": [
    "#### Training a new tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "75bcf828-aa99-47c3-9220-28c0c0b4f4b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "old_tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5b7db948-dd60-4cd6-aa8f-758894aa3091",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transformers.models.gpt2.tokenization_gpt2_fast.GPT2TokenizerFast"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(old_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4c48827c-3465-474b-8bee-aadce1575252",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['def', 'Ä add', '_', 'n', 'umbers', '(', 'a', ',', 'Ä b', '):', 'ÄŠ', 'Ä ', 'Ä ', 'Ä ', 'Ä \"\"\"', 'Add', 'Ä the', 'Ä two', 'Ä numbers', 'Ä `', 'a', '`', 'Ä and', 'Ä `', 'b', '`', '.\"', '\"\"', 'ÄŠ', 'Ä ', 'Ä ', 'Ä ', 'Ä return', 'Ä a', 'Ä +', 'Ä b']\n"
     ]
    }
   ],
   "source": [
    "example = '''def add_numbers(a, b):\n",
    "    \"\"\"Add the two numbers `a` and `b`.\"\"\"\n",
    "    return a + b'''\n",
    "\n",
    "tokens = old_tokenizer.tokenize(example)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e7d46d15-4ca3-4db3-8912-3b693319b2fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50257"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "old_tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e180be73-7dce-4127-97be-3064b596b401",
   "metadata": {},
   "source": [
    "##### Please read\n",
    "\n",
    "> Note that AutoTokenizer.train_new_from_iterator() only works if the tokenizer you are using is a â€œfastâ€ tokenizer. \n",
    "\n",
    "* [API documentation for `Tokenizer.train_new_from_iterator`](https://huggingface.co/docs/transformers/main_classes/tokenizer#transformers.PreTrainedTokenizerFast.train_new_from_iterator)\n",
    "* StackOverflow question from alvas: [How to add new tokens to an existing Huggingface AutoTokenizer?](https://stackoverflow.com/questions/76198051/how-to-add-new-tokens-to-an-existing-huggingface-tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c4638eda-4135-42a4-b423-86a8787067ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "CPU times: user 5min 53s, sys: 9.82 s, total: 6min 2s\n",
      "Wall time: 1min 40s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "tokenizer = old_tokenizer.train_new_from_iterator(training_corpus, 52000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "485c52cc-2ec9-46a4-8d24-d3562d4d5467",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['def', 'Ä add', '_', 'numbers', '(', 'a', ',', 'Ä b', '):', 'ÄŠÄ Ä Ä ', 'Ä \"\"\"', 'Add', 'Ä the', 'Ä two', 'Ä numbers', 'Ä `', 'a', '`', 'Ä and', 'Ä `', 'b', '`.\"\"\"', 'ÄŠÄ Ä Ä ', 'Ä return', 'Ä a', 'Ä +', 'Ä b']\n"
     ]
    }
   ],
   "source": [
    "tokens = tokenizer.tokenize(example)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c0df9b1a-00dc-43d6-8df4-390d8f0a17ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27\n",
      "36\n"
     ]
    }
   ],
   "source": [
    "print(len(tokens))\n",
    "print(len(old_tokenizer.tokenize(example)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a64ca226-26a2-4cce-8569-dbbf488e03f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['class', 'Ä Linear', 'Layer', '():', 'ÄŠÄ Ä Ä ', 'Ä def', 'Ä __', 'init', '__(', 'self', ',', 'Ä input', '_', 'size', ',', 'Ä output', '_', 'size', '):', 'ÄŠÄ Ä Ä Ä Ä Ä Ä ', 'Ä self', '.', 'weight', 'Ä =', 'Ä torch', '.', 'randn', '(', 'input', '_', 'size', ',', 'Ä output', '_', 'size', ')', 'ÄŠÄ Ä Ä Ä Ä Ä Ä ', 'Ä self', '.', 'bias', 'Ä =', 'Ä torch', '.', 'zeros', '(', 'output', '_', 'size', ')', 'ÄŠÄŠÄ Ä Ä ', 'Ä def', 'Ä __', 'call', '__(', 'self', ',', 'Ä x', '):', 'ÄŠÄ Ä Ä Ä Ä Ä Ä ', 'Ä return', 'Ä x', 'Ä @', 'Ä self', '.', 'weights', 'Ä +', 'Ä self', '.', 'bias', 'ÄŠÄ Ä Ä Ä ']\n"
     ]
    }
   ],
   "source": [
    "example = \"\"\"class LinearLayer():\n",
    "    def __init__(self, input_size, output_size):\n",
    "        self.weight = torch.randn(input_size, output_size)\n",
    "        self.bias = torch.zeros(output_size)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        return x @ self.weights + self.bias\n",
    "    \"\"\"\n",
    "\n",
    "print(tokenizer.tokenize(example))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fca62b65-b3d3-45aa-ba26-eaa170e7b7ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('code-search-net-tokenizer/tokenizer_config.json',\n",
       " 'code-search-net-tokenizer/special_tokens_map.json',\n",
       " 'code-search-net-tokenizer/vocab.json',\n",
       " 'code-search-net-tokenizer/merges.txt',\n",
       " 'code-search-net-tokenizer/added_tokens.json',\n",
       " 'code-search-net-tokenizer/tokenizer.json')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.save_pretrained(\"code-search-net-tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1637c5d1-1f19-4e4c-8b0a-413c959cb5e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 4864\n",
      "drwxr-xr-x 2 se_olliphant se_olliphant    4096 Feb  8 06:02 .\n",
      "drwxr-xr-x 5 se_olliphant se_olliphant    4096 Feb 11 07:30 ..\n",
      "-rw-r--r-- 1 se_olliphant se_olliphant  466894 Feb 11 07:30 merges.txt\n",
      "-rw-r--r-- 1 se_olliphant se_olliphant      99 Feb 11 07:30 special_tokens_map.json\n",
      "-rw-r--r-- 1 se_olliphant se_olliphant 3673415 Feb 11 07:30 tokenizer.json\n",
      "-rw-r--r-- 1 se_olliphant se_olliphant     471 Feb 11 07:30 tokenizer_config.json\n",
      "-rw-r--r-- 1 se_olliphant se_olliphant  822037 Feb 11 07:30 vocab.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!ls -la code-search-net-tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d52ae06-4e12-4654-8f0c-d7210cc52f24",
   "metadata": {},
   "source": [
    ",,,"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a189ce53-25fe-4031-b251-0a76ba2d48f3",
   "metadata": {},
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3be0ddc9-9ec1-47c2-99fe-727cc1a56c55",
   "metadata": {},
   "source": [
    "huggingface-cli login"
   ]
  },
  {
   "cell_type": "raw",
   "id": "29a2ba8d-fe25-4240-b4d5-5aaa10e74b6a",
   "metadata": {},
   "source": [
    "tokenizer.push_to_hub(\"code-search-net-tokenizer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fa5dbfa-895f-42f5-9761-4ea63746b8da",
   "metadata": {},
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "86b5fa11-84ce-4d18-88be-96db6f4efec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace \"huggingface-course\" below with your actual namespace to use your own tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"code-search-net-tokenizer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33479199-7a38-46c6-ae12-64ae0c0774ab",
   "metadata": {},
   "source": [
    "#### Fast tokenizersâ€™ special powers\n",
    "\n",
    "> <span style=\"background-color:#33FFFF\"><b>Slow</b> tokenizers</span> are those written in Python inside the ðŸ¤— Transformers library<p/>\n",
    "> <span style=\"background-color:#AFFF33\">the <b>fast</b> versions</span> are the ones provided by ðŸ¤— Tokenizers, which are written in Rust"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cb0ee55a-2785-4348-8f50-68f6842e4889",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizer.is_fast ? True\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "print(f\"tokenizer.is_fast ? {tokenizer.is_fast}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2db3532d-f6f3-413c-a821-8f6d9bcd2f40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "example: My name is Sylvain and I work at Hugging Face in Brooklyn.\n",
      "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
      "\n",
      "encoding.is_fast ? True\n",
      "\n",
      "tokens:\n",
      "['[CLS]', 'My', 'name', 'is', 'S', '##yl', '##va', '##in', 'and', 'I', 'work', 'at', 'Hu', '##gging', 'Face', 'in', 'Brooklyn', '.', '[SEP]']\n",
      "\n",
      "word IDs:\n",
      "[None, 0, 1, 2, 3, 3, 3, 3, 4, 5, 6, 7, 8, 8, 9, 10, 11, 12, None]\n"
     ]
    }
   ],
   "source": [
    "example = \"My name is Sylvain and I work at Hugging Face in Brooklyn.\"\n",
    "encoding = tokenizer(example)\n",
    "\n",
    "print(f\"example: {example}\")\n",
    "print(type(encoding))\n",
    "print()\n",
    "\n",
    "print(f\"encoding.is_fast ? {encoding.is_fast}\\n\")\n",
    "print(f\"tokens:\\n{encoding.tokens()}\\n\")\n",
    "print(f\"word IDs:\\n{encoding.word_ids()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2562c499-2209-4785-bd08-52557ca92a2d",
   "metadata": {},
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "781ef697-25a4-4628-b107-0d1286d03a86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert-base-cased\n",
      "tokens: ['[CLS]', '81', '##s', '[SEP]']\n",
      "word IDs: [None, 0, 0, None]\n",
      "\n",
      "roberta-based\n",
      "tokens: ['<s>', '81', 's', '</s>']\n",
      "word IDs: [None, 0, 1, None]\n"
     ]
    }
   ],
   "source": [
    "tokenizer_roberta = AutoTokenizer.from_pretrained(\"roberta-base\")\n",
    "\n",
    "check_this = \"81s\"\n",
    "\n",
    "encoding_bert = tokenizer(check_this)\n",
    "encoding_roberta = tokenizer_roberta(check_this)\n",
    "\n",
    "print(\"bert-base-cased\")\n",
    "print(f\"tokens: {encoding_bert.tokens()}\")\n",
    "print(f\"word IDs: {encoding_bert.word_ids()}\")\n",
    "\n",
    "print()\n",
    "\n",
    "print(\"roberta-based\")\n",
    "print(f\"tokens: {encoding_roberta.tokens()}\")\n",
    "print(f\"word IDs: {encoding_roberta.word_ids()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecb32f52-b784-4b08-aeb1-4c096e7d9bf8",
   "metadata": {},
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0847302d-992c-4c5f-aea0-9b7e307df0c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Sylvain'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start, end = encoding.word_to_chars(3)\n",
    "example[start:end]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c3f03b4-4490-4e4d-9196-d9286a89667c",
   "metadata": {},
   "source": [
    "...\n",
    "\n",
    "##### There is no `sentence_ids` method on the return encodings from `tokenizer(input)`!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c0191773-3e2d-4b06-9a01-1c66707ad454",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_1 = \"I am the son and heir of a shyness that is criminally vulgar. I am the son and the heir of nothing in particular. Really, I am.\"\n",
    "sentence_2 = \"If it's not love, then it's the Bomb that'll bring us together.\"\n",
    "\n",
    "encoding_sentence_1 = tokenizer(sentence_1)\n",
    "encoding_sentence_2 = tokenizer(sentence_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "db3e047e-18c6-4d25-9eed-9be67bfa4680",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['_MutableMapping__marker', '__abstractmethods__', '__class__', '__class_getitem__', '__contains__', '__copy__', '__delattr__', '__delitem__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattr__', '__getattribute__', '__getitem__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__ior__', '__iter__', '__le__', '__len__', '__lt__', '__module__', '__ne__', '__new__', '__or__', '__reduce__', '__reduce_ex__', '__repr__', '__reversed__', '__ror__', '__setattr__', '__setitem__', '__setstate__', '__sizeof__', '__slots__', '__str__', '__subclasshook__', '__weakref__', '_abc_impl', '_encodings', '_n_sequences', 'char_to_token', 'char_to_word', 'clear', 'convert_to_tensors', 'copy', 'data', 'encodings', 'fromkeys', 'get', 'is_fast', 'items', 'keys', 'n_sequences', 'pop', 'popitem', 'sequence_ids', 'setdefault', 'to', 'token_to_chars', 'token_to_sequence', 'token_to_word', 'tokens', 'update', 'values', 'word_ids', 'word_to_chars', 'word_to_tokens', 'words']\n"
     ]
    }
   ],
   "source": [
    "print(dir(encoding_sentence_1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7d102f9-f791-43b5-add9-664812b8e3bc",
   "metadata": {},
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b76a6083-1f39-4f10-8192-2c255b4c7f23",
   "metadata": {},
   "source": [
    "#### Inside the token-classification pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5c8e339-6c78-4838-a379-cf294bf5f017",
   "metadata": {},
   "source": [
    "> First, letâ€™s grab a token classification pipeline so we can get some results to compare manually. The model used by default is [`dbmdz/bert-large-cased-finetuned-conll03-english`](https://huggingface.co/dbmdz/bert-large-cased-finetuned-conll03-english); it performs NER on sentences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "408c43b6-9a6f-4a3b-982b-d6048f39cfbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to dbmdz/bert-large-cased-finetuned-conll03-english and revision 4c53496 (https://huggingface.co/dbmdz/bert-large-cased-finetuned-conll03-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cuda:0\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'entity': 'I-PER', 'score': 0.99938285, 'index': 4, 'word': 'S', 'start': 11, 'end': 12}\n",
      "{'entity': 'I-PER', 'score': 0.99815494, 'index': 5, 'word': '##yl', 'start': 12, 'end': 14}\n",
      "{'entity': 'I-PER', 'score': 0.9959072, 'index': 6, 'word': '##va', 'start': 14, 'end': 16}\n",
      "{'entity': 'I-PER', 'score': 0.99923277, 'index': 7, 'word': '##in', 'start': 16, 'end': 18}\n",
      "{'entity': 'I-ORG', 'score': 0.9738931, 'index': 12, 'word': 'Hu', 'start': 33, 'end': 35}\n",
      "{'entity': 'I-ORG', 'score': 0.976115, 'index': 13, 'word': '##gging', 'start': 35, 'end': 40}\n",
      "{'entity': 'I-ORG', 'score': 0.9887976, 'index': 14, 'word': 'Face', 'start': 41, 'end': 45}\n",
      "{'entity': 'I-LOC', 'score': 0.9932106, 'index': 16, 'word': 'Brooklyn', 'start': 49, 'end': 57}\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "token_classifier = pipeline(\"token-classification\")\n",
    "\n",
    "for ntt in token_classifier(\"My name is Sylvain and I work at Hugging Face in Brooklyn.\"):\n",
    "    print(ntt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9730ec2e-5944-4dc9-8a14-8acb8ac85578",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to dbmdz/bert-large-cased-finetuned-conll03-english and revision 4c53496 (https://huggingface.co/dbmdz/bert-large-cased-finetuned-conll03-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'entity_group': 'PER', 'score': 0.9981694, 'word': 'Sylvain', 'start': 11, 'end': 18}\n",
      "{'entity_group': 'ORG', 'score': 0.9796019, 'word': 'Hugging Face', 'start': 33, 'end': 45}\n",
      "{'entity_group': 'LOC', 'score': 0.9932106, 'word': 'Brooklyn', 'start': 49, 'end': 57}\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "token_classifier = pipeline(\"token-classification\", aggregation_strategy=\"simple\")\n",
    "#                                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
    "\n",
    "for ntt in token_classifier(\"My name is Sylvain and I work at Hugging Face in Brooklyn.\"):\n",
    "    print(ntt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72f4369e-82af-4ab1-92ca-ff633523d0a2",
   "metadata": {},
   "source": [
    "##### From inputs to predictions\n",
    "\n",
    "... let's try doing the same w/out using `pipeline`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e860e1a2-53f6-41e5-8c7b-1149c9085253",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertForTokenClassification(\n",
      "  (bert): BertModel(\n",
      "    (embeddings): BertEmbeddings(\n",
      "      (word_embeddings): Embedding(28996, 1024, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 1024)\n",
      "      (token_type_embeddings): Embedding(2, 1024)\n",
      "      (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): BertEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0-23): 24 x BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSdpaSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (classifier): Linear(in_features=1024, out_features=9, bias=True)\n",
      ")\n",
      "\n",
      "BertConfig {\n",
      "  \"_attn_implementation_autoset\": true,\n",
      "  \"_name_or_path\": \"dbmdz/bert-large-cased-finetuned-conll03-english\",\n",
      "  \"_num_labels\": 9,\n",
      "  \"architectures\": [\n",
      "    \"BertForTokenClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"O\",\n",
      "    \"1\": \"B-MISC\",\n",
      "    \"2\": \"I-MISC\",\n",
      "    \"3\": \"B-PER\",\n",
      "    \"4\": \"I-PER\",\n",
      "    \"5\": \"B-ORG\",\n",
      "    \"6\": \"I-ORG\",\n",
      "    \"7\": \"B-LOC\",\n",
      "    \"8\": \"I-LOC\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"label2id\": {\n",
      "    \"B-LOC\": 7,\n",
      "    \"B-MISC\": 1,\n",
      "    \"B-ORG\": 5,\n",
      "    \"B-PER\": 3,\n",
      "    \"I-LOC\": 8,\n",
      "    \"I-MISC\": 2,\n",
      "    \"I-ORG\": 6,\n",
      "    \"I-PER\": 4,\n",
      "    \"O\": 0\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.48.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "\n",
    "model_checkpoint = \"dbmdz/bert-large-cased-finetuned-conll03-english\"\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_checkpoint)\n",
    "\n",
    "print(model)\n",
    "print()\n",
    "print(model.config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9cf86219-d32d-4e88-b93e-00267ae1a268",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizer.is_fast ? True\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "print(f\"tokenizer.is_fast ? {tokenizer.is_fast}\")\n",
    "\n",
    "example = \"My name is Sylvain and I work at Hugging Face in Brooklyn.\"\n",
    "inputs = tokenizer(example, return_tensors=\"pt\")\n",
    "\n",
    "outputs = model(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5e2e16c8-0e0d-4666-815d-002889152485",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 19])\n",
      "torch.Size([1, 19, 9])\n"
     ]
    }
   ],
   "source": [
    "print(inputs[\"input_ids\"].shape)\n",
    "print(outputs.logits.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe59385e-123f-402c-b50c-ca525b93d88a",
   "metadata": {},
   "source": [
    "##### NOTE\n",
    "\n",
    "* 1st `dim` is batch (index)\n",
    "* 2nd `dim` is sequence (length)\n",
    "* 3rd `dim` is logits (labels)\n",
    "\n",
    "We use Torch's [`torch.nn.functional.softmax`](https://pytorch.org/docs/stable/generated/torch.nn.functional.softmax.html#torch-nn-functional-softmax) to convert the logits to probabilities, and then `argmax` to get the final NER prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ca32bdfb-1f26-440c-8b1c-f5a13091ef72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "probabilities = torch.nn.functional.softmax(outputs.logits, dim=-1)[0].tolist()\n",
    "predictions = outputs.logits.argmax(dim=-1)[0].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d0418e8c-a7cd-4b92-bcfa-2d28bad34d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_labels = [\n",
    "    model.config.id2label[p]\n",
    "    for p in predictions\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0813560c-31dc-42f2-9f78-ea6d6006003d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] 0 O\n",
      "My 0 O\n",
      "name 0 O\n",
      "is 0 O\n",
      "S 4 I-PER\n",
      "##yl 4 I-PER\n",
      "##va 4 I-PER\n",
      "##in 4 I-PER\n",
      "and 0 O\n",
      "I 0 O\n",
      "work 0 O\n",
      "at 0 O\n",
      "Hu 6 I-ORG\n",
      "##gging 6 I-ORG\n",
      "Face 6 I-ORG\n",
      "in 0 O\n",
      "Brooklyn 8 I-LOC\n",
      ". 0 O\n",
      "[SEP] 0 O\n"
     ]
    }
   ],
   "source": [
    "for tok, pred, label in zip(inputs.tokens(), predictions, predictions_labels):\n",
    "    print(tok, pred, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "18bbfb4e-178c-4a4a-8b61-8a24886e39f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'entity': 'I-PER', 'score': 0.9993828535079956, 'word': 'S'},\n",
       " {'entity': 'I-PER', 'score': 0.9981548190116882, 'word': '##yl'},\n",
       " {'entity': 'I-PER', 'score': 0.995907187461853, 'word': '##va'},\n",
       " {'entity': 'I-PER', 'score': 0.9992327690124512, 'word': '##in'},\n",
       " {'entity': 'I-ORG', 'score': 0.9738931059837341, 'word': 'Hu'},\n",
       " {'entity': 'I-ORG', 'score': 0.9761149883270264, 'word': '##gging'},\n",
       " {'entity': 'I-ORG', 'score': 0.9887974858283997, 'word': 'Face'},\n",
       " {'entity': 'I-LOC', 'score': 0.99321049451828, 'word': 'Brooklyn'}]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = []\n",
    "tokens = inputs.tokens()\n",
    "\n",
    "for idx, pred in enumerate(predictions):\n",
    "    label = model.config.id2label[pred]\n",
    "    if label != \"O\":\n",
    "        results.append(\n",
    "            {\"entity\": label, \"score\": probabilities[idx][pred], \"word\": tokens[idx]}\n",
    "        )\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fb57ebb5-8c11-4e2b-835a-a592279c4251",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 0),\n",
       " (0, 2),\n",
       " (3, 7),\n",
       " (8, 10),\n",
       " (11, 12),\n",
       " (12, 14),\n",
       " (14, 16),\n",
       " (16, 18),\n",
       " (19, 22),\n",
       " (23, 24),\n",
       " (25, 29),\n",
       " (30, 32),\n",
       " (33, 35),\n",
       " (35, 40),\n",
       " (41, 45),\n",
       " (46, 48),\n",
       " (49, 57),\n",
       " (57, 58),\n",
       " (0, 0)]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs_with_offsets = tokenizer(example, return_offsets_mapping=True)\n",
    "#                                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
    "\n",
    "inputs_with_offsets[\"offset_mapping\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "82c83857-3a28-44b8-afcf-7c9ccf6e2afd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'yl'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example[12:14]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f942f430-dea5-4d61-a457-effffbfc5429",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'entity': 'I-PER',\n",
       "  'score': 0.9993828535079956,\n",
       "  'word': 'S',\n",
       "  'start': 11,\n",
       "  'end': 12},\n",
       " {'entity': 'I-PER',\n",
       "  'score': 0.9981548190116882,\n",
       "  'word': '##yl',\n",
       "  'start': 12,\n",
       "  'end': 14},\n",
       " {'entity': 'I-PER',\n",
       "  'score': 0.995907187461853,\n",
       "  'word': '##va',\n",
       "  'start': 14,\n",
       "  'end': 16},\n",
       " {'entity': 'I-PER',\n",
       "  'score': 0.9992327690124512,\n",
       "  'word': '##in',\n",
       "  'start': 16,\n",
       "  'end': 18},\n",
       " {'entity': 'I-ORG',\n",
       "  'score': 0.9738931059837341,\n",
       "  'word': 'Hu',\n",
       "  'start': 33,\n",
       "  'end': 35},\n",
       " {'entity': 'I-ORG',\n",
       "  'score': 0.9761149883270264,\n",
       "  'word': '##gging',\n",
       "  'start': 35,\n",
       "  'end': 40},\n",
       " {'entity': 'I-ORG',\n",
       "  'score': 0.9887974858283997,\n",
       "  'word': 'Face',\n",
       "  'start': 41,\n",
       "  'end': 45},\n",
       " {'entity': 'I-LOC',\n",
       "  'score': 0.99321049451828,\n",
       "  'word': 'Brooklyn',\n",
       "  'start': 49,\n",
       "  'end': 57}]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = []\n",
    "inputs_with_offsets = tokenizer(example, return_offsets_mapping=True)\n",
    "tokens = inputs_with_offsets.tokens()\n",
    "offsets = inputs_with_offsets[\"offset_mapping\"]\n",
    "\n",
    "for idx, pred in enumerate(predictions):\n",
    "    label = model.config.id2label[pred]\n",
    "    if label != \"O\":\n",
    "        start, end = offsets[idx]\n",
    "        results.append(\n",
    "            {\n",
    "                \"entity\": label,\n",
    "                \"score\": probabilities[idx][pred],\n",
    "                \"word\": tokens[idx],\n",
    "                \"start\": start,\n",
    "                \"end\": end,\n",
    "            }\n",
    "        )\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d67c4de8-41c3-48cc-abed-ef7f95f05630",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hugging Face'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example[33:45]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6c6269e3-67a8-4294-9ec0-02c93c6e1fdf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'entity_group': 'PER',\n",
       "  'score': 0.998169407248497,\n",
       "  'word': 'Sylvain',\n",
       "  'start': 11,\n",
       "  'end': 18},\n",
       " {'entity_group': 'ORG',\n",
       "  'score': 0.9796018600463867,\n",
       "  'word': 'Hugging Face',\n",
       "  'start': 33,\n",
       "  'end': 45},\n",
       " {'entity_group': 'LOC',\n",
       "  'score': 0.99321049451828,\n",
       "  'word': 'Brooklyn',\n",
       "  'start': 49,\n",
       "  'end': 57}]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "results = []\n",
    "inputs_with_offsets = tokenizer(example, return_offsets_mapping=True)\n",
    "tokens = inputs_with_offsets.tokens()\n",
    "offsets = inputs_with_offsets[\"offset_mapping\"]\n",
    "\n",
    "idx = 0\n",
    "while idx < len(predictions):\n",
    "    pred = predictions[idx]\n",
    "    label = model.config.id2label[pred]\n",
    "    if label != \"O\":\n",
    "        # Remove the B- or I-\n",
    "        label = label[2:]\n",
    "        start, _ = offsets[idx]\n",
    "\n",
    "        # Grab all the tokens labeled with I-label\n",
    "        all_scores = []\n",
    "        while (\n",
    "            idx < len(predictions)\n",
    "            and model.config.id2label[predictions[idx]] == f\"I-{label}\"\n",
    "        ):\n",
    "            all_scores.append(probabilities[idx][pred])\n",
    "            _, end = offsets[idx]\n",
    "            idx += 1\n",
    "\n",
    "        # The score is the mean of all the scores of the tokens in that grouped entity\n",
    "        score = np.mean(all_scores).item()\n",
    "        word = example[start:end]\n",
    "        results.append(\n",
    "            {\n",
    "                \"entity_group\": label,\n",
    "                \"score\": score,\n",
    "                \"word\": word,\n",
    "                \"start\": start,\n",
    "                \"end\": end,\n",
    "            }\n",
    "        )\n",
    "    idx += 1\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2934e703-dbda-4004-8174-e21391171910",
   "metadata": {},
   "source": [
    "#### Fast tokenizers in the QA pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a8c21ef-27ea-48f0-97be-11bee9ef0c32",
   "metadata": {},
   "source": [
    "##### QA using `pipeline`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4816434c-3f22-4632-ac23-c7b901cd7c0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-cased-distilled-squad and revision 564e9b5 (https://huggingface.co/distilbert/distilbert-base-cased-distilled-squad).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Device set to use cuda:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'score': 0.9802603125572205,\n",
       " 'start': 78,\n",
       " 'end': 106,\n",
       " 'answer': 'Jax, PyTorch, and TensorFlow'}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "question = \"Which deep learning libraries back ðŸ¤— Transformers?\"\n",
    "\n",
    "context = \"\"\"\n",
    "ðŸ¤— Transformers is backed by the three most popular deep learning libraries â€” Jax, PyTorch, and TensorFlow â€” with a seamless integration\n",
    "between them. It's straightforward to train your models with one before loading them for inference with the other.\n",
    "\"\"\"\n",
    "\n",
    "question_answerer = pipeline(\"question-answering\")\n",
    "question_answerer(question=question, context=context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4fb942c5-56c5-4777-8e2d-22818a40b520",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 0.9714871048927307,\n",
       " 'start': 1892,\n",
       " 'end': 1919,\n",
       " 'answer': 'Jax, PyTorch and TensorFlow'}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "long_context = \"\"\"\n",
    "ðŸ¤— Transformers: State of the Art NLP\n",
    "\n",
    "ðŸ¤— Transformers provides thousands of pretrained models to perform tasks on texts such as classification, information extraction,\n",
    "question answering, summarization, translation, text generation and more in over 100 languages.\n",
    "Its aim is to make cutting-edge NLP easier to use for everyone.\n",
    "\n",
    "ðŸ¤— Transformers provides APIs to quickly download and use those pretrained models on a given text, fine-tune them on your own datasets and\n",
    "then share them with the community on our model hub. At the same time, each python module defining an architecture is fully standalone and\n",
    "can be modified to enable quick research experiments.\n",
    "\n",
    "Why should I use transformers?\n",
    "\n",
    "1. Easy-to-use state-of-the-art models:\n",
    "  - High performance on NLU and NLG tasks.\n",
    "  - Low barrier to entry for educators and practitioners.\n",
    "  - Few user-facing abstractions with just three classes to learn.\n",
    "  - A unified API for using all our pretrained models.\n",
    "  - Lower compute costs, smaller carbon footprint:\n",
    "\n",
    "2. Researchers can share trained models instead of always retraining.\n",
    "  - Practitioners can reduce compute time and production costs.\n",
    "  - Dozens of architectures with over 10,000 pretrained models, some in more than 100 languages.\n",
    "\n",
    "3. Choose the right framework for every part of a model's lifetime:\n",
    "  - Train state-of-the-art models in 3 lines of code.\n",
    "  - Move a single model between TF2.0/PyTorch frameworks at will.\n",
    "  - Seamlessly pick the right framework for training, evaluation and production.\n",
    "\n",
    "4. Easily customize a model or an example to your needs:\n",
    "  - We provide examples for each architecture to reproduce the results published by its original authors.\n",
    "  - Model internals are exposed as consistently as possible.\n",
    "  - Model files can be used independently of the library for quick experiments.\n",
    "\n",
    "ðŸ¤— Transformers is backed by the three most popular deep learning libraries â€” Jax, PyTorch and TensorFlow â€” with a seamless integration\n",
    "between them. It's straightforward to train your models with one before loading them for inference with the other.\n",
    "\"\"\"\n",
    "#                                                                              ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
    "#\n",
    "# ... see now how the answer is almost at the very end of this\n",
    "#     long, long context ...?\n",
    "\n",
    "question_answerer(question=question, context=long_context)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5546be38-2017-408c-a0c2-7f5e000f496a",
   "metadata": {},
   "source": [
    "##### Using a model for question answering\n",
    "\n",
    "... where we do things the hard way.\n",
    "\n",
    "> The checkpoint used by default for the question-answering pipeline is [`distilbert-base-cased-distilled-squad`](https://huggingface.co/distilbert/distilbert-base-cased-distilled-squad)\n",
    "\n",
    "> Models for question answering work a little differently from the models weâ€™ve seen up to now. Using the picture above as an example, the model has been trained to predict the index of the token starting the answer (here 21) and the index of the token where the answer ends (here 24). This is why those models donâ€™t return one tensor of logits but two: one for the logits corresponding to the start token of the answer, and one for the logits corresponding to the end token of the answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7643bc7f-2bb5-48c6-b7bd-9aef3c8eff99",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
    "\n",
    "model_checkpoint = \"distilbert-base-cased-distilled-squad\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(model_checkpoint)\n",
    "\n",
    "inputs = tokenizer(question, context, return_tensors=\"pt\")\n",
    "outputs = model(**inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9222955-cd95-4e09-97e4-89349088f074",
   "metadata": {},
   "source": [
    "##### For QA, we have logits on both the start of span with the answer; and the end of the span with the answer.\n",
    "\n",
    "... or we need to use the `[CLS]` token for indicating an impossible answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2458a90e-dda1-4890-8df5-31cce96bf91c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num. of tokens? 67\n",
      "torch.Size([1, 67]) torch.Size([1, 67])\n"
     ]
    }
   ],
   "source": [
    "start_logits = outputs.start_logits\n",
    "end_logits = outputs.end_logits\n",
    "\n",
    "print(f\"num. of tokens? {len(inputs.tokens())}\")\n",
    "print(start_logits.shape, end_logits.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f77d7298-25e8-401d-8ab2-c23dc1f2a812",
   "metadata": {},
   "source": [
    "> To convert those logits into probabilities, we will apply a softmax function â€” but before that, we need to make sure we mask the indices that are not part of the context. Our input is `[CLS]` question [SEP] context `[SEP]`, so we need to mask the tokens of the question as well as the `[SEP]` token. Weâ€™ll keep the `[CLS]` token, however, as some models use it to indicate that the answer is not in the context.\n",
    "\n",
    "##### To clarify\n",
    "\n",
    "* We want to calculate the probabilities for the `start` and `end` tokens using <u>only the context</u> and not the question.\n",
    "* We do that by setting the probabilities on the tokens in the question, as well as the `[SEP]` BERT tokens to 0\n",
    "* Some models use `[CLS]` for indicating an impossible answer (answer could not be found in the context), so we need to allow that through for calculating probabilities.\n",
    "* So, we need to create a mask of `1`s (or `True` on the positions for the tokens in the question and those `[SEP]` tokens\n",
    "* We can then set the logits on those tokens to some large, negative numbers, since $Softmax(x_{i}) = \\frac{exp(x_{i})}{\\sum_j{exp(x_{j})}}$ and the exponent of a large, negative number $x_{i}$ yields 0.\n",
    "\n",
    "We will use the `sequence_ids` values for building up a mask which we will use for setting the probabilities for the tokens in the question (and BERT special tokens) to 0. \n",
    "\n",
    "`sequence_ids` values are:\n",
    "* `None` for `[CLS]`, `[SEP]` special BERT tokens\n",
    "* `0` for the question\n",
    "* `1` for the context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8b543449-cad4-4b34-8558-ee04aeb4043f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('[CLS]', None),\n",
       " ('Which', 0),\n",
       " ('deep', 0),\n",
       " ('learning', 0),\n",
       " ('libraries', 0),\n",
       " ('back', 0),\n",
       " ('[UNK]', 0),\n",
       " ('Transformers', 0),\n",
       " ('?', 0),\n",
       " ('[SEP]', None),\n",
       " ('[UNK]', 1),\n",
       " ('Transformers', 1),\n",
       " ('is', 1),\n",
       " ('backed', 1),\n",
       " ('by', 1),\n",
       " ('the', 1),\n",
       " ('three', 1),\n",
       " ('most', 1),\n",
       " ('popular', 1),\n",
       " ('deep', 1),\n",
       " ('learning', 1),\n",
       " ('libraries', 1),\n",
       " ('â€”', 1),\n",
       " ('Jax', 1),\n",
       " (',', 1),\n",
       " ('P', 1),\n",
       " ('##y', 1),\n",
       " ('##T', 1),\n",
       " ('##or', 1),\n",
       " ('##ch', 1),\n",
       " (',', 1),\n",
       " ('and', 1),\n",
       " ('Ten', 1),\n",
       " ('##sor', 1),\n",
       " ('##F', 1),\n",
       " ('##low', 1),\n",
       " ('â€”', 1),\n",
       " ('with', 1),\n",
       " ('a', 1),\n",
       " ('sea', 1),\n",
       " ('##m', 1),\n",
       " ('##less', 1),\n",
       " ('integration', 1),\n",
       " ('between', 1),\n",
       " ('them', 1),\n",
       " ('.', 1),\n",
       " ('It', 1),\n",
       " (\"'\", 1),\n",
       " ('s', 1),\n",
       " ('straightforward', 1),\n",
       " ('to', 1),\n",
       " ('train', 1),\n",
       " ('your', 1),\n",
       " ('models', 1),\n",
       " ('with', 1),\n",
       " ('one', 1),\n",
       " ('before', 1),\n",
       " ('loading', 1),\n",
       " ('them', 1),\n",
       " ('for', 1),\n",
       " ('in', 1),\n",
       " ('##ference', 1),\n",
       " ('with', 1),\n",
       " ('the', 1),\n",
       " ('other', 1),\n",
       " ('.', 1),\n",
       " ('[SEP]', None)]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "sequence_ids = inputs.sequence_ids()\n",
    "#print(inputs.sequence_ids())\n",
    "#print()\n",
    "\n",
    "list(zip(inputs.tokens(), sequence_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f79fef03-e631-4a07-802a-5d5de11a4d3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 67])\n"
     ]
    }
   ],
   "source": [
    "# Mask indicating the positions of the question tokens and [SEP]\n",
    "mask = [i != 1 for i in sequence_ids]\n",
    "#print(mask)\n",
    "\n",
    "# Unmask the [CLS] token\n",
    "mask[0] = False\n",
    "#print(mask)\n",
    "#print()\n",
    "\n",
    "# N.B., the mask needs to be the same shape\n",
    "#       as the tensor of output logits\n",
    "#mask = torch.tensor(mask)[None]\n",
    "mask = torch.tensor(mask).unsqueeze(dim=0)\n",
    "#mask = torch.tensor(mask)\n",
    "print(mask.shape)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a47513c5-ade2-4145-87d0-5c07b1615fda",
   "metadata": {},
   "source": [
    "torch.tensor(mask).unsqueeze(dim=0)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "51850f6b-8971-43b4-822c-22de44c0b7fc",
   "metadata": {},
   "source": [
    "torch.tensor(mask)[None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4330b19e-8017-4950-a73a-bb4f77470ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_logits[mask] = -10000\n",
    "end_logits[mask] = -10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "de4cbc53-75f7-427e-bb26-cdf61406f276",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_probabilities = torch.nn.functional.softmax(start_logits, dim=-1)[0]\n",
    "end_probabilities = torch.nn.functional.softmax(end_logits, dim=-1)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4aca574-b6f2-4051-bf4a-e043e4802b9b",
   "metadata": {},
   "source": [
    "##### Create a matrix of scores\n",
    "\n",
    "`start_probabilities[:, None] * end_probabilities[None, :]`, or `unsqueeze`ing the tensors will create a $n \\times n$ tensor (matrix) of scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4a40c7bc-1a79-4e78-a58f-7108507be524",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([67, 67])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = start_probabilities[:, None] * end_probabilities[None, :]\n",
    "scores.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63650919-ad4f-4484-8ceb-02765cf5d759",
   "metadata": {},
   "source": [
    "[API documentation for `torch.triu`](https://pytorch.org/docs/stable/generated/torch.triu.html#torch-triu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "5ab36b83-d3fe-4331-a459-bc5f65f3e50f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[9.4340e-13, 0.0000e+00, 0.0000e+00,  ..., 1.1023e-12, 1.6345e-12,\n",
       "         0.0000e+00],\n",
       "        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00],\n",
       "        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00],\n",
       "        ...,\n",
       "        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 9.1136e-14, 1.3514e-13,\n",
       "         0.0000e+00],\n",
       "        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 1.2744e-13,\n",
       "         0.0000e+00],\n",
       "        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00]], grad_fn=<TriuBackward0>)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = torch.triu(scores)\n",
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4941d823-905a-4cc6-9508-4747adfa4c95",
   "metadata": {},
   "source": [
    "> Now we just have to get the index of the maximum. Since PyTorch will return the index in the flattened tensor, we need to use the floor division // and modulus % operations to get the `start_index` and `end_index`\n",
    "\n",
    "##### Clarification: `//` and `%` operations to get start and end???\n",
    "\n",
    "That explanation above is somewhat lacking...\n",
    "\n",
    "From ['s response to Get indices of the max of a 2D Tensor question at discuss.pytorch.org](https://discuss.pytorch.org/t/get-indices-of-the-max-of-a-2d-tensor/82150/5):\n",
    "\n",
    "> Since `argmax()` gives you the index in a flattened tensor, you can infer the position in your 2D tensor from size of the last dimension.\n",
    "> E.g. if `argmax()` returns 10 and youâ€™ve got 4 columns, you know itâ€™s on row 2, column 2.\n",
    "> You can use Python's (built-in function) [`divmod`](https://docs.python.org/3/library/functions.html#divmod) for this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f6b365b5-63e5-44b1-9fe8-a13e880b1045",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'int'>\n",
      "start_index: 23\n",
      "end_index: 35\n",
      "tensor(0.9803, grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "max_index = scores.argmax().item()\n",
    "print(type(max_index))\n",
    "\n",
    "start_index = max_index // scores.shape[1]\n",
    "end_index   = max_index %  scores.shape[1]\n",
    "#start_index, end_index = divmod(max_index, scores.shape[1])\n",
    "\n",
    "print(f\"start_index: {start_index}\")\n",
    "print(f\"end_index: {end_index}\")\n",
    "\n",
    "print(scores[start_index, end_index])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f4e3c30-7adf-4a5e-8c17-20ab0431be79",
   "metadata": {},
   "source": [
    "##### Slight detour for extra credit\n",
    "\n",
    "> âœï¸ Try it out! Compute the start and end indices for the five most likely answers.\n",
    "\n",
    "Please see API documentation for [`torch.topk`](https://pytorch.org/docs/stable/generated/torch.Tensor.topk.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "9aff1238-f059-4280-bac6-8c6ef3d13cd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(23, 35), (23, 36), (16, 35), (23, 29), (25, 35)]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "values, indices = torch.topk(scores.flatten(), 5)\n",
    "start_and_end = [\n",
    "    divmod(max_index, scores.shape[1])\n",
    "    for max_index in indices.tolist()\n",
    "]\n",
    "start_and_end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01c0d426-5204-4c84-9d4b-31707299302b",
   "metadata": {},
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "4d852658-4739-44ef-8865-729cd47fc255",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [101, 5979, 1996, 3776, 9818, 1171, 100, 25267, 136, 102, 100, 25267, 1110, 5534, 1118, 1103, 1210, 1211, 1927, 1996, 3776, 9818, 783, 13612, 117, 153, 1183, 1942, 1766, 1732, 117, 1105, 5157, 21484, 2271, 6737, 783, 1114, 170, 2343, 1306, 2008, 9111, 1206, 1172, 119, 1135, 112, 188, 21546, 1106, 2669, 1240, 3584, 1114, 1141, 1196, 10745, 1172, 1111, 1107, 16792, 1114, 1103, 1168, 119, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'offset_mapping': [(0, 0), (0, 5), (6, 10), (11, 19), (20, 29), (30, 34), (35, 36), (37, 49), (49, 50), (0, 0), (1, 2), (3, 15), (16, 18), (19, 25), (26, 28), (29, 32), (33, 38), (39, 43), (44, 51), (52, 56), (57, 65), (66, 75), (76, 77), (78, 81), (81, 82), (83, 84), (84, 85), (85, 86), (86, 88), (88, 90), (90, 91), (92, 95), (96, 99), (99, 102), (102, 103), (103, 106), (107, 108), (109, 113), (114, 115), (116, 119), (119, 120), (120, 124), (125, 136), (137, 144), (145, 149), (149, 150), (151, 153), (153, 154), (154, 155), (156, 171), (172, 174), (175, 180), (181, 185), (186, 192), (193, 197), (198, 201), (202, 208), (209, 216), (217, 221), (222, 225), (226, 228), (228, 235), (236, 240), (241, 244), (245, 250), (250, 251), (0, 0)]}\n"
     ]
    }
   ],
   "source": [
    "inputs_with_offsets = tokenizer(question, context, return_offsets_mapping=True)\n",
    "print(inputs_with_offsets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "fbf17ec3-4784-4fe1-808e-789836fe0812",
   "metadata": {},
   "outputs": [],
   "source": [
    "offsets = inputs_with_offsets[\"offset_mapping\"]\n",
    "\n",
    "start_char, _ = offsets[start_index]\n",
    "_, end_char = offsets[end_index]\n",
    "answer = context[start_char:end_char]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a8f594d3-ade4-491a-ba84-dcb9ad01a144",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'answer': 'Jax, PyTorch, and TensorFlow',\n",
       " 'start': 78,\n",
       " 'end': 106,\n",
       " 'score': tensor(0.9803, grad_fn=<SelectBackward0>)}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = {\n",
    "    \"answer\": answer,\n",
    "    \"start\": start_char,\n",
    "    \"end\": end_char,\n",
    "    \"score\": scores[start_index, end_index],\n",
    "}\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddd628ea-2821-48c1-8e6e-2c8c5c0ddefc",
   "metadata": {},
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a3228b7a-9d31-4e4d-8d50-9039b93f95a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 0.9802603125572205,\n",
       "  'start': 78,\n",
       "  'end': 106,\n",
       "  'answer': 'Jax, PyTorch, and TensorFlow'},\n",
       " {'score': 0.008247793652117252,\n",
       "  'start': 78,\n",
       "  'end': 108,\n",
       "  'answer': 'Jax, PyTorch, and TensorFlow â€”'},\n",
       " {'score': 0.0013676970265805721,\n",
       "  'start': 78,\n",
       "  'end': 90,\n",
       "  'answer': 'Jax, PyTorch'},\n",
       " {'score': 0.00038108686567284167,\n",
       "  'start': 83,\n",
       "  'end': 106,\n",
       "  'answer': 'PyTorch, and TensorFlow'},\n",
       " {'score': 0.00021684505918528885,\n",
       "  'start': 96,\n",
       "  'end': 106,\n",
       "  'answer': 'TensorFlow'}]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question_answerer(question=question, context=context, top_k=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c826c9ae-2a48-416b-84f7-ce4a5eba53c3",
   "metadata": {},
   "source": [
    "#### Handling long contexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59750c48-b0a4-40db-adc9-f63aca18b544",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "918e9513-07fc-4c13-8ffa-d5168685e65a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "641af323-dfa9-413a-bf26-3ce9a4e2235b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d36dc830-dc70-495b-bb3c-04a6b920b7c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b07212-ef28-404b-9975-0721c52276b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9176ce96-7587-43f3-9c92-bb7759e4ec14",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d168ce9-51cf-4f8c-ad06-e3a3c244abf3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
