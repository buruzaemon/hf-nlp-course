{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c523620f-85ce-4308-ac40-51d7790c44fe",
   "metadata": {},
   "source": [
    "# NLP Course\n",
    "\n",
    "Please see the [Hugging Face NLP Course page](https://huggingface.co/learn/nlp-course/chapter0/1?fw=pt)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25289ec0-bd3b-4928-9f5f-853a3273d8f8",
   "metadata": {},
   "source": [
    "## 6. The ðŸ¤— Tokenizers library"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2df83c6d-d216-462c-b0ee-e257a4dbbe9d",
   "metadata": {},
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54b8a564-4d1f-4466-9e94-1f0d45ef0421",
   "metadata": {},
   "source": [
    "#### Assembling a corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9dc3e72-41f3-4227-a8dd-18883ad254aa",
   "metadata": {},
   "source": [
    "\n",
    "> The repository for code_search_net contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/code_search_net.\n",
    "> You can avoid this prompt in future by passing the argument `trust_remote_code=True`.\n",
    ">\n",
    "> Do you wish to run the custom code? [y/N]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cac1ce98-cb82-4113-a964-7e8b8ca20427",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45467157b2ae487c8ff08ed4425c478a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/12.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f3f7f9744fc439a865d707da4dc7a2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "code_search_net.py:   0%|          | 0.00/8.44k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c55fd6326a994d90aac39a0717df253c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "python.zip:   0%|          | 0.00/941M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6e231cbeaf444ecabaffc1d4c35d205",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/412178 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef2861dbefb347c083302533c17da814",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/22176 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "109ea3edc6b44a02a0f25c2513b6c6e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/23107 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['repository_name', 'func_path_in_repository', 'func_name', 'whole_func_string', 'language', 'func_code_string', 'func_code_tokens', 'func_documentation_string', 'func_documentation_tokens', 'split_name', 'func_code_url'],\n",
       "        num_rows: 412178\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['repository_name', 'func_path_in_repository', 'func_name', 'whole_func_string', 'language', 'func_code_string', 'func_code_tokens', 'func_documentation_string', 'func_documentation_tokens', 'split_name', 'func_code_url'],\n",
       "        num_rows: 22176\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['repository_name', 'func_path_in_repository', 'func_name', 'whole_func_string', 'language', 'func_code_string', 'func_code_tokens', 'func_documentation_string', 'func_documentation_tokens', 'split_name', 'func_code_url'],\n",
       "        num_rows: 23107\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# This can take a few minutes to load, so grab a coffee or tea while you wait!\n",
    "raw_datasets = load_dataset(\"code_search_net\", \"python\", trust_remote_code=True)\n",
    "raw_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "040623d3-8a75-469a-80fd-3100e2e62f33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['repository_name', 'func_path_in_repository', 'func_name', 'whole_func_string', 'language', 'func_code_string', 'func_code_tokens', 'func_documentation_string', 'func_documentation_tokens', 'split_name', 'func_code_url'],\n",
       "    num_rows: 412178\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4d1576ce-9a29-4cf2-b5cd-6e6e5116c4e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def _compress_for_distribute(max_vol, plan, **kwargs):\n",
      "    \"\"\"\n",
      "    Combines as many dispenses as can fit within the maximum volume\n",
      "    \"\"\"\n",
      "    source = None\n",
      "    new_source = None\n",
      "    a_vol = 0\n",
      "    temp_dispenses = []\n",
      "    new_transfer_plan = []\n",
      "    disposal_vol = kwargs.get('disposal_vol', 0)\n",
      "    max_vol = max_vol - disposal_vol\n",
      "\n",
      "    def _append_dispenses():\n",
      "        nonlocal a_vol, temp_dispenses, new_transfer_plan, source\n",
      "        if not temp_dispenses:\n",
      "            return\n",
      "        added_volume = 0\n",
      "        if len(temp_dispenses) > 1:\n",
      "            added_volume = disposal_vol\n",
      "        new_transfer_plan.append({\n",
      "            'aspirate': {\n",
      "                'location': source,\n",
      "                'volume': a_vol + added_volume\n",
      "            }\n",
      "        })\n",
      "        for d in temp_dispenses:\n",
      "            new_transfer_plan.append({\n",
      "                'dispense': {\n",
      "                    'location': d['location'],\n",
      "                    'volume': d['volume']\n",
      "                }\n",
      "            })\n",
      "        a_vol = 0\n",
      "        temp_dispenses = []\n",
      "\n",
      "    for p in plan:\n",
      "        this_vol = p['aspirate']['volume']\n",
      "        new_source = p['aspirate']['location']\n",
      "        if (new_source is not source) or (this_vol + a_vol > max_vol):\n",
      "            _append_dispenses()\n",
      "        source = new_source\n",
      "        a_vol += this_vol\n",
      "        temp_dispenses.append(p['dispense'])\n",
      "    _append_dispenses()\n",
      "    return new_transfer_plan\n"
     ]
    }
   ],
   "source": [
    "print(raw_datasets[\"train\"][123456][\"whole_func_string\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "524a58f9-f8cc-4b41-961b-450771cb2ff2",
   "metadata": {},
   "source": [
    "> Using a Python generator, we can avoid Python loading anything into memory\n",
    "> until itâ€™s actually necessary. To create such a generator, \n",
    "> <span style=\"background-color:#33ffff;\">you just to need to replace the brackets with parentheses</span>"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b6e7b715-0908-47c5-aaca-d3fa7dc5fdaf",
   "metadata": {},
   "source": [
    "training_corpus = (\n",
    "    raw_datasets[\"train\"][i : i + 1000][\"whole_func_string\"]\n",
    "    for i in range(0, len(raw_datasets[\"train\"]), 1000)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bf03a776-d0e5-447a-86e9-435eebbe2047",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "gen = (i for i in range(10))\n",
    "print(list(gen))\n",
    "print(list(gen))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ab175cc9-8da9-4293-bfe8-2c4a02081f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_training_corpus():\n",
    "    return (\n",
    "        raw_datasets[\"train\"][i : i + 1000][\"whole_func_string\"]\n",
    "        for i in range(0, len(raw_datasets[\"train\"]), 1000)\n",
    "    )\n",
    "\n",
    "\n",
    "training_corpus = get_training_corpus()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ca8a732-a3ae-4d91-95a3-d4f0b5776989",
   "metadata": {},
   "source": [
    "> You can also define your generator inside a for loop by using the yield statement...\n",
    "> which will produce the exact same generator as before,\n",
    "> <span style=\"background-color:#33ffff\">but allows you to use more complex logic than you can in a list comprehension</span>"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8f6020b9-3911-41a8-829c-79e1b0ab977c",
   "metadata": {},
   "source": [
    "def get_training_corpus():\n",
    "    dataset = raw_datasets[\"train\"]\n",
    "    for start_idx in range(0, len(dataset), 1000):\n",
    "        samples = dataset[start_idx : start_idx + 1000]\n",
    "        yield samples[\"whole_func_string\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d0a9f49-6784-43f0-90dd-c8c25356f92d",
   "metadata": {},
   "source": [
    "#### Training a new tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "75bcf828-aa99-47c3-9220-28c0c0b4f4b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f4450952b774efa9627056c723ae568",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0658e0a2707145beb85747fb4e5c5e88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4354af75b66041528825f6f9e09deb6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f594277aea7a4487998461acbc98aafc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ddd7a7671a049deb14aaa8a629ae3d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "old_tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5b7db948-dd60-4cd6-aa8f-758894aa3091",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transformers.models.gpt2.tokenization_gpt2_fast.GPT2TokenizerFast"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(old_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4c48827c-3465-474b-8bee-aadce1575252",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['def', 'Ä add', '_', 'n', 'umbers', '(', 'a', ',', 'Ä b', '):', 'ÄŠ', 'Ä ', 'Ä ', 'Ä ', 'Ä \"\"\"', 'Add', 'Ä the', 'Ä two', 'Ä numbers', 'Ä `', 'a', '`', 'Ä and', 'Ä `', 'b', '`', '.\"', '\"\"', 'ÄŠ', 'Ä ', 'Ä ', 'Ä ', 'Ä return', 'Ä a', 'Ä +', 'Ä b']\n"
     ]
    }
   ],
   "source": [
    "example = '''def add_numbers(a, b):\n",
    "    \"\"\"Add the two numbers `a` and `b`.\"\"\"\n",
    "    return a + b'''\n",
    "\n",
    "tokens = old_tokenizer.tokenize(example)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e7d46d15-4ca3-4db3-8912-3b693319b2fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50257"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "old_tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e180be73-7dce-4127-97be-3064b596b401",
   "metadata": {},
   "source": [
    "##### Please read\n",
    "\n",
    "> Note that AutoTokenizer.train_new_from_iterator() only works if the tokenizer you are using is a â€œfastâ€ tokenizer. \n",
    "\n",
    "* [API documentation for `Tokenizer.train_new_from_iterator`](https://huggingface.co/docs/transformers/main_classes/tokenizer#transformers.PreTrainedTokenizerFast.train_new_from_iterator)\n",
    "* StackOverflow question from alvas: [How to add new tokens to an existing Huggingface AutoTokenizer?](https://stackoverflow.com/questions/76198051/how-to-add-new-tokens-to-an-existing-huggingface-tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c4638eda-4135-42a4-b423-86a8787067ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "CPU times: user 6min 8s, sys: 10.7 s, total: 6min 19s\n",
      "Wall time: 1min 42s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "tokenizer = old_tokenizer.train_new_from_iterator(training_corpus, 52000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "485c52cc-2ec9-46a4-8d24-d3562d4d5467",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['def', 'Ä add', '_', 'numbers', '(', 'a', ',', 'Ä b', '):', 'ÄŠÄ Ä Ä ', 'Ä \"\"\"', 'Add', 'Ä the', 'Ä two', 'Ä numbers', 'Ä `', 'a', '`', 'Ä and', 'Ä `', 'b', '`.\"\"\"', 'ÄŠÄ Ä Ä ', 'Ä return', 'Ä a', 'Ä +', 'Ä b']\n"
     ]
    }
   ],
   "source": [
    "tokens = tokenizer.tokenize(example)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c0df9b1a-00dc-43d6-8df4-390d8f0a17ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27\n",
      "36\n"
     ]
    }
   ],
   "source": [
    "print(len(tokens))\n",
    "print(len(old_tokenizer.tokenize(example)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a64ca226-26a2-4cce-8569-dbbf488e03f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['class', 'Ä Linear', 'Layer', '():', 'ÄŠÄ Ä Ä ', 'Ä def', 'Ä __', 'init', '__(', 'self', ',', 'Ä input', '_', 'size', ',', 'Ä output', '_', 'size', '):', 'ÄŠÄ Ä Ä Ä Ä Ä Ä ', 'Ä self', '.', 'weight', 'Ä =', 'Ä torch', '.', 'randn', '(', 'input', '_', 'size', ',', 'Ä output', '_', 'size', ')', 'ÄŠÄ Ä Ä Ä Ä Ä Ä ', 'Ä self', '.', 'bias', 'Ä =', 'Ä torch', '.', 'zeros', '(', 'output', '_', 'size', ')', 'ÄŠÄŠÄ Ä Ä ', 'Ä def', 'Ä __', 'call', '__(', 'self', ',', 'Ä x', '):', 'ÄŠÄ Ä Ä Ä Ä Ä Ä ', 'Ä return', 'Ä x', 'Ä @', 'Ä self', '.', 'weights', 'Ä +', 'Ä self', '.', 'bias', 'ÄŠÄ Ä Ä Ä ']\n"
     ]
    }
   ],
   "source": [
    "example = \"\"\"class LinearLayer():\n",
    "    def __init__(self, input_size, output_size):\n",
    "        self.weight = torch.randn(input_size, output_size)\n",
    "        self.bias = torch.zeros(output_size)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        return x @ self.weights + self.bias\n",
    "    \"\"\"\n",
    "\n",
    "print(tokenizer.tokenize(example))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fca62b65-b3d3-45aa-ba26-eaa170e7b7ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('code-search-net-tokenizer/tokenizer_config.json',\n",
       " 'code-search-net-tokenizer/special_tokens_map.json',\n",
       " 'code-search-net-tokenizer/vocab.json',\n",
       " 'code-search-net-tokenizer/merges.txt',\n",
       " 'code-search-net-tokenizer/added_tokens.json',\n",
       " 'code-search-net-tokenizer/tokenizer.json')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.save_pretrained(\"code-search-net-tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1637c5d1-1f19-4e4c-8b0a-413c959cb5e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 4864\n",
      "drwxr-xr-x 2 so_olliphant so_olliphant    4096 Feb 18 05:13 .\n",
      "drwxr-xr-x 8 so_olliphant so_olliphant    4096 Feb 18 05:13 ..\n",
      "-rw-r--r-- 1 so_olliphant so_olliphant  466894 Feb 18 05:13 merges.txt\n",
      "-rw-r--r-- 1 so_olliphant so_olliphant      99 Feb 18 05:13 special_tokens_map.json\n",
      "-rw-r--r-- 1 so_olliphant so_olliphant 3673415 Feb 18 05:13 tokenizer.json\n",
      "-rw-r--r-- 1 so_olliphant so_olliphant     471 Feb 18 05:13 tokenizer_config.json\n",
      "-rw-r--r-- 1 so_olliphant so_olliphant  822037 Feb 18 05:13 vocab.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!ls -la code-search-net-tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d52ae06-4e12-4654-8f0c-d7210cc52f24",
   "metadata": {},
   "source": [
    ",,,"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a189ce53-25fe-4031-b251-0a76ba2d48f3",
   "metadata": {},
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3be0ddc9-9ec1-47c2-99fe-727cc1a56c55",
   "metadata": {},
   "source": [
    "huggingface-cli login"
   ]
  },
  {
   "cell_type": "raw",
   "id": "29a2ba8d-fe25-4240-b4d5-5aaa10e74b6a",
   "metadata": {},
   "source": [
    "tokenizer.push_to_hub(\"code-search-net-tokenizer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fa5dbfa-895f-42f5-9761-4ea63746b8da",
   "metadata": {},
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "86b5fa11-84ce-4d18-88be-96db6f4efec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace \"huggingface-course\" below with your actual namespace to use your own tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"code-search-net-tokenizer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33479199-7a38-46c6-ae12-64ae0c0774ab",
   "metadata": {},
   "source": [
    "#### Fast tokenizersâ€™ special powers\n",
    "\n",
    "> <span style=\"background-color:#33FFFF\"><b>Slow</b> tokenizers</span> are those written in Python inside the ðŸ¤— Transformers library<p/>\n",
    "> <span style=\"background-color:#AFFF33\">the <b>fast</b> versions</span> are the ones provided by ðŸ¤— Tokenizers, which are written in Rust"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cb0ee55a-2785-4348-8f50-68f6842e4889",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizer.is_fast ? True\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "print(f\"tokenizer.is_fast ? {tokenizer.is_fast}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2db3532d-f6f3-413c-a821-8f6d9bcd2f40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "example: My name is Sylvain and I work at Hugging Face in Brooklyn.\n",
      "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
      "\n",
      "encoding.is_fast ? True\n",
      "\n",
      "tokens:\n",
      "['[CLS]', 'My', 'name', 'is', 'S', '##yl', '##va', '##in', 'and', 'I', 'work', 'at', 'Hu', '##gging', 'Face', 'in', 'Brooklyn', '.', '[SEP]']\n",
      "\n",
      "word IDs:\n",
      "[None, 0, 1, 2, 3, 3, 3, 3, 4, 5, 6, 7, 8, 8, 9, 10, 11, 12, None]\n"
     ]
    }
   ],
   "source": [
    "example = \"My name is Sylvain and I work at Hugging Face in Brooklyn.\"\n",
    "encoding = tokenizer(example)\n",
    "\n",
    "print(f\"example: {example}\")\n",
    "print(type(encoding))\n",
    "print()\n",
    "\n",
    "print(f\"encoding.is_fast ? {encoding.is_fast}\\n\")\n",
    "print(f\"tokens:\\n{encoding.tokens()}\\n\")\n",
    "print(f\"word IDs:\\n{encoding.word_ids()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2562c499-2209-4785-bd08-52557ca92a2d",
   "metadata": {},
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "781ef697-25a4-4628-b107-0d1286d03a86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a8d0c98a45e48459f376bfb8323747d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd9454f64719495b99c0262c8fe0c39c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/481 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b51c44b12b845ef8395306c1e0c1552",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac3e291ad2a148a0ab86911a6d14c54c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d90d4a202e06446c9c933272b350b43a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert-base-cased\n",
      "tokens: ['[CLS]', '81', '##s', '[SEP]']\n",
      "word IDs: [None, 0, 0, None]\n",
      "\n",
      "roberta-based\n",
      "tokens: ['<s>', '81', 's', '</s>']\n",
      "word IDs: [None, 0, 1, None]\n"
     ]
    }
   ],
   "source": [
    "tokenizer_roberta = AutoTokenizer.from_pretrained(\"roberta-base\")\n",
    "\n",
    "check_this = \"81s\"\n",
    "\n",
    "encoding_bert = tokenizer(check_this)\n",
    "encoding_roberta = tokenizer_roberta(check_this)\n",
    "\n",
    "print(\"bert-base-cased\")\n",
    "print(f\"tokens: {encoding_bert.tokens()}\")\n",
    "print(f\"word IDs: {encoding_bert.word_ids()}\")\n",
    "\n",
    "print()\n",
    "\n",
    "print(\"roberta-based\")\n",
    "print(f\"tokens: {encoding_roberta.tokens()}\")\n",
    "print(f\"word IDs: {encoding_roberta.word_ids()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecb32f52-b784-4b08-aeb1-4c096e7d9bf8",
   "metadata": {},
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0847302d-992c-4c5f-aea0-9b7e307df0c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Sylvain'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start, end = encoding.word_to_chars(3)\n",
    "example[start:end]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c3f03b4-4490-4e4d-9196-d9286a89667c",
   "metadata": {},
   "source": [
    "...\n",
    "\n",
    "##### There is no `sentence_ids` method on the return encodings from `tokenizer(input)`!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c0191773-3e2d-4b06-9a01-1c66707ad454",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_1 = \"I am the son and heir of a shyness that is criminally vulgar. I am the son and the heir of nothing in particular. Really, I am.\"\n",
    "sentence_2 = \"If it's not love, then it's the Bomb that'll bring us together.\"\n",
    "\n",
    "encoding_sentence_1 = tokenizer(sentence_1)\n",
    "encoding_sentence_2 = tokenizer(sentence_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "db3e047e-18c6-4d25-9eed-9be67bfa4680",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['_MutableMapping__marker', '__abstractmethods__', '__class__', '__class_getitem__', '__contains__', '__copy__', '__delattr__', '__delitem__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattr__', '__getattribute__', '__getitem__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__ior__', '__iter__', '__le__', '__len__', '__lt__', '__module__', '__ne__', '__new__', '__or__', '__reduce__', '__reduce_ex__', '__repr__', '__reversed__', '__ror__', '__setattr__', '__setitem__', '__setstate__', '__sizeof__', '__slots__', '__str__', '__subclasshook__', '__weakref__', '_abc_impl', '_encodings', '_n_sequences', 'char_to_token', 'char_to_word', 'clear', 'convert_to_tensors', 'copy', 'data', 'encodings', 'fromkeys', 'get', 'is_fast', 'items', 'keys', 'n_sequences', 'pop', 'popitem', 'sequence_ids', 'setdefault', 'to', 'token_to_chars', 'token_to_sequence', 'token_to_word', 'tokens', 'update', 'values', 'word_ids', 'word_to_chars', 'word_to_tokens', 'words']\n"
     ]
    }
   ],
   "source": [
    "print(dir(encoding_sentence_1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7d102f9-f791-43b5-add9-664812b8e3bc",
   "metadata": {},
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b76a6083-1f39-4f10-8192-2c255b4c7f23",
   "metadata": {},
   "source": [
    "#### Inside the token-classification pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5c8e339-6c78-4838-a379-cf294bf5f017",
   "metadata": {},
   "source": [
    "> First, letâ€™s grab a token classification pipeline so we can get some results to compare manually. The model used by default is [`dbmdz/bert-large-cased-finetuned-conll03-english`](https://huggingface.co/dbmdz/bert-large-cased-finetuned-conll03-english); it performs NER on sentences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "408c43b6-9a6f-4a3b-982b-d6048f39cfbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to dbmdz/bert-large-cased-finetuned-conll03-english and revision 4c53496 (https://huggingface.co/dbmdz/bert-large-cased-finetuned-conll03-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "181ab965a2cc496686c4abe516c179e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/998 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1feb30eee75b4032bf0020ff443a3485",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.33G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90c1feaaec4440f7a209a0251d0d58c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/60.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f3b4995953b4e15bde7003194127e3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/213k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'entity': 'I-PER', 'score': 0.99938285, 'index': 4, 'word': 'S', 'start': 11, 'end': 12}\n",
      "{'entity': 'I-PER', 'score': 0.99815494, 'index': 5, 'word': '##yl', 'start': 12, 'end': 14}\n",
      "{'entity': 'I-PER', 'score': 0.9959072, 'index': 6, 'word': '##va', 'start': 14, 'end': 16}\n",
      "{'entity': 'I-PER', 'score': 0.99923277, 'index': 7, 'word': '##in', 'start': 16, 'end': 18}\n",
      "{'entity': 'I-ORG', 'score': 0.9738931, 'index': 12, 'word': 'Hu', 'start': 33, 'end': 35}\n",
      "{'entity': 'I-ORG', 'score': 0.976115, 'index': 13, 'word': '##gging', 'start': 35, 'end': 40}\n",
      "{'entity': 'I-ORG', 'score': 0.9887976, 'index': 14, 'word': 'Face', 'start': 41, 'end': 45}\n",
      "{'entity': 'I-LOC', 'score': 0.9932106, 'index': 16, 'word': 'Brooklyn', 'start': 49, 'end': 57}\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "token_classifier = pipeline(\"token-classification\")\n",
    "\n",
    "for ntt in token_classifier(\"My name is Sylvain and I work at Hugging Face in Brooklyn.\"):\n",
    "    print(ntt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9730ec2e-5944-4dc9-8a14-8acb8ac85578",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to dbmdz/bert-large-cased-finetuned-conll03-english and revision 4c53496 (https://huggingface.co/dbmdz/bert-large-cased-finetuned-conll03-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'entity_group': 'PER', 'score': 0.9981694, 'word': 'Sylvain', 'start': 11, 'end': 18}\n",
      "{'entity_group': 'ORG', 'score': 0.9796019, 'word': 'Hugging Face', 'start': 33, 'end': 45}\n",
      "{'entity_group': 'LOC', 'score': 0.9932106, 'word': 'Brooklyn', 'start': 49, 'end': 57}\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "token_classifier = pipeline(\"token-classification\", aggregation_strategy=\"simple\")\n",
    "#                                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
    "\n",
    "for ntt in token_classifier(\"My name is Sylvain and I work at Hugging Face in Brooklyn.\"):\n",
    "    print(ntt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72f4369e-82af-4ab1-92ca-ff633523d0a2",
   "metadata": {},
   "source": [
    "##### From inputs to predictions\n",
    "\n",
    "... let's try doing the same w/out using `pipeline`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e860e1a2-53f6-41e5-8c7b-1149c9085253",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertForTokenClassification(\n",
      "  (bert): BertModel(\n",
      "    (embeddings): BertEmbeddings(\n",
      "      (word_embeddings): Embedding(28996, 1024, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 1024)\n",
      "      (token_type_embeddings): Embedding(2, 1024)\n",
      "      (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): BertEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0-23): 24 x BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSdpaSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (classifier): Linear(in_features=1024, out_features=9, bias=True)\n",
      ")\n",
      "\n",
      "BertConfig {\n",
      "  \"_attn_implementation_autoset\": true,\n",
      "  \"_name_or_path\": \"dbmdz/bert-large-cased-finetuned-conll03-english\",\n",
      "  \"_num_labels\": 9,\n",
      "  \"architectures\": [\n",
      "    \"BertForTokenClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"O\",\n",
      "    \"1\": \"B-MISC\",\n",
      "    \"2\": \"I-MISC\",\n",
      "    \"3\": \"B-PER\",\n",
      "    \"4\": \"I-PER\",\n",
      "    \"5\": \"B-ORG\",\n",
      "    \"6\": \"I-ORG\",\n",
      "    \"7\": \"B-LOC\",\n",
      "    \"8\": \"I-LOC\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"label2id\": {\n",
      "    \"B-LOC\": 7,\n",
      "    \"B-MISC\": 1,\n",
      "    \"B-ORG\": 5,\n",
      "    \"B-PER\": 3,\n",
      "    \"I-LOC\": 8,\n",
      "    \"I-MISC\": 2,\n",
      "    \"I-ORG\": 6,\n",
      "    \"I-PER\": 4,\n",
      "    \"O\": 0\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.49.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "\n",
    "model_checkpoint = \"dbmdz/bert-large-cased-finetuned-conll03-english\"\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_checkpoint)\n",
    "\n",
    "print(model)\n",
    "print()\n",
    "print(model.config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9cf86219-d32d-4e88-b93e-00267ae1a268",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizer.is_fast ? True\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "print(f\"tokenizer.is_fast ? {tokenizer.is_fast}\")\n",
    "\n",
    "example = \"My name is Sylvain and I work at Hugging Face in Brooklyn.\"\n",
    "inputs = tokenizer(example, return_tensors=\"pt\")\n",
    "\n",
    "outputs = model(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5e2e16c8-0e0d-4666-815d-002889152485",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 19])\n",
      "torch.Size([1, 19, 9])\n"
     ]
    }
   ],
   "source": [
    "print(inputs[\"input_ids\"].shape)\n",
    "print(outputs.logits.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe59385e-123f-402c-b50c-ca525b93d88a",
   "metadata": {},
   "source": [
    "##### NOTE\n",
    "\n",
    "* 1st `dim` is batch (index)\n",
    "* 2nd `dim` is sequence (length)\n",
    "* 3rd `dim` is logits (labels)\n",
    "\n",
    "We use Torch's [`torch.nn.functional.softmax`](https://pytorch.org/docs/stable/generated/torch.nn.functional.softmax.html#torch-nn-functional-softmax) to convert the logits to probabilities, and then `argmax` to get the final NER prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ca32bdfb-1f26-440c-8b1c-f5a13091ef72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "probabilities = torch.nn.functional.softmax(outputs.logits, dim=-1)[0].tolist()\n",
    "predictions = outputs.logits.argmax(dim=-1)[0].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d0418e8c-a7cd-4b92-bcfa-2d28bad34d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_labels = [\n",
    "    model.config.id2label[p]\n",
    "    for p in predictions\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0813560c-31dc-42f2-9f78-ea6d6006003d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] 0 O\n",
      "My 0 O\n",
      "name 0 O\n",
      "is 0 O\n",
      "S 4 I-PER\n",
      "##yl 4 I-PER\n",
      "##va 4 I-PER\n",
      "##in 4 I-PER\n",
      "and 0 O\n",
      "I 0 O\n",
      "work 0 O\n",
      "at 0 O\n",
      "Hu 6 I-ORG\n",
      "##gging 6 I-ORG\n",
      "Face 6 I-ORG\n",
      "in 0 O\n",
      "Brooklyn 8 I-LOC\n",
      ". 0 O\n",
      "[SEP] 0 O\n"
     ]
    }
   ],
   "source": [
    "for tok, pred, label in zip(inputs.tokens(), predictions, predictions_labels):\n",
    "    print(tok, pred, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "18bbfb4e-178c-4a4a-8b61-8a24886e39f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'entity': 'I-PER', 'score': 0.9993828535079956, 'word': 'S'},\n",
       " {'entity': 'I-PER', 'score': 0.9981548190116882, 'word': '##yl'},\n",
       " {'entity': 'I-PER', 'score': 0.995907187461853, 'word': '##va'},\n",
       " {'entity': 'I-PER', 'score': 0.9992327690124512, 'word': '##in'},\n",
       " {'entity': 'I-ORG', 'score': 0.9738931059837341, 'word': 'Hu'},\n",
       " {'entity': 'I-ORG', 'score': 0.9761149883270264, 'word': '##gging'},\n",
       " {'entity': 'I-ORG', 'score': 0.9887974858283997, 'word': 'Face'},\n",
       " {'entity': 'I-LOC', 'score': 0.99321049451828, 'word': 'Brooklyn'}]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = []\n",
    "tokens = inputs.tokens()\n",
    "\n",
    "for idx, pred in enumerate(predictions):\n",
    "    label = model.config.id2label[pred]\n",
    "    if label != \"O\":\n",
    "        results.append(\n",
    "            {\"entity\": label, \"score\": probabilities[idx][pred], \"word\": tokens[idx]}\n",
    "        )\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fb57ebb5-8c11-4e2b-835a-a592279c4251",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 0),\n",
       " (0, 2),\n",
       " (3, 7),\n",
       " (8, 10),\n",
       " (11, 12),\n",
       " (12, 14),\n",
       " (14, 16),\n",
       " (16, 18),\n",
       " (19, 22),\n",
       " (23, 24),\n",
       " (25, 29),\n",
       " (30, 32),\n",
       " (33, 35),\n",
       " (35, 40),\n",
       " (41, 45),\n",
       " (46, 48),\n",
       " (49, 57),\n",
       " (57, 58),\n",
       " (0, 0)]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs_with_offsets = tokenizer(example, return_offsets_mapping=True)\n",
    "#                                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
    "\n",
    "inputs_with_offsets[\"offset_mapping\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "82c83857-3a28-44b8-afcf-7c9ccf6e2afd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'yl'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example[12:14]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f942f430-dea5-4d61-a457-effffbfc5429",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'entity': 'I-PER',\n",
       "  'score': 0.9993828535079956,\n",
       "  'word': 'S',\n",
       "  'start': 11,\n",
       "  'end': 12},\n",
       " {'entity': 'I-PER',\n",
       "  'score': 0.9981548190116882,\n",
       "  'word': '##yl',\n",
       "  'start': 12,\n",
       "  'end': 14},\n",
       " {'entity': 'I-PER',\n",
       "  'score': 0.995907187461853,\n",
       "  'word': '##va',\n",
       "  'start': 14,\n",
       "  'end': 16},\n",
       " {'entity': 'I-PER',\n",
       "  'score': 0.9992327690124512,\n",
       "  'word': '##in',\n",
       "  'start': 16,\n",
       "  'end': 18},\n",
       " {'entity': 'I-ORG',\n",
       "  'score': 0.9738931059837341,\n",
       "  'word': 'Hu',\n",
       "  'start': 33,\n",
       "  'end': 35},\n",
       " {'entity': 'I-ORG',\n",
       "  'score': 0.9761149883270264,\n",
       "  'word': '##gging',\n",
       "  'start': 35,\n",
       "  'end': 40},\n",
       " {'entity': 'I-ORG',\n",
       "  'score': 0.9887974858283997,\n",
       "  'word': 'Face',\n",
       "  'start': 41,\n",
       "  'end': 45},\n",
       " {'entity': 'I-LOC',\n",
       "  'score': 0.99321049451828,\n",
       "  'word': 'Brooklyn',\n",
       "  'start': 49,\n",
       "  'end': 57}]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = []\n",
    "inputs_with_offsets = tokenizer(example, return_offsets_mapping=True)\n",
    "tokens = inputs_with_offsets.tokens()\n",
    "offsets = inputs_with_offsets[\"offset_mapping\"]\n",
    "\n",
    "for idx, pred in enumerate(predictions):\n",
    "    label = model.config.id2label[pred]\n",
    "    if label != \"O\":\n",
    "        start, end = offsets[idx]\n",
    "        results.append(\n",
    "            {\n",
    "                \"entity\": label,\n",
    "                \"score\": probabilities[idx][pred],\n",
    "                \"word\": tokens[idx],\n",
    "                \"start\": start,\n",
    "                \"end\": end,\n",
    "            }\n",
    "        )\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d67c4de8-41c3-48cc-abed-ef7f95f05630",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hugging Face'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example[33:45]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6c6269e3-67a8-4294-9ec0-02c93c6e1fdf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'entity_group': 'PER',\n",
       "  'score': 0.998169407248497,\n",
       "  'word': 'Sylvain',\n",
       "  'start': 11,\n",
       "  'end': 18},\n",
       " {'entity_group': 'ORG',\n",
       "  'score': 0.9796018600463867,\n",
       "  'word': 'Hugging Face',\n",
       "  'start': 33,\n",
       "  'end': 45},\n",
       " {'entity_group': 'LOC',\n",
       "  'score': 0.99321049451828,\n",
       "  'word': 'Brooklyn',\n",
       "  'start': 49,\n",
       "  'end': 57}]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "results = []\n",
    "inputs_with_offsets = tokenizer(example, return_offsets_mapping=True)\n",
    "tokens = inputs_with_offsets.tokens()\n",
    "offsets = inputs_with_offsets[\"offset_mapping\"]\n",
    "\n",
    "idx = 0\n",
    "while idx < len(predictions):\n",
    "    pred = predictions[idx]\n",
    "    label = model.config.id2label[pred]\n",
    "    if label != \"O\":\n",
    "        # Remove the B- or I-\n",
    "        label = label[2:]\n",
    "        start, _ = offsets[idx]\n",
    "\n",
    "        # Grab all the tokens labeled with I-label\n",
    "        all_scores = []\n",
    "        while (\n",
    "            idx < len(predictions)\n",
    "            and model.config.id2label[predictions[idx]] == f\"I-{label}\"\n",
    "        ):\n",
    "            all_scores.append(probabilities[idx][pred])\n",
    "            _, end = offsets[idx]\n",
    "            idx += 1\n",
    "\n",
    "        # The score is the mean of all the scores of the tokens in that grouped entity\n",
    "        score = np.mean(all_scores).item()\n",
    "        word = example[start:end]\n",
    "        results.append(\n",
    "            {\n",
    "                \"entity_group\": label,\n",
    "                \"score\": score,\n",
    "                \"word\": word,\n",
    "                \"start\": start,\n",
    "                \"end\": end,\n",
    "            }\n",
    "        )\n",
    "    idx += 1\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2934e703-dbda-4004-8174-e21391171910",
   "metadata": {},
   "source": [
    "#### Fast tokenizers in the QA pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a8c21ef-27ea-48f0-97be-11bee9ef0c32",
   "metadata": {},
   "source": [
    "##### QA using `pipeline`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4816434c-3f22-4632-ac23-c7b901cd7c0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-cased-distilled-squad and revision 564e9b5 (https://huggingface.co/distilbert/distilbert-base-cased-distilled-squad).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca3f8953176a4c498c4cd47307632aa5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/473 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "442fe65620a543609267f923257c01a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/261M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d598bbeb62644b63b6d54b3388be4508",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/49.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "847ecdb6c5a44f72bab599f1272ffbf3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/213k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "136bca4167914853b06cc7b244e242a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/436k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'score': 0.98026043176651,\n",
       " 'start': 78,\n",
       " 'end': 106,\n",
       " 'answer': 'Jax, PyTorch, and TensorFlow'}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "question = \"Which deep learning libraries back ðŸ¤— Transformers?\"\n",
    "\n",
    "context = \"\"\"\n",
    "ðŸ¤— Transformers is backed by the three most popular deep learning libraries â€” Jax, PyTorch, and TensorFlow â€” with a seamless integration\n",
    "between them. It's straightforward to train your models with one before loading them for inference with the other.\n",
    "\"\"\"\n",
    "\n",
    "question_answerer = pipeline(\"question-answering\")\n",
    "question_answerer(question=question, context=context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4fb942c5-56c5-4777-8e2d-22818a40b520",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 0.9714871048927307,\n",
       " 'start': 1892,\n",
       " 'end': 1919,\n",
       " 'answer': 'Jax, PyTorch and TensorFlow'}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "long_context = \"\"\"\n",
    "ðŸ¤— Transformers: State of the Art NLP\n",
    "\n",
    "ðŸ¤— Transformers provides thousands of pretrained models to perform tasks on texts such as classification, information extraction,\n",
    "question answering, summarization, translation, text generation and more in over 100 languages.\n",
    "Its aim is to make cutting-edge NLP easier to use for everyone.\n",
    "\n",
    "ðŸ¤— Transformers provides APIs to quickly download and use those pretrained models on a given text, fine-tune them on your own datasets and\n",
    "then share them with the community on our model hub. At the same time, each python module defining an architecture is fully standalone and\n",
    "can be modified to enable quick research experiments.\n",
    "\n",
    "Why should I use transformers?\n",
    "\n",
    "1. Easy-to-use state-of-the-art models:\n",
    "  - High performance on NLU and NLG tasks.\n",
    "  - Low barrier to entry for educators and practitioners.\n",
    "  - Few user-facing abstractions with just three classes to learn.\n",
    "  - A unified API for using all our pretrained models.\n",
    "  - Lower compute costs, smaller carbon footprint:\n",
    "\n",
    "2. Researchers can share trained models instead of always retraining.\n",
    "  - Practitioners can reduce compute time and production costs.\n",
    "  - Dozens of architectures with over 10,000 pretrained models, some in more than 100 languages.\n",
    "\n",
    "3. Choose the right framework for every part of a model's lifetime:\n",
    "  - Train state-of-the-art models in 3 lines of code.\n",
    "  - Move a single model between TF2.0/PyTorch frameworks at will.\n",
    "  - Seamlessly pick the right framework for training, evaluation and production.\n",
    "\n",
    "4. Easily customize a model or an example to your needs:\n",
    "  - We provide examples for each architecture to reproduce the results published by its original authors.\n",
    "  - Model internals are exposed as consistently as possible.\n",
    "  - Model files can be used independently of the library for quick experiments.\n",
    "\n",
    "ðŸ¤— Transformers is backed by the three most popular deep learning libraries â€” Jax, PyTorch and TensorFlow â€” with a seamless integration\n",
    "between them. It's straightforward to train your models with one before loading them for inference with the other.\n",
    "\"\"\"\n",
    "#                                                                              ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
    "#\n",
    "# ... see now how the answer is almost at the very end of this\n",
    "#     long, long context ...?\n",
    "\n",
    "question_answerer(question=question, context=long_context)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5546be38-2017-408c-a0c2-7f5e000f496a",
   "metadata": {},
   "source": [
    "##### Using a model for question answering\n",
    "\n",
    "... where we do things the hard way.\n",
    "\n",
    "> The checkpoint used by default for the question-answering pipeline is [`distilbert-base-cased-distilled-squad`](https://huggingface.co/distilbert/distilbert-base-cased-distilled-squad)\n",
    "\n",
    "> Models for question answering work a little differently from the models weâ€™ve seen up to now. Using the picture above as an example, the model has been trained to predict the index of the token starting the answer (here 21) and the index of the token where the answer ends (here 24). This is why those models donâ€™t return one tensor of logits but two: one for the logits corresponding to the start token of the answer, and one for the logits corresponding to the end token of the answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7643bc7f-2bb5-48c6-b7bd-9aef3c8eff99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1799b2fcfbeb4dbcab1effc5185b36a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/49.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f07d016f9d64a4c80f3f0678d8f7999",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/473 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7a3108946054de58af49debc39dc974",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/213k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "069bad7cd1ae4cf0bad0113216d3fe01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/436k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8795c76f85864ed18bca3429d3f80cc7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/261M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
    "\n",
    "model_checkpoint = \"distilbert-base-cased-distilled-squad\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(model_checkpoint)\n",
    "\n",
    "inputs = tokenizer(question, context, return_tensors=\"pt\")\n",
    "outputs = model(**inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9222955-cd95-4e09-97e4-89349088f074",
   "metadata": {},
   "source": [
    "##### For QA, we have logits on both the start of span with the answer; and the end of the span with the answer.\n",
    "\n",
    "... or we need to use the `[CLS]` token for indicating an impossible answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2458a90e-dda1-4890-8df5-31cce96bf91c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num. of tokens? 67\n",
      "torch.Size([1, 67]) torch.Size([1, 67])\n"
     ]
    }
   ],
   "source": [
    "start_logits = outputs.start_logits\n",
    "end_logits = outputs.end_logits\n",
    "\n",
    "print(f\"num. of tokens? {len(inputs.tokens())}\")\n",
    "print(start_logits.shape, end_logits.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f77d7298-25e8-401d-8ab2-c23dc1f2a812",
   "metadata": {},
   "source": [
    "> To convert those logits into probabilities, we will apply a softmax function â€” but before that, we need to make sure we mask the indices that are not part of the context. Our input is `[CLS]` question [SEP] context `[SEP]`, so we need to mask the tokens of the question as well as the `[SEP]` token. Weâ€™ll keep the `[CLS]` token, however, as some models use it to indicate that the answer is not in the context.\n",
    "\n",
    "##### To clarify\n",
    "\n",
    "* We want to calculate the probabilities for the `start` and `end` tokens using <u>only the context</u> and not the question.\n",
    "* We do that by setting the probabilities on the tokens in the question, as well as the `[SEP]` BERT tokens to 0\n",
    "* Some models use `[CLS]` for indicating an impossible answer (answer could not be found in the context), so we need to allow that through for calculating probabilities.\n",
    "* So, we need to create a mask of `1`s (or `True` on the positions for the tokens in the question and those `[SEP]` tokens\n",
    "* We can then set the logits on those tokens to some large, negative numbers, since $Softmax(x_{i}) = \\frac{exp(x_{i})}{\\sum_j{exp(x_{j})}}$ and the exponent of a large, negative number $x_{i}$ yields 0.\n",
    "\n",
    "We will use the `sequence_ids` values for building up a mask which we will use for setting the probabilities for the tokens in the question (and BERT special tokens) to 0. \n",
    "\n",
    "`sequence_ids` values are:\n",
    "* `None` for `[CLS]`, `[SEP]` special BERT tokens\n",
    "* `0` for the question\n",
    "* `1` for the context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8b543449-cad4-4b34-8558-ee04aeb4043f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('[CLS]', None),\n",
       " ('Which', 0),\n",
       " ('deep', 0),\n",
       " ('learning', 0),\n",
       " ('libraries', 0),\n",
       " ('back', 0),\n",
       " ('[UNK]', 0),\n",
       " ('Transformers', 0),\n",
       " ('?', 0),\n",
       " ('[SEP]', None),\n",
       " ('[UNK]', 1),\n",
       " ('Transformers', 1),\n",
       " ('is', 1),\n",
       " ('backed', 1),\n",
       " ('by', 1),\n",
       " ('the', 1),\n",
       " ('three', 1),\n",
       " ('most', 1),\n",
       " ('popular', 1),\n",
       " ('deep', 1),\n",
       " ('learning', 1),\n",
       " ('libraries', 1),\n",
       " ('â€”', 1),\n",
       " ('Jax', 1),\n",
       " (',', 1),\n",
       " ('P', 1),\n",
       " ('##y', 1),\n",
       " ('##T', 1),\n",
       " ('##or', 1),\n",
       " ('##ch', 1),\n",
       " (',', 1),\n",
       " ('and', 1),\n",
       " ('Ten', 1),\n",
       " ('##sor', 1),\n",
       " ('##F', 1),\n",
       " ('##low', 1),\n",
       " ('â€”', 1),\n",
       " ('with', 1),\n",
       " ('a', 1),\n",
       " ('sea', 1),\n",
       " ('##m', 1),\n",
       " ('##less', 1),\n",
       " ('integration', 1),\n",
       " ('between', 1),\n",
       " ('them', 1),\n",
       " ('.', 1),\n",
       " ('It', 1),\n",
       " (\"'\", 1),\n",
       " ('s', 1),\n",
       " ('straightforward', 1),\n",
       " ('to', 1),\n",
       " ('train', 1),\n",
       " ('your', 1),\n",
       " ('models', 1),\n",
       " ('with', 1),\n",
       " ('one', 1),\n",
       " ('before', 1),\n",
       " ('loading', 1),\n",
       " ('them', 1),\n",
       " ('for', 1),\n",
       " ('in', 1),\n",
       " ('##ference', 1),\n",
       " ('with', 1),\n",
       " ('the', 1),\n",
       " ('other', 1),\n",
       " ('.', 1),\n",
       " ('[SEP]', None)]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "sequence_ids = inputs.sequence_ids()\n",
    "#print(inputs.sequence_ids())\n",
    "#print()\n",
    "\n",
    "list(zip(inputs.tokens(), sequence_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f79fef03-e631-4a07-802a-5d5de11a4d3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 67])\n"
     ]
    }
   ],
   "source": [
    "# Mask indicating the positions of the question tokens and [SEP]\n",
    "mask = [i != 1 for i in sequence_ids]\n",
    "#print(mask)\n",
    "\n",
    "# Unmask the [CLS] token\n",
    "mask[0] = False\n",
    "#print(mask)\n",
    "#print()\n",
    "\n",
    "# N.B., the mask needs to be the same shape\n",
    "#       as the tensor of output logits\n",
    "#mask = torch.tensor(mask)[None]\n",
    "mask = torch.tensor(mask).unsqueeze(dim=0)\n",
    "#mask = torch.tensor(mask)\n",
    "print(mask.shape)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a47513c5-ade2-4145-87d0-5c07b1615fda",
   "metadata": {},
   "source": [
    "torch.tensor(mask).unsqueeze(dim=0)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "51850f6b-8971-43b4-822c-22de44c0b7fc",
   "metadata": {},
   "source": [
    "torch.tensor(mask)[None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4330b19e-8017-4950-a73a-bb4f77470ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_logits[mask] = -10000\n",
    "end_logits[mask] = -10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "de4cbc53-75f7-427e-bb26-cdf61406f276",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_probabilities = torch.nn.functional.softmax(start_logits, dim=-1)[0]\n",
    "end_probabilities = torch.nn.functional.softmax(end_logits, dim=-1)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4aca574-b6f2-4051-bf4a-e043e4802b9b",
   "metadata": {},
   "source": [
    "##### Create a matrix of scores\n",
    "\n",
    "`start_probabilities[:, None] * end_probabilities[None, :]`, or `unsqueeze`ing the tensors will create a $n \\times n$ tensor (matrix) of scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4a40c7bc-1a79-4e78-a58f-7108507be524",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([67, 67])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = start_probabilities[:, None] * end_probabilities[None, :]\n",
    "scores.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63650919-ad4f-4484-8ceb-02765cf5d759",
   "metadata": {},
   "source": [
    "[API documentation for `torch.triu`](https://pytorch.org/docs/stable/generated/torch.triu.html#torch-triu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "5ab36b83-d3fe-4331-a459-bc5f65f3e50f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[9.4340e-13, 0.0000e+00, 0.0000e+00,  ..., 1.1023e-12, 1.6345e-12,\n",
       "         0.0000e+00],\n",
       "        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00],\n",
       "        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00],\n",
       "        ...,\n",
       "        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 9.1136e-14, 1.3514e-13,\n",
       "         0.0000e+00],\n",
       "        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 1.2744e-13,\n",
       "         0.0000e+00],\n",
       "        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00]], grad_fn=<TriuBackward0>)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = torch.triu(scores)\n",
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4941d823-905a-4cc6-9508-4747adfa4c95",
   "metadata": {},
   "source": [
    "> Now we just have to get the index of the maximum. Since PyTorch will return the index in the flattened tensor, we need to use the floor division // and modulus % operations to get the `start_index` and `end_index`\n",
    "\n",
    "##### Clarification: `//` and `%` operations to get start and end???\n",
    "\n",
    "That explanation above is somewhat lacking...\n",
    "\n",
    "From ['s response to Get indices of the max of a 2D Tensor question at discuss.pytorch.org](https://discuss.pytorch.org/t/get-indices-of-the-max-of-a-2d-tensor/82150/5):\n",
    "\n",
    "> Since `argmax()` gives you the index in a flattened tensor, you can infer the position in your 2D tensor from size of the last dimension.\n",
    "> E.g. if `argmax()` returns 10 and youâ€™ve got 4 columns, you know itâ€™s on row 2, column 2.\n",
    "> You can use Python's (built-in function) [`divmod`](https://docs.python.org/3/library/functions.html#divmod) for this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f6b365b5-63e5-44b1-9fe8-a13e880b1045",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'int'>\n",
      "start_index: 23\n",
      "end_index: 35\n",
      "tensor(0.9803, grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "max_index = scores.argmax().item()\n",
    "print(type(max_index))\n",
    "\n",
    "start_index = max_index // scores.shape[1]\n",
    "end_index   = max_index %  scores.shape[1]\n",
    "#start_index, end_index = divmod(max_index, scores.shape[1])\n",
    "\n",
    "print(f\"start_index: {start_index}\")\n",
    "print(f\"end_index: {end_index}\")\n",
    "\n",
    "print(scores[start_index, end_index])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f4e3c30-7adf-4a5e-8c17-20ab0431be79",
   "metadata": {},
   "source": [
    "##### Slight detour for extra credit\n",
    "\n",
    "> âœï¸ Try it out! Compute the start and end indices for the five most likely answers.\n",
    "\n",
    "Please see API documentation for [`torch.topk`](https://pytorch.org/docs/stable/generated/torch.Tensor.topk.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "9aff1238-f059-4280-bac6-8c6ef3d13cd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(23, 35), (23, 36), (16, 35), (23, 29), (25, 35)]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "values, indices = torch.topk(scores.flatten(), 5)\n",
    "start_and_end = [\n",
    "    divmod(max_index, scores.shape[1])\n",
    "    for max_index in indices.tolist()\n",
    "]\n",
    "start_and_end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01c0d426-5204-4c84-9d4b-31707299302b",
   "metadata": {},
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "4d852658-4739-44ef-8865-729cd47fc255",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [101, 5979, 1996, 3776, 9818, 1171, 100, 25267, 136, 102, 100, 25267, 1110, 5534, 1118, 1103, 1210, 1211, 1927, 1996, 3776, 9818, 783, 13612, 117, 153, 1183, 1942, 1766, 1732, 117, 1105, 5157, 21484, 2271, 6737, 783, 1114, 170, 2343, 1306, 2008, 9111, 1206, 1172, 119, 1135, 112, 188, 21546, 1106, 2669, 1240, 3584, 1114, 1141, 1196, 10745, 1172, 1111, 1107, 16792, 1114, 1103, 1168, 119, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'offset_mapping': [(0, 0), (0, 5), (6, 10), (11, 19), (20, 29), (30, 34), (35, 36), (37, 49), (49, 50), (0, 0), (1, 2), (3, 15), (16, 18), (19, 25), (26, 28), (29, 32), (33, 38), (39, 43), (44, 51), (52, 56), (57, 65), (66, 75), (76, 77), (78, 81), (81, 82), (83, 84), (84, 85), (85, 86), (86, 88), (88, 90), (90, 91), (92, 95), (96, 99), (99, 102), (102, 103), (103, 106), (107, 108), (109, 113), (114, 115), (116, 119), (119, 120), (120, 124), (125, 136), (137, 144), (145, 149), (149, 150), (151, 153), (153, 154), (154, 155), (156, 171), (172, 174), (175, 180), (181, 185), (186, 192), (193, 197), (198, 201), (202, 208), (209, 216), (217, 221), (222, 225), (226, 228), (228, 235), (236, 240), (241, 244), (245, 250), (250, 251), (0, 0)]}\n"
     ]
    }
   ],
   "source": [
    "inputs_with_offsets = tokenizer(question, context, return_offsets_mapping=True)\n",
    "print(inputs_with_offsets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "fbf17ec3-4784-4fe1-808e-789836fe0812",
   "metadata": {},
   "outputs": [],
   "source": [
    "offsets = inputs_with_offsets[\"offset_mapping\"]\n",
    "\n",
    "start_char, _ = offsets[start_index]\n",
    "_, end_char = offsets[end_index]\n",
    "answer = context[start_char:end_char]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a8f594d3-ade4-491a-ba84-dcb9ad01a144",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'answer': 'Jax, PyTorch, and TensorFlow',\n",
       " 'start': 78,\n",
       " 'end': 106,\n",
       " 'score': tensor(0.9803, grad_fn=<SelectBackward0>)}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = {\n",
    "    \"answer\": answer,\n",
    "    \"start\": start_char,\n",
    "    \"end\": end_char,\n",
    "    \"score\": scores[start_index, end_index],\n",
    "}\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddd628ea-2821-48c1-8e6e-2c8c5c0ddefc",
   "metadata": {},
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a3228b7a-9d31-4e4d-8d50-9039b93f95a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 0.98026043176651,\n",
       "  'start': 78,\n",
       "  'end': 106,\n",
       "  'answer': 'Jax, PyTorch, and TensorFlow'},\n",
       " {'score': 0.008247777819633484,\n",
       "  'start': 78,\n",
       "  'end': 108,\n",
       "  'answer': 'Jax, PyTorch, and TensorFlow â€”'},\n",
       " {'score': 0.001367696444503963,\n",
       "  'start': 78,\n",
       "  'end': 90,\n",
       "  'answer': 'Jax, PyTorch'},\n",
       " {'score': 0.00038108485750854015,\n",
       "  'start': 83,\n",
       "  'end': 106,\n",
       "  'answer': 'PyTorch, and TensorFlow'},\n",
       " {'score': 0.00021684444800484926,\n",
       "  'start': 96,\n",
       "  'end': 106,\n",
       "  'answer': 'TensorFlow'}]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question_answerer(question=question, context=context, top_k=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c826c9ae-2a48-416b-84f7-ce4a5eba53c3",
   "metadata": {},
   "source": [
    "#### Handling long contexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "59750c48-b0a4-40db-adc9-f63aca18b544",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "461\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer(question, long_context)\n",
    "\n",
    "print(len(inputs[\"input_ids\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "918e9513-07fc-4c13-8ffa-d5168685e65a",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(question, long_context, max_length=384, truncation=\"only_second\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "641af323-dfa9-413a-bf26-3ce9a4e2235b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] Which deep learning libraries back [UNK] Transformers? [SEP] [UNK] Transformers : State of the Art NLP [UNK] Transformers provides thousands of pretrained models to perform tasks on texts such as classification, information extraction, question answering, summarization, translation, text generation and more in over 100 languages. Its aim is to make cutting - edge NLP easier to use for everyone. [UNK] Transformers provides APIs to quickly download and use those pretrained models on a given text, fine - tune them on your own datasets and then share them with the community on our model hub. At the same time, each python module defining an architecture is fully standalone and can be modified to enable quick research experiments. Why should I use transformers? 1. Easy - to - use state - of - the - art models : - High performance on NLU and NLG tasks. - Low barrier to entry for educators and practitioners. - Few user - facing abstractions with just three classes to learn. - A unified API for using all our pretrained models. - Lower compute costs, smaller carbon footprint : 2. Researchers can share trained models instead of always retraining. - Practitioners can reduce compute time and production costs. - Dozens of architectures with over 10, 000 pretrained models, some in more than 100 languages. 3. Choose the right framework for every part of a model ' s lifetime : - Train state - of - the - art models in 3 lines of code. - Move a single model between TF2. 0 / PyTorch frameworks at will. - Seamlessly pick the right framework for training, evaluation and production. 4. Easily customize a model or an example to your needs : - We provide examples for each architecture to reproduce the results published by its original authors. - Model internal [SEP]\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(inputs[\"input_ids\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "d36dc830-dc70-495b-bb3c-04a6b920b7c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] This sentence is not [SEP]\n",
      "[CLS] is not too long [SEP]\n",
      "[CLS] too long but we [SEP]\n",
      "[CLS] but we are going [SEP]\n",
      "[CLS] are going to split [SEP]\n",
      "[CLS] to split it anyway [SEP]\n",
      "[CLS] it anyway. [SEP]\n"
     ]
    }
   ],
   "source": [
    "sentence = \"This sentence is not too long but we are going to split it anyway.\"\n",
    "inputs = tokenizer(\n",
    "    sentence, truncation=True, return_overflowing_tokens=True, max_length=6, stride=2\n",
    ")\n",
    "\n",
    "for ids in inputs[\"input_ids\"]:\n",
    "    print(tokenizer.decode(ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "f4b07212-ef28-404b-9975-0721c52276b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['input_ids', 'attention_mask', 'overflow_to_sample_mapping'])\n"
     ]
    }
   ],
   "source": [
    "print(inputs.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "9176ce96-7587-43f3-9c92-bb7759e4ec14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "print(inputs[\"overflow_to_sample_mapping\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "4d168ce9-51cf-4f8c-ad06-e3a3c244abf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n"
     ]
    }
   ],
   "source": [
    "sentences = [\n",
    "    \"This sentence is not too long but we are going to split it anyway.\",\n",
    "    \"This sentence is shorter but will still get split.\",\n",
    "    \"If it's not Love, then it's the Bomb that'll bring us together!\"\n",
    "]\n",
    "inputs = tokenizer(\n",
    "    sentences, truncation=True, return_overflowing_tokens=True, max_length=6, stride=2\n",
    ")\n",
    "\n",
    "print(inputs[\"overflow_to_sample_mapping\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9257467d-5379-4e8f-a5ca-b2dade012949",
   "metadata": {},
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "c572b6e2-99fd-4078-a349-6a7851308458",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(\n",
    "    question,\n",
    "    long_context,\n",
    "    stride=128,                        # stride wrt the model\n",
    "    max_length=384,                    # max_seq_len wrt the model\n",
    "    padding=\"longest\",                 # so we can treat this as a batch to build tensors\n",
    "    truncation=\"only_second\",          # only truncate the second input, the long_context\n",
    "    return_overflowing_tokens=True,\n",
    "    return_offsets_mapping=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "9b7a3596-98c4-4bc1-a7fc-97cab8a69596",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nðŸ¤— Transformers: State of the Art NLP\\n\\nðŸ¤— Transformers provides thousands of pretrained models to perform tasks on texts such as classification, information extraction,\\nquestion answering, summarization, translation, text generation and more in over 100 languages.\\nIts aim is to make cutting-edge NLP easier to use for everyone.\\n\\nðŸ¤— Transformers provides APIs to quickly download and use those pretrained models on a given text, fine-tune them on your own datasets and\\nthen share them with the community on our model hub. At the same time, each python module defining an architecture is fully standalone and\\ncan be modified to enable quick research experiments.\\n\\nWhy should I use transformers?\\n\\n1. Easy-to-use state-of-the-art models:\\n  - High performance on NLU and NLG tasks.\\n  - Low barrier to entry for educators and practitioners.\\n  - Few user-facing abstractions with just three classes to learn.\\n  - A unified API for using all our pretrained models.\\n  - Lower compute costs, smaller carbon footprint:\\n\\n2. Researchers can share trained models instead of always retraining.\\n  - Practitioners can reduce compute time and production costs.\\n  - Dozens of architectures with over 10,000 pretrained models, some in more than 100 languages.\\n\\n3. Choose the right framework for every part of a model's lifetime:\\n  - Train state-of-the-art models in 3 lines of code.\\n  - Move a single model between TF2.0/PyTorch frameworks at will.\\n  - Seamlessly pick the right framework for training, evaluation and production.\\n\\n4. Easily customize a model or an example to your needs:\\n  - We provide examples for each architecture to reproduce the results published by its original authors.\\n  - Model internals are exposed as consistently as possible.\\n  - Model files can be used independently of the library for quick experiments.\\n\\nðŸ¤— Transformers is backed by the three most popular deep learning libraries â€” Jax, PyTorch and TensorFlow â€” with a seamless integration\\nbetween them. It's straightforward to train your models with one before loading them for inference with the other.\\n\""
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "long_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "d0ca9482-c5bf-4d4d-b04a-acbf22b8fc08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 384])\n"
     ]
    }
   ],
   "source": [
    "_ = inputs.pop(\"overflow_to_sample_mapping\")\n",
    "offsets = inputs.pop(\"offset_mapping\")\n",
    "\n",
    "inputs = inputs.convert_to_tensors(\"pt\")\n",
    "print(inputs[\"input_ids\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "b6c7b241-f660-4ba0-9a82-0ca1a7c0ea21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 384]) torch.Size([2, 384])\n"
     ]
    }
   ],
   "source": [
    "outputs = model(**inputs)\n",
    "\n",
    "start_logits = outputs.start_logits\n",
    "end_logits = outputs.end_logits\n",
    "print(start_logits.shape, end_logits.shape)        # since our long_context was split into 2, we have 2 starts and 2 ends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "45e5020f-486d-462e-846d-6f3d24a87200",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_ids = inputs.sequence_ids()\n",
    "\n",
    "# Mask everything apart from the tokens of the context\n",
    "mask = [i != 1 for i in sequence_ids]\n",
    "\n",
    "# Unmask the [CLS] token\n",
    "mask[0] = False\n",
    "\n",
    "# Mask all the [PAD] tokens\n",
    "mask = torch.logical_or(torch.tensor(mask)[None], (inputs[\"attention_mask\"] == 0))\n",
    "\n",
    "start_logits[mask] = -10000\n",
    "end_logits[mask] = -10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "bec28499-27ab-4077-884e-b2963fcdfac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_probabilities = torch.nn.functional.softmax(start_logits, dim=-1)\n",
    "end_probabilities = torch.nn.functional.softmax(end_logits, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "1ee35205-73e5-4415-9b42-28897a15f5c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 18, 0.33866992592811584), (173, 184, 0.9714868664741516)]\n"
     ]
    }
   ],
   "source": [
    "candidates = []\n",
    "for start_probs, end_probs in zip(start_probabilities, end_probabilities):\n",
    "    scores = start_probs[:, None] * end_probs[None, :]\n",
    "    idx = torch.triu(scores).argmax().item()\n",
    "\n",
    "    #start_idx = idx // scores.shape[1]\n",
    "    #end_idx = idx % scores.shape[1]\n",
    "    start_idx, end_idx = divmod(idx, scores.shape[1])\n",
    "    \n",
    "    score = scores[start_idx, end_idx].item()\n",
    "    candidates.append((start_idx, end_idx, score))\n",
    "\n",
    "print(candidates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "84f2d592-8f02-47d8-8ace-06068b87be76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Which deep learning libraries back ðŸ¤— Transformers?\n",
      "\n",
      "{'answer': '\\nðŸ¤— Transformers: State of the Art NLP', 'start': 0, 'end': 37, 'score': 0.33866992592811584}\n",
      "{'answer': 'Jax, PyTorch and TensorFlow', 'start': 1892, 'end': 1919, 'score': 0.9714868664741516}\n"
     ]
    }
   ],
   "source": [
    "print(question)\n",
    "print()\n",
    "\n",
    "for candidate, offset in zip(candidates, offsets):\n",
    "    start_token, end_token, score = candidate\n",
    "    start_char, _ = offset[start_token]\n",
    "    _, end_char = offset[end_token]\n",
    "    answer = long_context[start_char:end_char]\n",
    "    result = {\"answer\": answer, \"start\": start_char, \"end\": end_char, \"score\": score}\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "667d094c-3666-4f59-8fc3-bdcf7c2e990b",
   "metadata": {},
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdc7dfc5-ef53-4b33-8a80-63f83d9be2b5",
   "metadata": {},
   "source": [
    "##### âœï¸ Try it out! (extra-curricular work)\n",
    "\n",
    "> Adapt the code above to return the scores and spans for the five most likely answers (in total, not per chunk).\n",
    "\n",
    "> Use the best scores you computed before to show the five most likely answers\n",
    "> (for the whole context, not each chunk).\n",
    "> To check your results, go back to the first pipeline and pass in `top_k=5` when calling it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b22e552f-3129-42b3-a89d-cdd47e217fb3",
   "metadata": {},
   "source": [
    "##### Naive implementation for `top_k=5` for QA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "1604379a-2791-4d2c-978c-579a834d45a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 0.9714868664741516,\n",
       "  'start': 1892,\n",
       "  'end': 1919,\n",
       "  'answer': 'Jax, PyTorch and TensorFlow'},\n",
       " {'score': 0.14949451386928558,\n",
       "  'start': 1153,\n",
       "  'end': 1182,\n",
       "  'answer': 'of architectures with over 10'},\n",
       " {'score': 0.015565164387226105,\n",
       "  'start': 1892,\n",
       "  'end': 1921,\n",
       "  'answer': 'Jax, PyTorch and TensorFlow â€”'},\n",
       " {'score': 0.013705423101782799,\n",
       "  'start': 1175,\n",
       "  'end': 1182,\n",
       "  'answer': 'over 10'},\n",
       " {'score': 0.007162676192820072,\n",
       "  'start': 1847,\n",
       "  'end': 1919,\n",
       "  'answer': 'three most popular deep learning libraries â€” Jax, PyTorch and TensorFlow'}]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = []\n",
    "\n",
    "for i, (start_probs, end_probs) in enumerate(list(zip(start_probabilities, end_probabilities))):\n",
    "    \n",
    "    scores = start_probs[:, None] * end_probs[None, :]\n",
    "    \n",
    "    values, indices = torch.topk(scores.flatten(), 5)\n",
    "    \n",
    "    start_and_end = [\n",
    "        divmod(max_index, scores.shape[1])\n",
    "        for max_index in indices.tolist()\n",
    "    ]\n",
    "    \n",
    "    for (start_token, end_token), score in zip(start_and_end, values):\n",
    "        if start_token != 0 and end_token != 0:\n",
    "            #print(i)\n",
    "            start_char, _ = offset[start_token]\n",
    "            _, end_char = offset[end_token]\n",
    "            results.append({\n",
    "                'score': score.item(),\n",
    "                'start': start_char,\n",
    "                'end': end_char,\n",
    "                'answer': long_context[start_char:end_char]              \n",
    "            })\n",
    "\n",
    "results = sorted(results, key=lambda r: r['score'], reverse=True)[:5]\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aad073d-bea2-497e-91a7-f893e8a98fa4",
   "metadata": {},
   "source": [
    "##### Specifying `top_k=5` in `pipeline(\"question-answering\")`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "f2cd115a-a841-4f01-813e-a714ee087bd4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 0.9714871048927307,\n",
       "  'start': 1892,\n",
       "  'end': 1919,\n",
       "  'answer': 'Jax, PyTorch and TensorFlow'},\n",
       " {'score': 0.1494959443807602,\n",
       "  'start': 17,\n",
       "  'end': 37,\n",
       "  'answer': 'State of the Art NLP'},\n",
       " {'score': 0.015565137378871441,\n",
       "  'start': 1892,\n",
       "  'end': 1921,\n",
       "  'answer': 'Jax, PyTorch and TensorFlow â€”'},\n",
       " {'score': 0.013705498538911343, 'start': 34, 'end': 37, 'answer': 'NLP'},\n",
       " {'score': 0.010596820153295994,\n",
       "  'start': 3,\n",
       "  'end': 37,\n",
       "  'answer': 'Transformers: State of the Art NLP'}]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question_answerer(question=question, context=long_context, top_k=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8cff94d-9cd5-4a54-b9ee-ff0225fdf8ed",
   "metadata": {},
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1a7ff56-dbb6-4380-b21d-ffd00f364248",
   "metadata": {},
   "source": [
    "#### Normalization and pre-tokenization\n",
    "\n",
    "> Before splitting a text into subtokens (according to its model), the tokenizer performs two steps: _normalization_ and _pre-tokenization._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bce13e93-7d24-4fe3-b50d-82202a0d3b59",
   "metadata": {},
   "source": [
    "##### Normalization\n",
    "\n",
    "> ... general cleanup, such as removing needless whitespace, lowercasing, and/or removing accents\n",
    "\n",
    "* Fast tokenizers provide access to the underlying tokenization implementation logic via the `backend_tokenzier` attribute\n",
    "* `backend_tokenizer.normalizer.normalize_str` method lets us see how an input string is normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "36ba8b03-4477-42b5-9e14-870bee82d2f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tokenizers.Tokenizer'>\n",
      "\n",
      "hello how are u?\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "print(type(tokenizer.backend_tokenizer))\n",
    "print()\n",
    "print(tokenizer.backend_tokenizer.normalizer.normalize_str(\"HÃ©llÃ² hÃ´w are Ã¼?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "701137f5-1d74-4a36-aee6-8cf6d2e0681c",
   "metadata": {},
   "source": [
    "> âœï¸ Try it out! Load a tokenizer from the `bert-base-cased` checkpoint and pass the same example to it.<br/> What are the main differences you can see between the cased and uncased versions of the tokenizer (normalization)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "d109556f-5c05-4b81-8161-b8d985e79137",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tokenizers.Tokenizer'>\n",
      "\n",
      "HÃ©llÃ² hÃ´w are Ã¼?\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "print(type(tokenizer.backend_tokenizer))\n",
    "print()\n",
    "print(tokenizer.backend_tokenizer.normalizer.normalize_str(\"HÃ©llÃ² hÃ´w are Ã¼?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dede9a7-3843-415f-a1ba-ce3060f96749",
   "metadata": {},
   "source": [
    "##### Unicode normalization\n",
    "\n",
    "Some tokenizer schemes also do Unicode normalization. There are four Normalization Forms:\n",
    "\n",
    "1. Normalization Form D (NFD): canonical decomposition\n",
    "2. Normalization Form C (NFC): canonical decomposition, followed by canonical composition\n",
    "3. Normalization Form KD (NFKD): compatibility decomposition\n",
    "4. Normalization Form KC (NFKC): compatibility decomposition, followed by canonical composition\n",
    "\n",
    "Please see [Normalization Forms](https://www.unicode.org/reports/tr15/#Norm_Forms) at unicode.org\n",
    "\n",
    "If this is important to your task, then **you better be sure to choose the right tokenizer**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "5b51d283-0e87-42d5-aa5f-15625de88115",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ã§\n"
     ]
    }
   ],
   "source": [
    "print('\\u00E7')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "0c8dda3c-5dbe-4d4e-abf0-69fad8dc0339",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c Ì§\n",
      "\n",
      "cÌ§\n"
     ]
    }
   ],
   "source": [
    "print('\\u0063', '\\u0327')\n",
    "print()\n",
    "print('\\u0063\\u0327')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1327905d-0bbb-46cb-bc50-6685221e0604",
   "metadata": {},
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea529f3a-8ec8-4f8d-8bca-91777006a45b",
   "metadata": {},
   "source": [
    "##### Pre-tokenization\n",
    "\n",
    "Performed after the normalization step, pre-tokenization applies rules for an initial division of the input text. These rules do not need to be learned.\n",
    "\n",
    "* A Fast tokenizer's `backend_tokenizer.pre_tokenizer.pre_tokenize_str` method lets us see how an input string is initially divided into tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "29d09d18-2b1e-448e-a608-e967fa788751",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"3.2.1: let's get started!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "99af4acd-e06f-4b9c-be16-aab22a5936b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('3', (0, 1)),\n",
       " ('.', (1, 2)),\n",
       " ('2', (2, 3)),\n",
       " ('.', (3, 4)),\n",
       " ('1', (4, 5)),\n",
       " (':', (5, 6)),\n",
       " ('Ä let', (6, 10)),\n",
       " (\"'s\", (10, 12)),\n",
       " ('Ä get', (12, 16)),\n",
       " ('Ä started', (16, 24)),\n",
       " ('!', (24, 25))]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# gpt2\n",
    "AutoTokenizer.from_pretrained(\"gpt2\").backend_tokenizer.pre_tokenizer.pre_tokenize_str(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "4e5faaae-ab24-4558-a507-3cdee1bbcc84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18a638a7aae24244adf4ce1db6925387",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f78572bdf7b04876b81e986f971eb7da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/684 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abd1221e4cc44d2fb8c2b042e3b6e26f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/760k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56d83280198d4b58889b48bb4715501f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.31M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[('â–3.2.1:', (0, 6)),\n",
       " (\"â–let's\", (7, 12)),\n",
       " ('â–get', (13, 16)),\n",
       " ('â–started!', (17, 25))]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# albert-base-v1\n",
    "AutoTokenizer.from_pretrained(\"albert-base-v1\").backend_tokenizer.pre_tokenizer.pre_tokenize_str(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "d3c62df2-4adb-4c9e-be14-0b690b079482",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('3', (0, 1)),\n",
       " ('.', (1, 2)),\n",
       " ('2', (2, 3)),\n",
       " ('.', (3, 4)),\n",
       " ('1', (4, 5)),\n",
       " (':', (5, 6)),\n",
       " ('let', (7, 10)),\n",
       " (\"'\", (10, 11)),\n",
       " ('s', (11, 12)),\n",
       " ('get', (13, 16)),\n",
       " ('started', (17, 24)),\n",
       " ('!', (24, 25))]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# bert-base-uncased\n",
    "AutoTokenizer.from_pretrained(\"bert-base-uncased\").backend_tokenizer.pre_tokenizer.pre_tokenize_str(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08977fd3-84a4-4fa9-8485-dc53f2d4ffe1",
   "metadata": {},
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b9afe2e-337f-4749-b37b-e2db59fbf4a4",
   "metadata": {},
   "source": [
    "##### SentencePiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "4be468a2-71e2-4430-96d3-33b548aecd58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('â–æ—¥æœ¬èªžã ã‚ˆã€ã“ã‚ŒãŒã€‚', (0, 10))]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AutoTokenizer.from_pretrained(\"albert-base-v1\").backend_tokenizer.pre_tokenizer.pre_tokenize_str('æ—¥æœ¬èªžã ã‚ˆã€ã“ã‚ŒãŒã€‚')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55bea310-1b29-4927-a016-0c3a935a8609",
   "metadata": {},
   "source": [
    "#### Byte-Pair Encoding tokenization\n",
    "\n",
    "##### Implementing BPE\n",
    "\n",
    "... a naive approach..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "79e9cd56-af6c-46e5-a6ef-53fa651c48d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\n",
    "    \"This is the Hugging Face Course.\",\n",
    "    \"This chapter is about tokenization.\",\n",
    "    \"This section shows several tokenizer algorithms.\",\n",
    "    \"Hopefully, you will be able to understand how they are trained and generate tokens.\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "cd162ba3-9bc6-4ebf-99e1-840fc347fc3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "9797a800-c5f2-4a7e-92f7-0409666749ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'int'>, {'This': 3, 'Ä is': 2, 'Ä the': 1, 'Ä Hugging': 1, 'Ä Face': 1, 'Ä Course': 1, '.': 4, 'Ä chapter': 1, 'Ä about': 1, 'Ä tokenization': 1, 'Ä section': 1, 'Ä shows': 1, 'Ä several': 1, 'Ä tokenizer': 1, 'Ä algorithms': 1, 'Hopefully': 1, ',': 1, 'Ä you': 1, 'Ä will': 1, 'Ä be': 1, 'Ä able': 1, 'Ä to': 1, 'Ä understand': 1, 'Ä how': 1, 'Ä they': 1, 'Ä are': 1, 'Ä trained': 1, 'Ä and': 1, 'Ä generate': 1, 'Ä tokens': 1})\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "word_freqs = defaultdict(int)\n",
    "\n",
    "for text in corpus:\n",
    "    words_with_offsets = tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(text)\n",
    "    new_words = [word for word, offset in words_with_offsets]\n",
    "    for word in new_words:\n",
    "        word_freqs[word] += 1\n",
    "\n",
    "print(word_freqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "001442ef-5e50-400f-b488-5af1aca43dcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[',', '.', 'C', 'F', 'H', 'T', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'k', 'l', 'm', 'n', 'o', 'p', 'r', 's', 't', 'u', 'v', 'w', 'y', 'z', 'Ä ']\n"
     ]
    }
   ],
   "source": [
    "alphabet = []\n",
    "\n",
    "for word in word_freqs.keys():\n",
    "    for letter in word:\n",
    "        if letter not in alphabet:\n",
    "            alphabet.append(letter)\n",
    "alphabet.sort()\n",
    "\n",
    "print(alphabet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "62a15785-4671-430b-9aec-75a76497e3ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<|endoftext|>', ',', '.', 'C', 'F', 'H', 'T', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'k', 'l', 'm', 'n', 'o', 'p', 'r', 's', 't', 'u', 'v', 'w', 'y', 'z', 'Ä ']\n"
     ]
    }
   ],
   "source": [
    "vocab = [\"<|endoftext|>\"] + alphabet.copy()\n",
    "\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "672865bf-7827-40d6-8da2-22809e046aaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'This': ['T', 'h', 'i', 's'], 'Ä is': ['Ä ', 'i', 's'], 'Ä the': ['Ä ', 't', 'h', 'e'], 'Ä Hugging': ['Ä ', 'H', 'u', 'g', 'g', 'i', 'n', 'g'], 'Ä Face': ['Ä ', 'F', 'a', 'c', 'e'], 'Ä Course': ['Ä ', 'C', 'o', 'u', 'r', 's', 'e'], '.': ['.'], 'Ä chapter': ['Ä ', 'c', 'h', 'a', 'p', 't', 'e', 'r'], 'Ä about': ['Ä ', 'a', 'b', 'o', 'u', 't'], 'Ä tokenization': ['Ä ', 't', 'o', 'k', 'e', 'n', 'i', 'z', 'a', 't', 'i', 'o', 'n'], 'Ä section': ['Ä ', 's', 'e', 'c', 't', 'i', 'o', 'n'], 'Ä shows': ['Ä ', 's', 'h', 'o', 'w', 's'], 'Ä several': ['Ä ', 's', 'e', 'v', 'e', 'r', 'a', 'l'], 'Ä tokenizer': ['Ä ', 't', 'o', 'k', 'e', 'n', 'i', 'z', 'e', 'r'], 'Ä algorithms': ['Ä ', 'a', 'l', 'g', 'o', 'r', 'i', 't', 'h', 'm', 's'], 'Hopefully': ['H', 'o', 'p', 'e', 'f', 'u', 'l', 'l', 'y'], ',': [','], 'Ä you': ['Ä ', 'y', 'o', 'u'], 'Ä will': ['Ä ', 'w', 'i', 'l', 'l'], 'Ä be': ['Ä ', 'b', 'e'], 'Ä able': ['Ä ', 'a', 'b', 'l', 'e'], 'Ä to': ['Ä ', 't', 'o'], 'Ä understand': ['Ä ', 'u', 'n', 'd', 'e', 'r', 's', 't', 'a', 'n', 'd'], 'Ä how': ['Ä ', 'h', 'o', 'w'], 'Ä they': ['Ä ', 't', 'h', 'e', 'y'], 'Ä are': ['Ä ', 'a', 'r', 'e'], 'Ä trained': ['Ä ', 't', 'r', 'a', 'i', 'n', 'e', 'd'], 'Ä and': ['Ä ', 'a', 'n', 'd'], 'Ä generate': ['Ä ', 'g', 'e', 'n', 'e', 'r', 'a', 't', 'e'], 'Ä tokens': ['Ä ', 't', 'o', 'k', 'e', 'n', 's']}\n"
     ]
    }
   ],
   "source": [
    "splits = {word: [c for c in word] for word in word_freqs.keys()}\n",
    "\n",
    "print(splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "d45d9d71-5d42-49b7-9657-22f24fd3d6df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_pair_freqs(splits):\n",
    "    pair_freqs = defaultdict(int)\n",
    "    for word, freq in word_freqs.items():\n",
    "        split = splits[word]\n",
    "        if len(split) == 1:\n",
    "            continue\n",
    "        for i in range(len(split) - 1):\n",
    "            pair = (split[i], split[i + 1])\n",
    "            pair_freqs[pair] += freq\n",
    "    return pair_freqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "d0edb778-8f98-40a5-af50-7dc5865b8739",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('T', 'h'): 3\n",
      "('h', 'i'): 3\n",
      "('i', 's'): 5\n",
      "('Ä ', 'i'): 2\n",
      "('Ä ', 't'): 7\n",
      "('t', 'h'): 3\n"
     ]
    }
   ],
   "source": [
    "pair_freqs = compute_pair_freqs(splits)\n",
    "\n",
    "for i, key in enumerate(pair_freqs.keys()):\n",
    "    print(f\"{key}: {pair_freqs[key]}\")\n",
    "    if i >= 5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "31e90bf4-a47c-405b-859c-df530a8fe13d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Ä ', 't') 7\n"
     ]
    }
   ],
   "source": [
    "best_pair = \"\"\n",
    "max_freq = None\n",
    "\n",
    "for pair, freq in pair_freqs.items():\n",
    "    if max_freq is None or max_freq < freq:\n",
    "        best_pair = pair\n",
    "        max_freq = freq\n",
    "\n",
    "print(best_pair, max_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "7281b819-5960-4c55-a7e5-64556e8b4be1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('Ä ', 't'), 7),\n",
       " (('Ä ', 'a'), 5),\n",
       " (('e', 'r'), 5),\n",
       " (('i', 's'), 5),\n",
       " (('e', 'n'), 4)]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_pair_freqs = sorted(pair_freqs.items(), key=lambda kv: kv[1])\n",
    "sorted_pair_freqs[-5:][::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "57131e54-9c0d-4a1c-8a84-6936253a0a79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<|endoftext|>', ',', '.', 'C', 'F', 'H', 'T', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'k', 'l', 'm', 'n', 'o', 'p', 'r', 's', 't', 'u', 'v', 'w', 'y', 'z', 'Ä ', 'Ä t']\n"
     ]
    }
   ],
   "source": [
    "merges = {(\"Ä \", \"t\"): \"Ä t\"}\n",
    "\n",
    "vocab.append(\"Ä t\")\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "e24b3da6-6d40-4714-844a-195a78cb81aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_pair(a, b, splits):\n",
    "    for word in word_freqs:\n",
    "        split = splits[word]\n",
    "        if len(split) == 1:\n",
    "            continue\n",
    "\n",
    "        i = 0\n",
    "        while i < len(split) - 1:\n",
    "            if split[i] == a and split[i + 1] == b:\n",
    "                split = split[:i] + [a + b] + split[i + 2 :]\n",
    "            else:\n",
    "                i += 1\n",
    "        splits[word] = split\n",
    "    return splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "2dd2b6cf-3ada-445b-9db7-dc6df623fb09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Ä t', 'r', 'a', 'i', 'n', 'e', 'd']\n"
     ]
    }
   ],
   "source": [
    "splits = merge_pair(\"Ä \", \"t\", splits)\n",
    "\n",
    "print(splits[\"Ä trained\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "79e94a3f-3151-4a5a-a1af-1bacb1e56649",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 50\n",
    "\n",
    "while len(vocab) < vocab_size:\n",
    "    pair_freqs = compute_pair_freqs(splits)\n",
    "    best_pair = \"\"\n",
    "    max_freq = None\n",
    "    for pair, freq in pair_freqs.items():\n",
    "        if max_freq is None or max_freq < freq:\n",
    "            best_pair = pair\n",
    "            max_freq = freq\n",
    "    splits = merge_pair(*best_pair, splits)\n",
    "    merges[best_pair] = best_pair[0] + best_pair[1]\n",
    "    vocab.append(best_pair[0] + best_pair[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "dc00ec72-5290-4eab-b8f8-8d60dae0e20f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<|endoftext|>', ',', '.', 'C', 'F', 'H', 'T', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'k', 'l', 'm', 'n', 'o', 'p', 'r', 's', 't', 'u', 'v', 'w', 'y', 'z', 'Ä ', 'Ä t', 'is', 'er', 'Ä a', 'Ä to', 'en', 'Th', 'This', 'ou', 'se', 'Ä tok', 'Ä token', 'nd', 'Ä is', 'Ä th', 'Ä the', 'in', 'Ä ab', 'Ä tokeni']\n"
     ]
    }
   ],
   "source": [
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "64380997-78c1-42cb-8d29-ac6703bbdbb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{('Ä ', 't'): 'Ä t', ('i', 's'): 'is', ('e', 'r'): 'er', ('Ä ', 'a'): 'Ä a', ('Ä t', 'o'): 'Ä to', ('e', 'n'): 'en', ('T', 'h'): 'Th', ('Th', 'is'): 'This', ('o', 'u'): 'ou', ('s', 'e'): 'se', ('Ä to', 'k'): 'Ä tok', ('Ä tok', 'en'): 'Ä token', ('n', 'd'): 'nd', ('Ä ', 'is'): 'Ä is', ('Ä t', 'h'): 'Ä th', ('Ä th', 'e'): 'Ä the', ('i', 'n'): 'in', ('Ä a', 'b'): 'Ä ab', ('Ä token', 'i'): 'Ä tokeni'}\n"
     ]
    }
   ],
   "source": [
    "print(merges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "28235967-7391-498c-a457-e4674519c627",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    pre_tokenize_result = tokenizer._tokenizer.pre_tokenizer.pre_tokenize_str(text)\n",
    "    pre_tokenized_text = [word for word, offset in pre_tokenize_result]\n",
    "    splits = [[l for l in word] for word in pre_tokenized_text]\n",
    "    for pair, merge in merges.items():\n",
    "        for idx, split in enumerate(splits):\n",
    "            i = 0\n",
    "            while i < len(split) - 1:\n",
    "                if split[i] == pair[0] and split[i + 1] == pair[1]:\n",
    "                    split = split[:i] + [merge] + split[i + 2 :]\n",
    "                else:\n",
    "                    i += 1\n",
    "            splits[idx] = split\n",
    "\n",
    "    return sum(splits, [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "12b3cedd-d06d-44bb-8941-3d1aa4169d65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This', 'Ä is', 'Ä ', 'n', 'o', 't', 'Ä a', 'Ä token', '.']"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize(\"This is not a token.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec2f9f77-a979-48e7-800e-b66b15112f09",
   "metadata": {},
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "997f3459-dc24-42c8-a575-2ace7a63c498",
   "metadata": {},
   "source": [
    "> âš ï¸ Our implementation will throw an error if there is an unknown character since we didnâ€™t do anything to handle them. GPT-2 doesnâ€™t actually have an unknown token (itâ€™s impossible to get an unknown character when using byte-level BPE), but this could happen here because we did not include all possible bytes in the initial vocabulary.\n",
    "\n",
    "From Byte Level Text Representation, Encoding Byte-Level Representation, page 1 of [Neural Machine Translation with Byte-Level Subwords, Wnag, Cho & Gu, 2019](https://arxiv.org/pdf/1909.03341).\n",
    "> While there are 138K Unicode characters covering over 150 languages, we represent a sentence in any language as a sequence of UTF-8 bytes (248 out of 256 possible bytes)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81686341-ba31-45a3-ab75-6aff5e12381d",
   "metadata": {},
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dcf581b-a8a0-4960-bba1-91faf333f7ae",
   "metadata": {},
   "source": [
    "#### WordPiece tokenization\n",
    "\n",
    "> âš ï¸ Google never open-sourced its implementation of the training algorithm of WordPiece, so what follows is our best guess based on the published literature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "46bec6dd-5e4c-421c-8492-89eaecdb71f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\n",
    "    \"This is the Hugging Face Course.\",\n",
    "    \"This chapter is about tokenization.\",\n",
    "    \"This section shows several tokenizer algorithms.\",\n",
    "    \"Hopefully, you will be able to understand how they are trained and generate tokens.\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "293e93fb-cfe5-4663-bd05-a77ea73682bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "6825de25-22c2-4217-9eb6-bb10414776da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int,\n",
       "            {'This': 3,\n",
       "             'is': 2,\n",
       "             'the': 1,\n",
       "             'Hugging': 1,\n",
       "             'Face': 1,\n",
       "             'Course': 1,\n",
       "             '.': 4,\n",
       "             'chapter': 1,\n",
       "             'about': 1,\n",
       "             'tokenization': 1,\n",
       "             'section': 1,\n",
       "             'shows': 1,\n",
       "             'several': 1,\n",
       "             'tokenizer': 1,\n",
       "             'algorithms': 1,\n",
       "             'Hopefully': 1,\n",
       "             ',': 1,\n",
       "             'you': 1,\n",
       "             'will': 1,\n",
       "             'be': 1,\n",
       "             'able': 1,\n",
       "             'to': 1,\n",
       "             'understand': 1,\n",
       "             'how': 1,\n",
       "             'they': 1,\n",
       "             'are': 1,\n",
       "             'trained': 1,\n",
       "             'and': 1,\n",
       "             'generate': 1,\n",
       "             'tokens': 1})"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "word_freqs = defaultdict(int)\n",
    "for text in corpus:\n",
    "    words_with_offsets = tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(text)\n",
    "    new_words = [word for word, offset in words_with_offsets]\n",
    "    for word in new_words:\n",
    "        word_freqs[word] += 1\n",
    "\n",
    "word_freqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "084d1f97-d396-42fd-914a-0fca6b9993de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['##a', '##b', '##c', '##d', '##e', '##f', '##g', '##h', '##i', '##k', '##l', '##m', '##n', '##o', '##p', '##r', '##s', '##t', '##u', '##v', '##w', '##y', '##z', ',', '.', 'C', 'F', 'H', 'T', 'a', 'b', 'c', 'g', 'h', 'i', 's', 't', 'u', 'w', 'y']\n"
     ]
    }
   ],
   "source": [
    "alphabet = []\n",
    "for word in word_freqs.keys():\n",
    "    if word[0] not in alphabet:\n",
    "        alphabet.append(word[0])\n",
    "    for letter in word[1:]:\n",
    "        if f\"##{letter}\" not in alphabet:\n",
    "            alphabet.append(f\"##{letter}\")\n",
    "\n",
    "alphabet.sort()\n",
    "alphabet\n",
    "\n",
    "print(alphabet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "03d1ea4b-4a5b-4b46-aa56-af3cdaae3864",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = [\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"] + alphabet.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "8000e1c0-4c42-44b6-85c9-2f50b2255086",
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = {\n",
    "    word: [c if i == 0 else f\"##{c}\" for i, c in enumerate(word)]\n",
    "    for word in word_freqs.keys()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "013ed572-9cad-4c39-8c17-a8bd656253b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_pair_scores(splits):\n",
    "    letter_freqs = defaultdict(int)\n",
    "    pair_freqs = defaultdict(int)\n",
    "    for word, freq in word_freqs.items():\n",
    "        split = splits[word]\n",
    "        if len(split) == 1:\n",
    "            letter_freqs[split[0]] += freq\n",
    "            continue\n",
    "        for i in range(len(split) - 1):\n",
    "            pair = (split[i], split[i + 1])\n",
    "            letter_freqs[split[i]] += freq\n",
    "            pair_freqs[pair] += freq\n",
    "        letter_freqs[split[-1]] += freq\n",
    "\n",
    "    scores = {\n",
    "        pair: freq / (letter_freqs[pair[0]] * letter_freqs[pair[1]])\n",
    "        for pair, freq in pair_freqs.items()\n",
    "    }\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "fcc65a03-9e4b-4ff8-8338-ee67174005cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('T', '##h'): 0.125\n",
      "('##h', '##i'): 0.03409090909090909\n",
      "('##i', '##s'): 0.02727272727272727\n",
      "('i', '##s'): 0.1\n",
      "('t', '##h'): 0.03571428571428571\n",
      "('##h', '##e'): 0.011904761904761904\n"
     ]
    }
   ],
   "source": [
    "pair_scores = compute_pair_scores(splits)\n",
    "for i, key in enumerate(pair_scores.keys()):\n",
    "    print(f\"{key}: {pair_scores[key]}\")\n",
    "    if i >= 5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "0daa81a3-97f6-42a0-a8a3-d077c02fe889",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('a', '##b') 0.2\n"
     ]
    }
   ],
   "source": [
    "best_pair = \"\"\n",
    "max_score = None\n",
    "for pair, score in pair_scores.items():\n",
    "    if max_score is None or max_score < score:\n",
    "        best_pair = pair\n",
    "        max_score = score\n",
    "\n",
    "print(best_pair, max_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "e9398b62-4f09-4186-abf1-bfe6431087be",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab.append(\"ab\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "7b9e4b66-581e-45dd-b1f1-14d033fb525f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_pair(a, b, splits):\n",
    "    for word in word_freqs:\n",
    "        split = splits[word]\n",
    "        if len(split) == 1:\n",
    "            continue\n",
    "        i = 0\n",
    "        while i < len(split) - 1:\n",
    "            if split[i] == a and split[i + 1] == b:\n",
    "                merge = a + b[2:] if b.startswith(\"##\") else a + b\n",
    "                split = split[:i] + [merge] + split[i + 2 :]\n",
    "            else:\n",
    "                i += 1\n",
    "        splits[word] = split\n",
    "    return splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "8f358c86-65f1-481b-8857-05cfe1ea880e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ab', '##o', '##u', '##t']"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splits = merge_pair(\"a\", \"##b\", splits)\n",
    "splits[\"about\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "4d692a7a-cdbb-4121-93e3-fe13f9b0e96f",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 70\n",
    "while len(vocab) < vocab_size:\n",
    "    scores = compute_pair_scores(splits)\n",
    "    best_pair, max_score = \"\", None\n",
    "    for pair, score in scores.items():\n",
    "        if max_score is None or max_score < score:\n",
    "            best_pair = pair\n",
    "            max_score = score\n",
    "    splits = merge_pair(*best_pair, splits)\n",
    "    new_token = (\n",
    "        best_pair[0] + best_pair[1][2:]\n",
    "        if best_pair[1].startswith(\"##\")\n",
    "        else best_pair[0] + best_pair[1]\n",
    "    )\n",
    "    vocab.append(new_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "06841b9b-74d7-4728-96eb-af4e011a8980",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[PAD]', '[UNK]', '[CLS]', '[SEP]', '[MASK]', '##a', '##b', '##c', '##d', '##e', '##f', '##g', '##h', '##i', '##k', '##l', '##m', '##n', '##o', '##p', '##r', '##s', '##t', '##u', '##v', '##w', '##y', '##z', ',', '.', 'C', 'F', 'H', 'T', 'a', 'b', 'c', 'g', 'h', 'i', 's', 't', 'u', 'w', 'y', 'ab', '##fu', 'Fa', 'Fac', '##ct', '##ful', '##full', '##fully', 'Th', 'ch', '##hm', 'cha', 'chap', 'chapt', '##thm', 'Hu', 'Hug', 'Hugg', 'sh', 'th', 'is', '##thms', '##za', '##zat', '##ut']\n"
     ]
    }
   ],
   "source": [
    "print(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d50db8ed-65c1-485d-8ad3-ae6dfe1ae7e8",
   "metadata": {},
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "4c250dba-af27-494c-ad8f-185ea74adc5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_word(word):\n",
    "    tokens = []\n",
    "    while len(word) > 0:\n",
    "        i = len(word)\n",
    "        while i > 0 and word[:i] not in vocab:\n",
    "            i -= 1\n",
    "        if i == 0:\n",
    "            return [\"[UNK]\"]\n",
    "        tokens.append(word[:i])\n",
    "        word = word[i:]\n",
    "        if len(word) > 0:\n",
    "            word = f\"##{word}\"\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "f5b7fe7e-b53c-4319-89b7-3f650454da2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hugg', '##i', '##n', '##g']\n",
      "['[UNK]']\n"
     ]
    }
   ],
   "source": [
    "print(encode_word(\"Hugging\"))\n",
    "print(encode_word(\"HOgging\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "8a155bab-d561-4e78-b7ab-c6ee5af1d967",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    pre_tokenize_result = tokenizer._tokenizer.pre_tokenizer.pre_tokenize_str(text)\n",
    "    pre_tokenized_text = [word for word, offset in pre_tokenize_result]\n",
    "    encoded_words = [encode_word(word) for word in pre_tokenized_text]\n",
    "    return sum(encoded_words, [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "ec4de5c6-da6f-484e-9cdb-8808c078971a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Th',\n",
       " '##i',\n",
       " '##s',\n",
       " 'is',\n",
       " 'th',\n",
       " '##e',\n",
       " 'Hugg',\n",
       " '##i',\n",
       " '##n',\n",
       " '##g',\n",
       " 'Fac',\n",
       " '##e',\n",
       " 'c',\n",
       " '##o',\n",
       " '##u',\n",
       " '##r',\n",
       " '##s',\n",
       " '##e',\n",
       " '[UNK]']"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize(\"This is the Hugging Face course!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fbb74dd-5201-4aa0-b850-66199c6986a8",
   "metadata": {},
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa34ac2f-6666-4351-a80c-30b8e6345648",
   "metadata": {},
   "source": [
    "#### Building a tokenizer, block by block"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41e0f8f1-a61e-4d25-94e5-4bdd2695087d",
   "metadata": {},
   "source": [
    "##### Acquiring a corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "4be36fe3-a673-438e-88ab-44163f137d40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6b8f4468a13462989ced7711ca3c21b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/10.5k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7b5c2132f1242eea1fa64b7537de68a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "test-00000-of-00001.parquet:   0%|          | 0.00/733k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0945100c64aa4e8587c2e2dd3c1d4a10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00000-of-00001.parquet:   0%|          | 0.00/6.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "370ac9e864a74f518dc5b943f3560ea1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "validation-00000-of-00001.parquet:   0%|          | 0.00/657k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5739519f5cc74aa5a9a0000c61d38c0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/4358 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3edc5726b5c414ca5667fdc3c68ef28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/36718 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0454650574514c6abe8e611962800bea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/3760 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"wikitext\", name=\"wikitext-2-raw-v1\", split=\"train\")\n",
    "\n",
    "def get_training_corpus():\n",
    "    for i in range(0, len(dataset), 1000):\n",
    "        yield dataset[i : i + 1000][\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "e578786c-9b0d-4fd5-b523-2eca08f406f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"wikitext-2.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for i in range(len(dataset)):\n",
    "        f.write(dataset[i][\"text\"] + \"\\n\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "4ec3681f-5349-49f2-a020-4bfe4be85478",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " = Valkyria Chronicles III = \n",
      "\n",
      "\n",
      " SenjÅ no Valkyria 3 : Unrecorded Chronicles ( Japanese : æˆ¦å ´ã®ãƒ´ã‚¡ãƒ«ã‚­ãƒ¥ãƒªã‚¢3 , lit . Valkyria of the Battlefield 3 ) , commonly referred to as Valkyria Chronicles III outside Japan , is a tactical role @-@ playing video game developed by Sega and Media.Vision for the PlayStation Portable . Released in January 2011 in Japan , it is the third game in the Valkyria series . Employing the same fusion of tactical and real @-@ time gameplay as its predecessors , the story runs parallel to the first game and follows the \" Nameless \" , a penal military unit serving the nation of Gallia during the Second Europan War who perform secret black operations and are pitted against the Imperial unit \" Calamaty Raven \" . \n",
      "\n",
      " The game began development in 2010 , carrying over a large portion of the work done on Valkyria Chronicles II . While it retained the standard features of the series , it also underwent multiple adjustments , such as making the game more forgiving for series newcomers . Character designer Raita Honjou and composer Hitoshi Sakimoto both returned from previous entries , along with Valkyria Chronicles II director Takeshi Ozawa . A large team of writers handled the script . The game 's opening theme was sung by May 'n . \n",
      "\n",
      " It met with positive sales in Japan , and was praised by both Japanese and western critics . After release , it received downloadable content , along with an expanded edition in November of that year . It was also adapted into manga and an original video animation series . Due to low sales of Valkyria Chronicles II , Valkyria Chronicles III was not localized , but a fan translation compatible with the game 's expanded edition was released in 2014 . Media.Vision would return to the franchise with the development of Valkyria : Azure Revolution for the PlayStation 4 . \n",
      "\n"
     ]
    }
   ],
   "source": [
    "!head wikitext-2.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e33192f1-2fda-439e-8e52-eff42a92224d",
   "metadata": {},
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f80cfe5b-004b-4ee7-b371-f6e799fd2297",
   "metadata": {},
   "source": [
    "##### Building a WordPiece tokenizer from scratch\n",
    "\n",
    "General steps are:\n",
    "\n",
    "1. instantiate a `Tokenizer` object with a `model`\n",
    "2. specify its `normalizer`\n",
    "3. specify its `pre_tokenizer`\n",
    "4. specify its `post_processor`\n",
    "5. and also a `decoder`\n",
    "\n",
    "for your custom tokenizer's attributes. Also,\n",
    "\n",
    "> We have to specify the `unk_token` so the model knows what to return when it encounters characters it hasnâ€™t seen before. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "80f81b90-a8a1-4f0b-b76b-96851e8bb6c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import (\n",
    "    decoders,\n",
    "    models,\n",
    "    normalizers,\n",
    "    pre_tokenizers,\n",
    "    processors,\n",
    "    trainers,\n",
    "    Tokenizer,\n",
    ")\n",
    "\n",
    "tokenizer = Tokenizer(models.WordPiece(unk_token=\"[UNK]\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8334b51b-74ac-44a7-a8be-fb07ccfafdc7",
   "metadata": {},
   "source": [
    "normalization..."
   ]
  },
  {
   "cell_type": "raw",
   "id": "89ea569e-b3be-4935-b958-a423100a2ce3",
   "metadata": {},
   "source": [
    "tokenizer.normalizer = normalizers.BertNormalizer(lowercase=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65800fc9-a9f5-46ff-9f71-cb56c6d7b026",
   "metadata": {},
   "source": [
    "> The library provides a `Lowercase` normalizer and a `StripAccents` normalizer,\n",
    "> and you can compose several normalizers using a `Sequence`...<br/>\n",
    "> We're also using an `NFD` Unicode normalizer, as otherwise the `StripAccents` normalizer\n",
    "> wonâ€™t properly recognize the accented characters and thus wonâ€™t strip them out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "aac48c3c-3a82-4527-a689-51c70502adb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.normalizer = normalizers.Sequence([\n",
    "    normalizers.NFD(), \n",
    "    normalizers.Lowercase(), \n",
    "    normalizers.StripAccents()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "7801d969-ade4-45b1-8ab4-9bc67ee208ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello how are u?\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.normalizer.normalize_str(\"HÃ©llÃ² hÃ´w are Ã¼?\"))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6d28b912-31bb-41fd-b9d8-76d48ef3b906",
   "metadata": {},
   "source": [
    "tokenizer.pre_tokenizer = pre_tokenizers.BertPreTokenizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90715273-248c-49a7-ab60-ab06bc3fe32d",
   "metadata": {},
   "source": [
    "> ... the `Whitespace` pre-tokenizer splits on whitespace and all characters that are not letters,\n",
    "> digits, or the underscore character, so it technically splits on whitespace and punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "04f64bb8-bf4d-45be-a395-29becb46fc95",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.pre_tokenizer = pre_tokenizers.Whitespace()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "53f4a0f7-a337-4658-8c7a-2df644ecb793",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Let', (0, 3)),\n",
       " (\"'\", (3, 4)),\n",
       " ('s', (4, 5)),\n",
       " ('test', (6, 10)),\n",
       " ('my', (11, 13)),\n",
       " ('pre', (14, 17)),\n",
       " ('-', (17, 18)),\n",
       " ('tokenizer', (18, 27)),\n",
       " ('.', (27, 28))]"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.pre_tokenizer.pre_tokenize_str(\"Let's test my pre-tokenizer.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "daacc6b0-1131-483c-8c07-f9848a5b8ba5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(\"Let's\", (0, 5)),\n",
       " ('test', (6, 10)),\n",
       " ('my', (11, 13)),\n",
       " ('pre-tokenizer.', (14, 28))]"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre_tokenizer = pre_tokenizers.WhitespaceSplit()\n",
    "\n",
    "pre_tokenizer.pre_tokenize_str(\"Let's test my pre-tokenizer.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dd5dedf-3a1b-4661-8dd6-5dbcc2d12c96",
   "metadata": {},
   "source": [
    "> As with ... normalizers, you can use a `Sequence` to compose several pre-tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "19d07441-ac2d-47e6-8098-b5581569d9e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Let', (0, 3)),\n",
       " (\"'\", (3, 4)),\n",
       " ('s', (4, 5)),\n",
       " ('test', (6, 10)),\n",
       " ('my', (11, 13)),\n",
       " ('pre', (14, 17)),\n",
       " ('-', (17, 18)),\n",
       " ('tokenizer', (18, 27)),\n",
       " ('.', (27, 28))]"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre_tokenizer = pre_tokenizers.Sequence([\n",
    "    pre_tokenizers.WhitespaceSplit(), \n",
    "    pre_tokenizers.Punctuation()\n",
    "])\n",
    "\n",
    "pre_tokenizer.pre_tokenize_str(\"Let's test my pre-tokenizer.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "602e06a8-768c-4695-808a-64bb8722bccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "special_tokens = [\"[UNK]\", \"[PAD]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"]\n",
    "\n",
    "trainer = trainers.WordPieceTrainer(vocab_size=25000, special_tokens=special_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "8bff963e-d76d-40dc-871b-7cd37228ddb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer.train_from_iterator(get_training_corpus(), trainer=trainer)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a8717c91-95dc-4971-8bf8-f3720e41a1bf",
   "metadata": {},
   "source": [
    "tokenizer.model = models.WordPiece(unk_token=\"[UNK]\")\n",
    "\n",
    "tokenizer.train([\"wikitext-2.txt\"], trainer=trainer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f327a087-fa13-4984-8e07-d668a5e8ca0b",
   "metadata": {},
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "a561e5be-7632-4a9b-b9d0-82f732e422ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['let', \"'\", 's', 'test', 'this', 'tok', '##eni', '##zer', '.']\n"
     ]
    }
   ],
   "source": [
    "encoding = tokenizer.encode(\"Let's test this tokenizer.\")\n",
    "\n",
    "print(encoding.tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "18862cb5-5120-4037-a7ab-1742581bfb01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 3\n"
     ]
    }
   ],
   "source": [
    "cls_token_id = tokenizer.token_to_id(\"[CLS]\")\n",
    "sep_token_id = tokenizer.token_to_id(\"[SEP]\")\n",
    "\n",
    "print(cls_token_id, sep_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "df33ed87-7918-4172-aeab-052b37ab83ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.post_processor = processors.TemplateProcessing(\n",
    "    single=f\"[CLS]:0 $A:0 [SEP]:0\",\n",
    "    pair=f\"[CLS]:0 $A:0 [SEP]:0 $B:1 [SEP]:1\",\n",
    "    special_tokens=[(\"[CLS]\", cls_token_id), (\"[SEP]\", sep_token_id)],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "eedac398-6ba9-428f-b504-4a08d3576c3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'let', \"'\", 's', 'test', 'this', 'tok', '##eni', '##zer', '.', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "encoding = tokenizer.encode(\"Let's test this tokenizer.\")\n",
    "\n",
    "print(encoding.tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "445487ba-a8e9-4a19-8165-ad99426d04cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'let', \"'\", 's', 'test', 'this', 'tok', '##eni', '##zer', '.', '[SEP]', 'again', '!', '[SEP]']\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "encoding = tokenizer.encode(\"Let's test this tokenizer.\",  \"Again!\")\n",
    "\n",
    "print(encoding.tokens)\n",
    "print(encoding.type_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "beb1ba19-6d9a-4f41-9675-51b701912830",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decoder = decoders.WordPiece(prefix=\"##\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "19f1d6dc-1281-4033-aae7-5e9b2a7c5c0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"let ' s test this tokenizer. again!\""
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(encoding.ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "44b00bf1-3a86-404a-a653-7c8a22afa78e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.save(\"tokenizer.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "eb832c61-d647-4df6-9e10-bde756df916f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 638956\n",
      "drwxr-xr-x 8 so_olliphant so_olliphant      4096 Feb 18 05:14 .\n",
      "drwxr-xr-x 3 so_olliphant so_olliphant      4096 Feb 18 02:34 ..\n",
      "drwxr-xr-x 8 so_olliphant so_olliphant      4096 Feb 18 02:28 .git\n",
      "drwxr-xr-x 2 so_olliphant so_olliphant      4096 Feb 18 05:10 .ipynb_checkpoints\n",
      "-rw-r--r-- 1 so_olliphant so_olliphant     14239 Feb 18 02:28 Ch2_p38.ipynb\n",
      "-rw-r--r-- 1 so_olliphant so_olliphant     26388 Feb 18 02:51 HF_NLP_Ch2.ipynb\n",
      "-rw-r--r-- 1 so_olliphant so_olliphant     58778 Feb 18 03:05 HF_NLP_Ch3.ipynb\n",
      "-rw-r--r-- 1 so_olliphant so_olliphant      7373 Feb 18 03:06 HF_NLP_Ch4.ipynb\n",
      "-rw-r--r-- 1 so_olliphant so_olliphant    293035 Feb 18 05:09 HF_NLP_Ch5.ipynb\n",
      "-rw-r--r-- 1 so_olliphant so_olliphant    115721 Feb 18 05:14 HF_NLP_Ch6.ipynb\n",
      "-rw-r--r-- 1 so_olliphant so_olliphant        15 Feb 18 02:28 README.md\n",
      "-rw-r--r-- 1 so_olliphant so_olliphant   8385528 Feb 18 03:06 SQuAD_it-test.json\n",
      "-rw-r--r-- 1 so_olliphant so_olliphant   1051245 Feb 18 03:06 SQuAD_it-test.json.gz\n",
      "-rw-r--r-- 1 so_olliphant so_olliphant  43605829 Feb 18 03:06 SQuAD_it-train.json\n",
      "-rw-r--r-- 1 so_olliphant so_olliphant   7725286 Feb 18 03:06 SQuAD_it-train.json.gz\n",
      "drwxr-xr-x 2 so_olliphant so_olliphant      4096 Feb 18 05:13 code-search-net-tokenizer\n",
      "drwxr-xr-x 5 so_olliphant so_olliphant      4096 Feb 18 03:26 drug-reviews\n",
      "-rw-r--r-- 1 so_olliphant so_olliphant 106288152 Feb 18 04:31 drug-reviews-test.jsonl\n",
      "-rw-r--r-- 1 so_olliphant so_olliphant 255839630 Feb 18 04:31 drug-reviews-train.jsonl\n",
      "-rw-r--r-- 1 so_olliphant so_olliphant  63913350 Feb 18 04:31 drug-reviews-validation.jsonl\n",
      "-rw-r--r-- 1 so_olliphant so_olliphant  28071166 Oct  2  2018 drugsComTest_raw.tsv\n",
      "-rw-r--r-- 1 so_olliphant so_olliphant  84289175 Oct  2  2018 drugsComTrain_raw.tsv\n",
      "-rw-r--r-- 1 so_olliphant so_olliphant  42989872 Feb 18 03:07 drugsCom_raw.zip\n",
      "-rw-r--r-- 1 so_olliphant so_olliphant     14948 Feb 18 02:28 environment.yml\n",
      "drwxr-xr-x 2 so_olliphant so_olliphant      4096 Feb 18 03:05 mydir\n",
      "drwxr-xr-x 5 so_olliphant so_olliphant      4096 Feb 18 02:55 test-trainer\n",
      "-rw-r--r-- 1 so_olliphant so_olliphant    571586 Feb 18 05:14 tokenizer.json\n",
      "-rw-r--r-- 1 so_olliphant so_olliphant  10951563 Feb 18 05:14 wikitext-2.txt\n"
     ]
    }
   ],
   "source": [
    "!ls -la"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "87c3e51f-15e8-4c79-a0c3-10623e256d0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"version\": \"1.0\",\n",
      "  \"truncation\": null,\n",
      "  \"padding\": null,\n",
      "  \"added_tokens\": [\n",
      "    {\n",
      "      \"id\": 0,\n",
      "      \"content\": \"[UNK]\",\n",
      "      \"single_word\": false,\n",
      "      \"lstrip\": false,\n"
     ]
    }
   ],
   "source": [
    "!head tokenizer.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "23615a72-90b0-494b-affe-23534e34fa8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_tokenizer = Tokenizer.from_file(\"tokenizer.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "5de3b1dc-e7cb-4d37-9bd4-eda4cac16306",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import PreTrainedTokenizerFast\n",
    "\n",
    "wrapped_tokenizer = PreTrainedTokenizerFast(\n",
    "    tokenizer_object=tokenizer,\n",
    "    # tokenizer_file=\"tokenizer.json\", # You can load from the tokenizer file, alternatively\n",
    "    unk_token=\"[UNK]\",\n",
    "    pad_token=\"[PAD]\",\n",
    "    cls_token=\"[CLS]\",\n",
    "    sep_token=\"[SEP]\",\n",
    "    mask_token=\"[MASK]\",\n",
    ")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ce7a87c4-fd06-4e61-afe4-3f638bdc911d",
   "metadata": {},
   "source": [
    "from transformers import BertTokenizerFast\n",
    "\n",
    "wrapped_tokenizer = BertTokenizerFast(tokenizer_object=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce0dfe36-d096-4ef5-8342-0e877a405677",
   "metadata": {},
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "093e97a2-9568-4723-bd21-e5587150ef44",
   "metadata": {},
   "source": [
    "##### Building a BPE tokenizer from scratch\n",
    "\n",
    "> We also <span style=\"background-color: #33ffff\">donâ€™t need to specify an `unk_token` because GPT-2 uses byte-level BPE</span>, which doesn't require it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "0d1fc35d-36a5-4a15-8e6f-476e710f44ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(models.BPE())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fdde806-7fc4-4436-9726-3dfd9083554c",
   "metadata": {},
   "source": [
    "> GPT-2 does not use a `normalizer`, so we skip that step and go directly to the pre-tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "d7390529-5284-41d9-842a-158e073c48fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "# yes, it's true... there's no normalizer set\n",
    "foo = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "print(foo.backend_tokenizer.normalizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "dfb736fb-1abd-4feb-bdcd-9cca77d4f6de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Let', (0, 3)),\n",
       " (\"'s\", (3, 5)),\n",
       " ('Ä test', (5, 10)),\n",
       " ('Ä pre', (10, 14)),\n",
       " ('-', (14, 15)),\n",
       " ('tokenization', (15, 27)),\n",
       " ('!', (27, 28))]"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel(add_prefix_space=False)\n",
    "\n",
    "tokenizer.pre_tokenizer.pre_tokenize_str(\"Let's test pre-tokenization!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "b8c39ade-c222-49d0-8eb6-08952c3079fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Let', (0, 3)),\n",
       " (\"'s\", (3, 5)),\n",
       " ('Ä test', (5, 10)),\n",
       " ('Ä pre', (10, 14)),\n",
       " ('-', (14, 15)),\n",
       " ('tokenization', (15, 27)),\n",
       " ('!', (27, 28))]"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "foo.backend_tokenizer.pre_tokenizer.pre_tokenize_str(\"Let's test pre-tokenization!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e95902da-23f0-4c48-9ec5-5635ad9edf42",
   "metadata": {},
   "source": [
    "> Next is the model, which needs training. For GPT-2, the only special token is the end-of-text token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "42a1e813-2ea4-45ea-9ed6-a4b77e7e30a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bos_token': '<|endoftext|>',\n",
       " 'eos_token': '<|endoftext|>',\n",
       " 'unk_token': '<|endoftext|>'}"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "foo.special_tokens_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "7c8b3616-a068-48ed-af49-b798f175eefc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "CPU times: user 22.6 s, sys: 35.5 ms, total: 22.6 s\n",
      "Wall time: 22.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "trainer = trainers.BpeTrainer(\n",
    "    vocab_size=25000, \n",
    "    special_tokens=[\"<|endoftext|>\"]\n",
    ")\n",
    "\n",
    "tokenizer.train_from_iterator(get_training_corpus(), trainer=trainer)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b8ea596d-c796-44b0-9437-f0e842a4c4b4",
   "metadata": {},
   "source": [
    "tokenizer.model = models.BPE()\n",
    "\n",
    "tokenizer.train([\"wikitext-2.txt\"], trainer=trainer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c87c4ad-84e2-47ad-a799-91f9fe47981d",
   "metadata": {},
   "source": [
    "...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "86853c78-5c8d-4c41-8996-f6a3054e97b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['L', 'et', \"'\", 's', 'Ä test', 'Ä this', 'Ä to', 'ken', 'izer', '.']\n"
     ]
    }
   ],
   "source": [
    "encoding = tokenizer.encode(\"Let's test this tokenizer.\")\n",
    "\n",
    "print(encoding.tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "9b600a26-2c7c-4d01-a776-1f95feee12a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Let', \"'s\", 'Ä test', 'Ä this', 'Ä token', 'izer', '.']\n"
     ]
    }
   ],
   "source": [
    "tokens = foo.tokenize(\"Let's test this tokenizer.\")\n",
    "\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "1ab7b904-3430-4ada-ad42-dc0d068b0423",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.post_processor = processors.ByteLevel(trim_offsets=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "38648e90-2808-415d-94d8-3d052e96745b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' test'"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = \"Let's test this tokenizer.\"\n",
    "encoding = tokenizer.encode(sentence)\n",
    "start, end = encoding.offsets[4]\n",
    "sentence[start:end]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "50d52627-a4fe-453f-abe5-6b55a256dcb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decoder = decoders.ByteLevel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "f5d1ecc4-d527-4514-9822-afb051af3594",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Let's test this tokenizer.\""
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(encoding.ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce938a9f-79b6-44fb-9e83-b6d660a925db",
   "metadata": {},
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "cb31ce32-db29-4eaf-979f-6169d484a5aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import PreTrainedTokenizerFast\n",
    "\n",
    "wrapped_tokenizer = PreTrainedTokenizerFast(\n",
    "    tokenizer_object=tokenizer,\n",
    "    bos_token=\"<|endoftext|>\",\n",
    "    eos_token=\"<|endoftext|>\",\n",
    ")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c369920c-d6e0-4892-ab5f-13d4326d80c6",
   "metadata": {},
   "source": [
    "from transformers import GPT2TokenizerFast\n",
    "\n",
    "wrapped_tokenizer = GPT2TokenizerFast(tokenizer_object=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "222841a9-a08c-49fe-8378-d3db837bc113",
   "metadata": {},
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0f45933-b9cc-47ad-a12a-234e90fe606a",
   "metadata": {},
   "source": [
    "##### Building a Unigram tokenizer from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c47b608e-97ac-486d-9f20-637f90a3ddbb",
   "metadata": {},
   "source": [
    "> the model starts with [`tokenizers.models.Unigram`](https://huggingface.co/docs/tokenizers/api/models#tokenizers.models.Unigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "43397854-9056-4f0d-83ac-62dc554f4c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(models.Unigram())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c08acf1-4aae-427c-b0d8-ac2f31635fb0",
   "metadata": {},
   "source": [
    "> For the normalization, XLNet uses a few replacements (which come from SentencePiece)\n",
    ">\n",
    "> This replaces â€œ and â€ with â€ and any sequence of two or more spaces with a single space, as well as removing the accents in the texts to tokenize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "f839b140-d88a-4989-b8dd-e5b4c584cd28",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Regex\n",
    "\n",
    "tokenizer.normalizer = normalizers.Sequence([\n",
    "    normalizers.Replace(\"``\", '\"'),\n",
    "    normalizers.Replace(\"''\", '\"'),\n",
    "    normalizers.NFKD(),\n",
    "    normalizers.StripAccents(),\n",
    "    normalizers.Replace(Regex(\" {2,}\"), \" \"),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c136059-c649-4fdd-8708-d892c698583b",
   "metadata": {},
   "source": [
    "> The pre-tokenizer to use for any `SentencePiece` tokenizer is [`Metaspace`](https://huggingface.co/docs/tokenizers/api/pre-tokenizers#tokenizers.pre_tokenizers.Metaspace):\n",
    ">\n",
    "> ... replacement character. Must be exactly one character. By default we use the â– (U+2581) meta symbol (Same as in SentencePiece)\n",
    ">\n",
    "> ... add(s) a space to the first word if there isn't already one. This lets us treat _hello_ exactly like _say hello_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "52749010-c63f-4cfe-8b37-1578121b4501",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.pre_tokenizer = pre_tokenizers.Metaspace()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "4e849fe2-fa07-4d72-82d3-ee9e84563893",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(\"â–Let's\", (0, 5)),\n",
       " ('â–test', (5, 10)),\n",
       " ('â–the', (10, 14)),\n",
       " ('â–pre-tokenizer!', (14, 29))]"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.pre_tokenizer.pre_tokenize_str(\"Let's test the pre-tokenizer!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "27eb2559-883c-4562-9c48-2652a869b555",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('â–hello', (0, 5))]"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.pre_tokenizer.pre_tokenize_str(\"hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "ef7336bc-4f09-47f7-9c16-1f88e8c5f626",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('â–say', (0, 3)), ('â–hello', (3, 9))]"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.pre_tokenizer.pre_tokenize_str(\"say hello\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d32b59c-6555-48fb-9317-911520f525e8",
   "metadata": {},
   "source": [
    "Special tokens for Unigram are:\n",
    "* `<cls>`\n",
    "* `<sep>`\n",
    "* `<unk>`\n",
    "* `<pad>`\n",
    "* `<mask>`\n",
    "* `<s>`\n",
    "* `</s>`\n",
    "\n",
    "We need to specify this behavior in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "0e551fd1-46f2-4285-b436-7019a8c0e278",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "special_tokens = [\n",
    "    \"<cls>\", \n",
    "    \"<sep>\", \n",
    "    \"<unk>\",\n",
    "    \"<pad>\", \n",
    "    \"<mask>\", \n",
    "    \"<s>\", \n",
    "    \"</s>\"\n",
    "]\n",
    "\n",
    "trainer = trainers.UnigramTrainer(\n",
    "    vocab_size=25000, special_tokens=special_tokens, unk_token=\"<unk>\"\n",
    ")\n",
    "\n",
    "tokenizer.train_from_iterator(get_training_corpus(), trainer=trainer)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "846eec8a-139c-48f8-b312-0554d0712c6b",
   "metadata": {},
   "source": [
    "tokenizer.model = models.Unigram()\n",
    "tokenizer.train([\"wikitext-2.txt\"], trainer=trainer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aef1b2b1-561c-4104-97d0-dba914c51ce5",
   "metadata": {},
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "f314b71e-6adc-4f26-bf37-e76f5673d186",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['â–Let', 'â€™', 's', 'â–test', 'â–this', 'â–to', 'ken', 'izer', '.']\n"
     ]
    }
   ],
   "source": [
    "encoding = tokenizer.encode(\"Letâ€™s  test this tokenizer.\")\n",
    "\n",
    "print(encoding.tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "307ac14f-9844-4c7d-94ee-a0bf5c528ca8",
   "metadata": {},
   "source": [
    "> A peculiarity of XLNet is that it puts the `<cls>` token at the end of the sentence,\n",
    "> with a type ID of 2 (to distinguish it from the other tokens). Itâ€™s padding on the left,\n",
    "> as a result.\n",
    "> We can deal with all the special tokens and token type IDs with a (post-processor) template,\n",
    "> like for BERT, but first we have to get the IDs of the `<cls>` and `<sep>` tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "3119b69d-d4e8-4c62-b4aa-b7572c1c4c52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1\n"
     ]
    }
   ],
   "source": [
    "cls_token_id = tokenizer.token_to_id(\"<cls>\")\n",
    "sep_token_id = tokenizer.token_to_id(\"<sep>\")\n",
    "\n",
    "print(cls_token_id, sep_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "9a93d7bc-d579-4a2d-9ada-76dae591ad5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.post_processor = processors.TemplateProcessing(\n",
    "    single=\"$A:0 <sep>:0 <cls>:2\",\n",
    "    pair=\"$A:0 <sep>:0 $B:1 <sep>:1 <cls>:2\",\n",
    "    special_tokens=[(\"<sep>\", sep_token_id), (\"<cls>\", cls_token_id)],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "3e88e7f3-b341-4cbd-8943-66e3b3f68cba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['â–Let', \"'\", 's', 'â–test', 'â–this', 'â–to', 'ken', 'izer', '.', '.', '.', '<sep>', 'â–', 'on', 'â–', 'a', 'â–pair', 'â–of', 'â–sentence', 's', '!', '<sep>', '<cls>']\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2]\n"
     ]
    }
   ],
   "source": [
    "encoding = tokenizer.encode(\n",
    "    \"Let's test this tokenizer...\", \n",
    "    \"on a pair of sentences!\"\n",
    ")\n",
    "\n",
    "print(encoding.tokens)\n",
    "print(encoding.type_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ce69ee1-a247-4a1a-ba31-7878e1564d0e",
   "metadata": {},
   "source": [
    "> Finally, we add a [`tokenizers.decoders.Metaspace`](https://huggingface.co/docs/tokenizers/api/decoders#tokenizers.decoders.Metaspace) decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "b065b026-8634-4005-8e52-8f920df34f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decoder = decoders.Metaspace()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0bccf96-452d-4646-b2c5-da6c38a71e53",
   "metadata": {},
   "source": [
    "> We can save the tokenizer like before, and wrap it in a `PreTrainedTokenizerFast`\n",
    "> or `XLNetTokenizerFast` if we want to use it in ðŸ¤— Transformers. One thing to note\n",
    "> when using `PreTrainedTokenizerFast` is that on top of the special tokens,\n",
    "> <span style=\"background-color:#33FFFF\">we need to tell the ðŸ¤— Transformers library to pad on the left</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "909cba13-15b2-4dec-8432-0fcd491e241f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import PreTrainedTokenizerFast\n",
    "\n",
    "wrapped_tokenizer = PreTrainedTokenizerFast(\n",
    "    tokenizer_object=tokenizer,\n",
    "    bos_token=\"<s>\",\n",
    "    eos_token=\"</s>\",\n",
    "    unk_token=\"<unk>\",\n",
    "    pad_token=\"<pad>\",\n",
    "    cls_token=\"<cls>\",\n",
    "    sep_token=\"<sep>\",\n",
    "    mask_token=\"<mask>\",\n",
    "    padding_side=\"left\",        # THIS IS KEY FOR (a SentencePiece tokenizer, wrapped in) PreTrainedTokenizerFast \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "3e1fcdc6-b79b-4ac9-84d6-ead76154870b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 638956\n",
      "drwxr-xr-x 8 so_olliphant so_olliphant      4096 Feb 18 05:14 .\n",
      "drwxr-xr-x 3 so_olliphant so_olliphant      4096 Feb 18 02:34 ..\n",
      "drwxr-xr-x 8 so_olliphant so_olliphant      4096 Feb 18 02:28 .git\n",
      "drwxr-xr-x 2 so_olliphant so_olliphant      4096 Feb 18 05:10 .ipynb_checkpoints\n",
      "-rw-r--r-- 1 so_olliphant so_olliphant     14239 Feb 18 02:28 Ch2_p38.ipynb\n",
      "-rw-r--r-- 1 so_olliphant so_olliphant     26388 Feb 18 02:51 HF_NLP_Ch2.ipynb\n",
      "-rw-r--r-- 1 so_olliphant so_olliphant     58778 Feb 18 03:05 HF_NLP_Ch3.ipynb\n",
      "-rw-r--r-- 1 so_olliphant so_olliphant      7373 Feb 18 03:06 HF_NLP_Ch4.ipynb\n",
      "-rw-r--r-- 1 so_olliphant so_olliphant    293035 Feb 18 05:09 HF_NLP_Ch5.ipynb\n",
      "-rw-r--r-- 1 so_olliphant so_olliphant    115721 Feb 18 05:14 HF_NLP_Ch6.ipynb\n",
      "-rw-r--r-- 1 so_olliphant so_olliphant        15 Feb 18 02:28 README.md\n",
      "-rw-r--r-- 1 so_olliphant so_olliphant   8385528 Feb 18 03:06 SQuAD_it-test.json\n",
      "-rw-r--r-- 1 so_olliphant so_olliphant   1051245 Feb 18 03:06 SQuAD_it-test.json.gz\n",
      "-rw-r--r-- 1 so_olliphant so_olliphant  43605829 Feb 18 03:06 SQuAD_it-train.json\n",
      "-rw-r--r-- 1 so_olliphant so_olliphant   7725286 Feb 18 03:06 SQuAD_it-train.json.gz\n",
      "drwxr-xr-x 2 so_olliphant so_olliphant      4096 Feb 18 05:13 code-search-net-tokenizer\n",
      "drwxr-xr-x 5 so_olliphant so_olliphant      4096 Feb 18 03:26 drug-reviews\n",
      "-rw-r--r-- 1 so_olliphant so_olliphant 106288152 Feb 18 04:31 drug-reviews-test.jsonl\n",
      "-rw-r--r-- 1 so_olliphant so_olliphant 255839630 Feb 18 04:31 drug-reviews-train.jsonl\n",
      "-rw-r--r-- 1 so_olliphant so_olliphant  63913350 Feb 18 04:31 drug-reviews-validation.jsonl\n",
      "-rw-r--r-- 1 so_olliphant so_olliphant  28071166 Oct  2  2018 drugsComTest_raw.tsv\n",
      "-rw-r--r-- 1 so_olliphant so_olliphant  84289175 Oct  2  2018 drugsComTrain_raw.tsv\n",
      "-rw-r--r-- 1 so_olliphant so_olliphant  42989872 Feb 18 03:07 drugsCom_raw.zip\n",
      "-rw-r--r-- 1 so_olliphant so_olliphant     14948 Feb 18 02:28 environment.yml\n",
      "drwxr-xr-x 2 so_olliphant so_olliphant      4096 Feb 18 03:05 mydir\n",
      "drwxr-xr-x 5 so_olliphant so_olliphant      4096 Feb 18 02:55 test-trainer\n",
      "-rw-r--r-- 1 so_olliphant so_olliphant    571586 Feb 18 05:14 tokenizer.json\n",
      "-rw-r--r-- 1 so_olliphant so_olliphant  10951563 Feb 18 05:14 wikitext-2.txt\n"
     ]
    }
   ],
   "source": [
    "!ls -la"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3c52ec2b-5ef9-48a5-9fed-7c9492a6f2eb",
   "metadata": {},
   "source": [
    "from transformers import XLNetTokenizerFast\n",
    "\n",
    "wrapped_tokenizer = XLNetTokenizerFast(tokenizer_object=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1e27abc-6d4d-4533-865d-5bff0b7ab883",
   "metadata": {},
   "source": [
    "### Tokenizers, check!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff91dcea-35a8-4f28-b6c0-bc2ad0a890c1",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3a4059b2-a4f9-4129-b726-45a4a1bb4296",
   "metadata": {},
   "source": [
    "1. When should you train a new tokenizer?\n",
    "\n",
    "> When your dataset is different from the one used by an existing pretrained model, and you want to pretrain a new model.<br/>\n",
    "> In this case there's no advantage to using the same tokenizer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "145486ed-10ab-486a-b695-9c8c5fae1a26",
   "metadata": {},
   "source": [
    "2. What is the advantage of using a generator of lists of texts compared to a list of lists of texts when using `train_new_from_iterator()`?\n",
    "\n",
    "> Each batch of texts will be released from memory when you iterate, and the gain will be especially visible if you use ðŸ¤— Datasets to store your texts.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2740b05c-7d81-42a2-b675-b20e2e07fa83",
   "metadata": {},
   "source": [
    "3. What are the advantages of using a â€œfastâ€ tokenizer?\n",
    "\n",
    "> It can process inputs faster than a slow tokenizer when you batch lots of inputs together.<br/>\n",
    "> ...Thanks to parallelism implemented in Rust, it will be faster on batches of inputs.\n",
    ">\n",
    "> It has some additional features allowing you to map tokens to the span of text that created them.<br/>\n",
    "> ...offset mappings. That's not the only advantage, though"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "724ae47e-b390-4d49-a03f-a1fadfbef115",
   "metadata": {},
   "source": [
    "4. How does the token-classification pipeline handle entities that span over several tokens?\n",
    "\n",
    "> There is a label for the beginning of an entity and a label for the continuation of an entity.\n",
    ">\n",
    "> In a given word, as long as the first token has the label of the entity, the whole word is considered labeled with that entity.\n",
    ">\n",
    "> When a token has the label of a given entity, any other following token with the same label is considered part of the same entity, unless it's labeled as the start of a new entity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87eb7ded-47d2-4fbf-a502-02356eea3445",
   "metadata": {},
   "source": [
    "5. How does the question-answering pipeline handle long contexts?\n",
    "\n",
    "> It splits the context into several parts (with overlap) and finds the maximum score for an answer in each part."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "002ea673-e198-4a7c-951a-010a73627b1d",
   "metadata": {},
   "source": [
    "6. What is normalization?\n",
    "\n",
    "> It's any cleanup the tokenizer performs on the texts in the initial stages."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc31060b-b9c9-416a-80a3-6f46b884d9cd",
   "metadata": {},
   "source": [
    "7. What is pre-tokenization for a subword tokenizer?\n",
    "\n",
    "It's the step before the tokenizer model is applied, to split the input into words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86910135-bec2-445c-9338-87f63f7d324b",
   "metadata": {},
   "source": [
    "8. Select the sentences that apply to the BPE model of tokenization.\n",
    "\n",
    "> BPE is a subword tokenization algorithm that starts with a small vocabulary and learns merge rules.\n",
    ">\n",
    "> BPE tokenizers learn merge rules by merging the pair of tokens that is the most frequent.\n",
    ">\n",
    "> BPE tokenizes words into subwords by splitting them into characters and then applying the merge rules."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19a9caf2-995d-4a1c-8de1-757f41705746",
   "metadata": {},
   "source": [
    "9. Select the sentences that apply to the WordPiece model of tokenization.\n",
    "\n",
    "> WordPiece is a subword tokenization algorithm that starts with a small vocabulary and learns merge rules.\n",
    ">\n",
    "> A WordPiece tokenizer learns a merge rule by merging the pair of tokens that maximizes a score that privileges frequent pairs with less frequent individual parts.\n",
    ">\n",
    "> WordPiece tokenizes words into subwords by finding the longest subword starting from the beginning that is in the vocabulary, then repeating the process for the rest of the text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b18ad397-ac2c-4a21-b61c-c066a0445697",
   "metadata": {},
   "source": [
    "10. Select the sentences that apply to the Unigram model of tokenization.\n",
    "\n",
    "> Unigram is a subword tokenization algorithm that starts with a big vocabulary and progressively removes tokens from it.\n",
    ">\n",
    "> Unigram adapts its vocabulary by minimizing a loss computed over the whole corpus.\n",
    ">\n",
    "> Unigram tokenizes words into subwords by finding the most likely segmentation into tokens, according to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f62f351c-6651-4564-8500-08c129cd1771",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
