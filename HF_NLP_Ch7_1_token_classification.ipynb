{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "74b4bae2-beb3-4746-b4e7-14f182a1e3a2",
   "metadata": {},
   "source": [
    "# NLP Course\n",
    "\n",
    "Please see the [Hugging Face NLP Course page](https://huggingface.co/learn/nlp-course/chapter0/1?fw=pt)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "061a200e-5a76-436c-be6f-61f893caa567",
   "metadata": {},
   "source": [
    "## 7. Main NLP Tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97076774-f4b7-4a26-b1b7-3997cef457e1",
   "metadata": {},
   "source": [
    "### Token classification\n",
    "\n",
    "> ...In this section, we will fine-tune a model (BERT) on a NER task...\n",
    "\n",
    "Please see [Token Classification](https://huggingface.co/learn/nlp-course/chapter7/2?fw=pt#token-classification), 7. Main NLP Tasks, in the ü§ó NLP Course."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "750e3595-51ab-4cf1-b4e3-9358f2cdd96e",
   "metadata": {},
   "source": [
    "#### Preparing the data\n",
    "\n",
    "> In this section we will use the CoNLL-2003 dataset (please see [`eriktks/conll2003`](https://huggingface.co/datasets/eriktks/conll2003), which contains news stories from Reuters.\n",
    "\n",
    "Please also see [Introduction to the CoNLL-2003 Shared Task: Language-Independent Named Entity Recognition, Sang and Meulder, 2003](https://aclanthology.org/W03-0419.pdf):\n",
    "> The shared task of CoNLL-2003 concerns language-independent named entity recognition. We will concentrate on four types of named entities: persons, locations, organizations and names of miscellaneous entities that do not belong to the previous three groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3940fe49-4610-4369-8b1f-d2b793a4c772",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "raw_datasets = load_dataset(\"conll2003\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "84fd2539-1344-4402-b945-92c516299381",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n",
       "        num_rows: 14041\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n",
       "        num_rows: 3250\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n",
       "        num_rows: 3453\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "38718c95-5fb5-4f23-ab71-95687d55fc09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['EU', 'rejects', 'German', 'call', 'to', 'boycott', 'British', 'lamb', '.']\n"
     ]
    }
   ],
   "source": [
    "print(raw_datasets[\"train\"][0][\"tokens\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "496520d5-923a-4d05-a51c-22777f3a6ee4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 0, 7, 0, 0, 0, 7, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "print(raw_datasets[\"train\"][0][\"ner_tags\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a40b1678-b04f-4db5-a443-8390ef9c7f5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence(feature=ClassLabel(names=['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC', 'B-MISC', 'I-MISC'], id=None), length=-1, id=None)\n"
     ]
    }
   ],
   "source": [
    "ner_feature = raw_datasets[\"train\"].features[\"ner_tags\"]\n",
    "print(ner_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dbe62b47-6ceb-4d18-a889-db97aec867b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC', 'B-MISC', 'I-MISC']\n"
     ]
    }
   ],
   "source": [
    "label_names = ner_feature.feature.names\n",
    "print(label_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52214d0c-ef4e-4fca-91cd-025397087e01",
   "metadata": {},
   "source": [
    "##### Examining the NER tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4730f7d6-0563-4656-b905-81b7ec34c43d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EU    rejects German call to boycott British lamb . \n",
      "B-ORG O       B-MISC O    O  O       B-MISC  O    O \n"
     ]
    }
   ],
   "source": [
    "words = raw_datasets[\"train\"][0][\"tokens\"]\n",
    "labels = raw_datasets[\"train\"][0][\"ner_tags\"]\n",
    "\n",
    "line1 = \"\"\n",
    "line2 = \"\"\n",
    "for word, label in zip(words, labels):\n",
    "    full_label = label_names[label]\n",
    "    max_length = max(len(word), len(full_label))\n",
    "    line1 += word + \" \" * (max_length - len(word) + 1)\n",
    "    line2 += full_label + \" \" * (max_length - len(full_label) + 1)\n",
    "\n",
    "print(line1)\n",
    "print(line2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e3a14708-1a4c-403a-a239-fc96ae65e561",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Germany 's representative to the European Union 's veterinary committee Werner Zwingmann said on Wednesday consumers should buy sheepmeat from countries other than Britain until the scientific advice was clearer . \n",
      "B-LOC   O  O              O  O   B-ORG    I-ORG O  O          O         B-PER  I-PER     O    O  O         O         O      O   O         O    O         O     O    B-LOC   O     O   O          O      O   O       O \n"
     ]
    }
   ],
   "source": [
    "words = raw_datasets[\"train\"][4][\"tokens\"]\n",
    "labels = raw_datasets[\"train\"][4][\"ner_tags\"]\n",
    "\n",
    "line1 = \"\"\n",
    "line2 = \"\"\n",
    "for word, label in zip(words, labels):\n",
    "    full_label = label_names[label]\n",
    "    max_length = max(len(word), len(full_label))\n",
    "    line1 += word + \" \" * (max_length - len(word) + 1)\n",
    "    line2 += full_label + \" \" * (max_length - len(full_label) + 1)\n",
    "\n",
    "print(line1)\n",
    "print(line2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd1fa5ff-bc22-4aea-b5ed-7769e531c217",
   "metadata": {},
   "source": [
    "‚úèÔ∏è Your turn! Print the same two sentences with their POS or chunking labels."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdb4f6c8-59c3-4cb9-bbf2-a603c3262ce2",
   "metadata": {},
   "source": [
    "##### Examining the POS tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ea7190dc-b6a5-46e7-a5fc-371925e5e832",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence(feature=ClassLabel(names=['\"', \"''\", '#', '$', '(', ')', ',', '.', ':', '``', 'CC', 'CD', 'DT', 'EX', 'FW', 'IN', 'JJ', 'JJR', 'JJS', 'LS', 'MD', 'NN', 'NNP', 'NNPS', 'NNS', 'NN|SYM', 'PDT', 'POS', 'PRP', 'PRP$', 'RB', 'RBR', 'RBS', 'RP', 'SYM', 'TO', 'UH', 'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ', 'WDT', 'WP', 'WP$', 'WRB'], id=None), length=-1, id=None)\n"
     ]
    }
   ],
   "source": [
    "pos_feature = raw_datasets[\"train\"].features[\"pos_tags\"]\n",
    "print(pos_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "21ff6c90-a4fa-429a-bd21-2aef16c77ed2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\"', \"''\", '#', '$', '(', ')', ',', '.', ':', '``', 'CC', 'CD', 'DT', 'EX', 'FW', 'IN', 'JJ', 'JJR', 'JJS', 'LS', 'MD', 'NN', 'NNP', 'NNPS', 'NNS', 'NN|SYM', 'PDT', 'POS', 'PRP', 'PRP$', 'RB', 'RBR', 'RBS', 'RP', 'SYM', 'TO', 'UH', 'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ', 'WDT', 'WP', 'WP$', 'WRB']\n"
     ]
    }
   ],
   "source": [
    "label_names = pos_feature.feature.names\n",
    "print(label_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3d8b4148-09a0-476c-b6f5-39dddbb7b9c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EU  rejects German call to boycott British lamb . \n",
      "NNP VBZ     JJ     NN   TO VB      JJ      NN   . \n"
     ]
    }
   ],
   "source": [
    "words = raw_datasets[\"train\"][0][\"tokens\"]\n",
    "labels = raw_datasets[\"train\"][0][\"pos_tags\"]\n",
    "\n",
    "line1 = \"\"\n",
    "line2 = \"\"\n",
    "for word, label in zip(words, labels):\n",
    "    full_label = label_names[label]\n",
    "    max_length = max(len(word), len(full_label))\n",
    "    line1 += word + \" \" * (max_length - len(word) + 1)\n",
    "    line2 += full_label + \" \" * (max_length - len(full_label) + 1)\n",
    "\n",
    "print(line1)\n",
    "print(line2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eaf1e754-3b39-4953-abb5-c387a0856094",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Germany 's  representative to the European Union 's  veterinary committee Werner Zwingmann said on Wednesday consumers should buy sheepmeat from countries other than Britain until the scientific advice was clearer . \n",
      "NNP     POS NN             TO DT  NNP      NNP   POS JJ         NN        NNP    NNP       VBD  IN NNP       NNS       MD     VB  NN        IN   NNS       JJ    IN   NNP     IN    DT  JJ         NN     VBD JJR     . \n"
     ]
    }
   ],
   "source": [
    "words = raw_datasets[\"train\"][4][\"tokens\"]\n",
    "labels = raw_datasets[\"train\"][4][\"pos_tags\"]\n",
    "\n",
    "line1 = \"\"\n",
    "line2 = \"\"\n",
    "for word, label in zip(words, labels):\n",
    "    full_label = label_names[label]\n",
    "    max_length = max(len(word), len(full_label))\n",
    "    line1 += word + \" \" * (max_length - len(word) + 1)\n",
    "    line2 += full_label + \" \" * (max_length - len(full_label) + 1)\n",
    "\n",
    "print(line1)\n",
    "print(line2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13e70482-f838-4733-be0b-980aa6372567",
   "metadata": {},
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd464185-ba3f-42fd-801e-a8db49d3c113",
   "metadata": {},
   "source": [
    "##### Examining the Chunking labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e755e920-b7a0-430f-b44d-80ac92d34b52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence(feature=ClassLabel(names=['O', 'B-ADJP', 'I-ADJP', 'B-ADVP', 'I-ADVP', 'B-CONJP', 'I-CONJP', 'B-INTJ', 'I-INTJ', 'B-LST', 'I-LST', 'B-NP', 'I-NP', 'B-PP', 'I-PP', 'B-PRT', 'I-PRT', 'B-SBAR', 'I-SBAR', 'B-UCP', 'I-UCP', 'B-VP', 'I-VP'], id=None), length=-1, id=None)\n"
     ]
    }
   ],
   "source": [
    "chunk_feature = raw_datasets[\"train\"].features[\"chunk_tags\"]\n",
    "print(chunk_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7f789831-1529-44cb-8cf9-5915bef8f19b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['O', 'B-ADJP', 'I-ADJP', 'B-ADVP', 'I-ADVP', 'B-CONJP', 'I-CONJP', 'B-INTJ', 'I-INTJ', 'B-LST', 'I-LST', 'B-NP', 'I-NP', 'B-PP', 'I-PP', 'B-PRT', 'I-PRT', 'B-SBAR', 'I-SBAR', 'B-UCP', 'I-UCP', 'B-VP', 'I-VP']\n"
     ]
    }
   ],
   "source": [
    "label_names = chunk_feature.feature.names\n",
    "print(label_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dd8aec8e-b77a-424d-baaf-c742985ca55e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EU   rejects German call to   boycott British lamb . \n",
      "B-NP B-VP    B-NP   I-NP B-VP I-VP    B-NP    I-NP O \n"
     ]
    }
   ],
   "source": [
    "words = raw_datasets[\"train\"][0][\"tokens\"]\n",
    "labels = raw_datasets[\"train\"][0][\"chunk_tags\"]\n",
    "\n",
    "line1 = \"\"\n",
    "line2 = \"\"\n",
    "for word, label in zip(words, labels):\n",
    "    full_label = label_names[label]\n",
    "    max_length = max(len(word), len(full_label))\n",
    "    line1 += word + \" \" * (max_length - len(word) + 1)\n",
    "    line2 += full_label + \" \" * (max_length - len(full_label) + 1)\n",
    "\n",
    "print(line1)\n",
    "print(line2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d19b9c3a-68e7-466b-807b-015607a14d88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Germany 's   representative to   the  European Union 's   veterinary committee Werner Zwingmann said on   Wednesday consumers should buy  sheepmeat from countries other  than Britain until  the  scientific advice was  clearer . \n",
      "B-NP    B-NP I-NP           B-PP B-NP I-NP     I-NP  B-NP I-NP       I-NP      I-NP   I-NP      B-VP B-PP B-NP      I-NP      B-VP   I-VP B-NP      B-PP B-NP      B-ADJP B-PP B-NP    B-SBAR B-NP I-NP       I-NP   B-VP B-ADJP  O \n"
     ]
    }
   ],
   "source": [
    "words = raw_datasets[\"train\"][4][\"tokens\"]\n",
    "labels = raw_datasets[\"train\"][4][\"chunk_tags\"]\n",
    "\n",
    "line1 = \"\"\n",
    "line2 = \"\"\n",
    "for word, label in zip(words, labels):\n",
    "    full_label = label_names[label]\n",
    "    max_length = max(len(word), len(full_label))\n",
    "    line1 += word + \" \" * (max_length - len(word) + 1)\n",
    "    line2 += full_label + \" \" * (max_length - len(full_label) + 1)\n",
    "\n",
    "print(line1)\n",
    "print(line2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a2c3bca-82b0-4577-8c76-bdd212e96800",
   "metadata": {},
   "source": [
    "#### Processing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3a33882f-76ad-424d-94c7-cb039d5c368a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_checkpoint = \"bert-base-cased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "eac8dd91-2388-4c92-b747-54f407c0e64f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.is_fast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e0c85888-098d-45ab-950c-f71f750861b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]',\n",
       " 'EU',\n",
       " 'rejects',\n",
       " 'German',\n",
       " 'call',\n",
       " 'to',\n",
       " 'boycott',\n",
       " 'British',\n",
       " 'la',\n",
       " '##mb',\n",
       " '.',\n",
       " '[SEP]']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = tokenizer(raw_datasets[\"train\"][0][\"tokens\"], is_split_into_words=True)\n",
    "inputs.tokens()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b78a96fe-2e6f-4c54-b918-c654a9f268fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None, 0, 1, 2, 3, 4, 5, 6, 7, 7, 8, None]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.word_ids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "540e84a8-8e61-453f-98c7-3f23414bf03a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_labels_with_tokens(labels, word_ids):\n",
    "    new_labels = []\n",
    "    current_word = None\n",
    "    for word_id in word_ids:\n",
    "        if word_id != current_word:\n",
    "            # Start of a new word!\n",
    "            current_word = word_id\n",
    "            label = -100 if word_id is None else labels[word_id]\n",
    "            new_labels.append(label)\n",
    "        elif word_id is None:\n",
    "            # Special token\n",
    "            new_labels.append(-100)\n",
    "        else:\n",
    "            # Same word as previous token\n",
    "            label = labels[word_id]\n",
    "            # If the label is B-XXX we change it to I-XXX\n",
    "            if label % 2 == 1:\n",
    "                label += 1\n",
    "            new_labels.append(label)\n",
    "\n",
    "    return new_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c07a0af0-6816-4f76-9bc9-8197759a16ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 0, 7, 0, 0, 0, 7, 0, 0]\n",
      "[-100, 3, 0, 7, 0, 0, 0, 7, 0, 0, 0, -100]\n"
     ]
    }
   ],
   "source": [
    "labels = raw_datasets[\"train\"][0][\"ner_tags\"]\n",
    "word_ids = inputs.word_ids()\n",
    "print(labels)\n",
    "print(align_labels_with_tokens(labels, word_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e755ba2d-a516-4002-84b0-4697bae72bf6",
   "metadata": {},
   "source": [
    "‚úèÔ∏è Your turn! Some researchers prefer to attribute only one label per word, and assign -100 to the other subtokens in a given word. This is to avoid long words that split into lots of subtokens contributing heavily to the loss. Change the previous function to align labels with input IDs by following this rule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "65ab408e-7650-44fc-a780-5793dbf839e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_labels_with_tokens2(labels, word_ids):\n",
    "    new_labels = []\n",
    "    current_word = None\n",
    "    for word_id in word_ids:\n",
    "        if word_id != current_word:\n",
    "            # Start of a new word!\n",
    "            current_word = word_id\n",
    "            label = -100 if word_id is None else labels[word_id]\n",
    "            #print(f\"!= current_word: {label}\")\n",
    "            new_labels.append(label)\n",
    "        elif word_id is None:\n",
    "            # Special token\n",
    "            #print(\"special token\")\n",
    "            new_labels.append(-100)\n",
    "        else:\n",
    "            # Same word as previous token\n",
    "            #label = labels[word_id]\n",
    "            # If the label is B-XXX we change it to I-XXX\n",
    "            #if label % 2 == 1:\n",
    "            #    label += 1\n",
    "            label = -100\n",
    "            #print(f\"same as previous token: {label}\")\n",
    "            new_labels.append(label)\n",
    "\n",
    "\n",
    "    return new_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5aa7557d-fca1-47d0-bc33-cacbb7d91e21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 0, 7, 0, 0, 0, 7, 0, 0]\n",
      "[-100, 3, 0, 7, 0, 0, 0, 7, 0, -100, 0, -100]\n"
     ]
    }
   ],
   "source": [
    "print(labels)\n",
    "print(align_labels_with_tokens2(labels, word_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ada445a-85e2-4458-b1f4-cfb2e9090ead",
   "metadata": {},
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5ea1a72-0736-43bb-8143-6e469abe8d1a",
   "metadata": {},
   "source": [
    "> To preprocess our whole dataset, we need to tokenize all the inputs and apply `align_labels_with_tokens()` on all the labels. To take advantage of the speed of our fast tokenizer, it‚Äôs best to tokenize lots of texts at the same time, so we‚Äôll write a function that processes a list of examples and use the `Dataset.map()` method with the option `batched=True`. The only thing that is different from our previous example is that the `word_ids()` function needs to get the index of the example we want the word IDs of when the inputs to the tokenizer are lists of texts (or in our case, list of lists of words), so we add that too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "05412440-e941-489b-ab52-c03a8f28fade",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(\n",
    "        examples[\"tokens\"], truncation=True, is_split_into_words=True\n",
    "    )\n",
    "    all_labels = examples[\"ner_tags\"]\n",
    "    new_labels = []\n",
    "    for i, labels in enumerate(all_labels):\n",
    "        word_ids = tokenized_inputs.word_ids(i)\n",
    "        new_labels.append(align_labels_with_tokens(labels, word_ids))\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = new_labels\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d00218ff-11eb-42c6-8b63-21bfdbc2a7b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_datasets = raw_datasets.map(\n",
    "    tokenize_and_align_labels,\n",
    "    batched=True,\n",
    "    remove_columns=raw_datasets[\"train\"].column_names,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e64b719-2518-4083-98ba-d7b8aee9c273",
   "metadata": {},
   "source": [
    "#### Fine-tuning the model with the Trainer API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "393a6dac-7967-4261-84ed-305f4ce48cd3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    " #### Data collation\n",
    "\n",
    "> We can‚Äôt just use a DataCollatorWithPadding like in Chapter 3 because that only pads the inputs\n",
    "> (input IDs, attention mask, and token type IDs).\n",
    "> Here our labels should be padded the exact same way as the inputs so that they stay the same size,\n",
    "> using -100 as a value so that the corresponding predictions are ignored in the loss computation.\n",
    ">\n",
    "> This is all done by a [`DataCollatorForTokenClassification`](https://huggingface.co/docs/transformers/main_classes/data_collator#transformers.DataCollatorForTokenClassification). Like the `DataCollatorWithPadding`, <span style=\"background-color:#33FFFF\">it takes the tokenizer used to preprocess the inputs</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d5a35c94-a961-4302-9e27-35fc833ff1dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForTokenClassification\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ce026dca-e52e-4b98-8353-037f1c8f4960",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-100,    3,    0,    7,    0,    0,    0,    7,    0,    0,    0, -100],\n",
       "        [-100,    1,    2, -100, -100, -100, -100, -100, -100, -100, -100, -100]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = data_collator([tokenized_datasets[\"train\"][i] for i in range(2)])\n",
    "\n",
    "batch[\"labels\"]"
   ]
  },
  {
   "cell_type": "raw",
   "id": "358eb4b5-1aec-4495-adc2-a6074857aaf4",
   "metadata": {},
   "source": [
    "raw_datasets[\"train\"][1]"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5ed5519a-750c-4af8-a469-90b516e66e94",
   "metadata": {},
   "source": [
    "tokenized_datasets[\"train\"][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3c0fc2dc-bb71-44e2-b1f5-ab64ef6407ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-100, 3, 0, 7, 0, 0, 0, 7, 0, 0, 0, -100]\n",
      "[-100, 1, 2, -100]\n"
     ]
    }
   ],
   "source": [
    "for i in range(2):\n",
    "    print(tokenized_datasets[\"train\"][i][\"labels\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cab034d-0229-4e7a-aecc-1253bf058d9c",
   "metadata": {},
   "source": [
    "#### Metrics\n",
    "\n",
    "From [Metric: sequeval](https://huggingface.co/spaces/evaluate-metric/seqeval) on ü§ó Spaces:\n",
    "\n",
    "> Seqeval produces labelling scores along with its sufficient statistics from a source against one or more references.\n",
    ">\n",
    "> It takes two mandatory arguments:\n",
    "> <br/><span style=\"padding-left:20px;\"/><tt>predictions</tt>: a list of lists of predicted labels, i.e. estimated targets as returned by a tagger.</span>\n",
    "> <br/><span style=\"padding-left:20px;\"/><tt>references</tt>: a list of lists of reference labels, i.e. the ground truth/target values.</span>\n",
    "\n",
    "\n",
    "It works like this:\n",
    " \n",
    "    seqeval = evaluate.load('seqeval')\n",
    "    predictions = [['O', 'O', 'B-MISC', 'I-MISC', 'I-MISC', 'I-MISC', 'O'], ['B-PER', 'I-PER', 'O']]\n",
    "    references = [['O', 'O', 'O', 'B-MISC', 'I-MISC', 'I-MISC', 'O'], ['B-PER', 'I-PER', 'O']]\n",
    "    results = seqeval.compute(predictions=predictions, references=references)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c33e754b-3cc8-4e2c-b37c-04c2b4d684da",
   "metadata": {},
   "source": [
    "# !pip install seqeval"
   ]
  },
  {
   "cell_type": "raw",
   "id": "dba4491c-3dcb-42ac-963d-d63b42e71589",
   "metadata": {},
   "source": [
    "!conda install seqeval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e9e81ac9-49ba-4237-9344-f4ab57d03b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"seqeval\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b4d52c54-21bd-47c9-b41f-d4848e35b6ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['B-ADVP', 'O', 'B-INTJ', 'O', 'O', 'O', 'B-INTJ', 'O', 'O']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = raw_datasets[\"train\"][0][\"ner_tags\"]\n",
    "labels = [label_names[i] for i in labels]\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7c38dce5-8c27-4377-992e-3bc86486b673",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ADVP': {'precision': 1.0, 'recall': 1.0, 'f1': 1.0, 'number': 1},\n",
       " 'INTJ': {'precision': 1.0,\n",
       "  'recall': 0.5,\n",
       "  'f1': 0.6666666666666666,\n",
       "  'number': 2},\n",
       " 'overall_precision': 1.0,\n",
       " 'overall_recall': 0.6666666666666666,\n",
       " 'overall_f1': 0.8,\n",
       " 'overall_accuracy': 0.8888888888888888}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = labels.copy()\n",
    "predictions[2] = \"O\"\n",
    "\n",
    "metric.compute(predictions=[predictions], references=[labels])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66c21721-9111-4c47-ae95-00197a4441b6",
   "metadata": {},
   "source": [
    "...\n",
    "\n",
    "> This `compute_metrics()` function first takes the argmax of the logits to convert them to predictions (as usual, the logits and the probabilities are in the same order, so we don‚Äôt need to apply the softmax). Then we have to convert both labels and predictions from integers to strings. We remove all the values where the label is `-100`, then pass the results to the `metric.compute()` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "910b5c33-bfb1-4ca8-920d-3e9887ccddbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    # unpack\n",
    "    logits, labels = eval_preds\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "\n",
    "    # Remove ignored index (special tokens) and convert to labels\n",
    "    true_labels = [\n",
    "        [label_names[l] for l in label if l != -100]\n",
    "        for label in labels\n",
    "    ]\n",
    "    \n",
    "    true_predictions = [\n",
    "        [label_names[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    \n",
    "    all_metrics = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "    \n",
    "    return {\n",
    "        \"precision\": all_metrics[\"overall_precision\"],\n",
    "        \"recall\": all_metrics[\"overall_recall\"],\n",
    "        \"f1\": all_metrics[\"overall_f1\"],\n",
    "        \"accuracy\": all_metrics[\"overall_accuracy\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5348ae63-8356-4c8f-af1a-a097ce1c6d4b",
   "metadata": {},
   "source": [
    "#### Defining the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "19415d06-16e3-45c9-8076-ead1cf54abef",
   "metadata": {},
   "outputs": [],
   "source": [
    "id2label = {i: label for i, label in enumerate(label_names)}\n",
    "label2id = {v: k for k, v in id2label.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "480bf332-aba1-4473-abba-4ae668ab3233",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForTokenClassification\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    model_checkpoint,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "22eaacc3-a842-4e63-a565-3635dd9d3bbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config.num_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "689403be-3878-4de3-b1d7-2dc93d202f59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'O',\n",
       " 1: 'B-ADJP',\n",
       " 2: 'I-ADJP',\n",
       " 3: 'B-ADVP',\n",
       " 4: 'I-ADVP',\n",
       " 5: 'B-CONJP',\n",
       " 6: 'I-CONJP',\n",
       " 7: 'B-INTJ',\n",
       " 8: 'I-INTJ',\n",
       " 9: 'B-LST',\n",
       " 10: 'I-LST',\n",
       " 11: 'B-NP',\n",
       " 12: 'I-NP',\n",
       " 13: 'B-PP',\n",
       " 14: 'I-PP',\n",
       " 15: 'B-PRT',\n",
       " 16: 'I-PRT',\n",
       " 17: 'B-SBAR',\n",
       " 18: 'I-SBAR',\n",
       " 19: 'B-UCP',\n",
       " 20: 'I-UCP',\n",
       " 21: 'B-VP',\n",
       " 22: 'I-VP'}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id2label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eb7dc4a-8168-4c1a-b1e1-6268bcc9023b",
   "metadata": {},
   "source": [
    "#### Fine-tuning the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "cf84902f-926c-4ea4-ac99-1c53a0fe3abc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b087db5f3c5640ee9b2954e43dc5a18f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5e7b1c1f-4d37-4293-8e42-4fa08eb99499",
   "metadata": {},
   "source": [
    "huggingface-cli login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ea35a834-542f-49bd-8abb-4e14c3582c0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/learn-hf/lib/python3.9/site-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "args = TrainingArguments(\n",
    "    \"bert-finetuned-ner\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    push_to_hub=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feb565a2-e0c4-438b-9caf-b40e280c734c",
   "metadata": {},
   "source": [
    "    tokenizer=tokenizer\n",
    "    \n",
    "    var/tmp/ipykernel_60487/3203677919.py:3: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e72bb99a-efcb-4649-84ad-b408d790a997",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    processing_class=tokenizer,\n",
    ")\n",
    "#trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1a998005-67ec-4f5b-85b3-aaff62b29e30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5268' max='5268' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5268/5268 04:26, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.080100</td>\n",
       "      <td>0.068105</td>\n",
       "      <td>0.911199</td>\n",
       "      <td>0.932514</td>\n",
       "      <td>0.921733</td>\n",
       "      <td>0.981692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.037100</td>\n",
       "      <td>0.064741</td>\n",
       "      <td>0.934030</td>\n",
       "      <td>0.948334</td>\n",
       "      <td>0.941127</td>\n",
       "      <td>0.986269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.020700</td>\n",
       "      <td>0.060307</td>\n",
       "      <td>0.940179</td>\n",
       "      <td>0.952205</td>\n",
       "      <td>0.946154</td>\n",
       "      <td>0.987123</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4min 1s, sys: 10.7 s, total: 4min 12s\n",
      "Wall time: 4min 37s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=5268, training_loss=0.07206919162492455, metrics={'train_runtime': 267.2737, 'train_samples_per_second': 157.603, 'train_steps_per_second': 19.71, 'total_flos': 920888121858078.0, 'train_loss': 0.07206919162492455, 'epoch': 3.0})"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "07f4cb24-2ecc-4352-9cfc-629dcf7eb631",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/buruzaemon/bert-finetuned-ner/commit/1babb502b4f2c6b5d1a0e989eb6b9ca595fd3aa4', commit_message='Training complete', commit_description='', oid='1babb502b4f2c6b5d1a0e989eb6b9ca595fd3aa4', pr_url=None, repo_url=RepoUrl('https://huggingface.co/buruzaemon/bert-finetuned-ner', endpoint='https://huggingface.co', repo_type='model', repo_id='buruzaemon/bert-finetuned-ner'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.push_to_hub(commit_message=\"Training complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b6947bc-eb85-4610-8c7a-338706a7aa96",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
