{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ef5416d0-6937-424b-886d-419e363ca3b3",
   "metadata": {},
   "source": [
    "# NLP Course\n",
    "\n",
    "Please see the [Hugging Face NLP Course page](https://huggingface.co/learn/nlp-course/chapter0/1?fw=pt)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9821a5a0-d82d-4470-94de-5e7859a8b4dc",
   "metadata": {},
   "source": [
    "## Fine-tuning a masked language model\n",
    "\n",
    "> However, there are a few cases where you’ll want to first fine-tune the language models on your data, before training a task-specific head. For example, if your dataset contains legal contracts or scientific articles, a vanilla Transformer model like BERT will typically treat the domain-specific words in your corpus as rare tokens, and the resulting performance may be less than satisfactory. By fine-tuning the language model on in-domain data you can boost the performance of many downstream tasks, which means you usually only have to do this step once!\n",
    "\n",
    "Please see [Fine-tuning a masked language model](https://huggingface.co/learn/nlp-course/chapter7/3?fw=pt#fine-tuning-a-masked-language-model), 7. Main NLP Tasks, in the 🤗 NLP Course."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb001d5b-1ce8-4346-af8d-20878f60e1c2",
   "metadata": {},
   "source": [
    "### Picking a pretrained model for masked language modeling\n",
    "\n",
    "> Although the BERT and RoBERTa family of models are the most downloaded, we’ll use a model called [DistilBERT](https://huggingface.co/distilbert/distilbert-base-uncased) that can be trained much faster with little to no loss in downstream performance. This model was trained using a special technique called [knowledge distillation](https://en.wikipedia.org/wiki/Knowledge_distillation), where a large “teacher model” like BERT is used to guide the training of a “student model” that has far fewer parameters...\n",
    "\n",
    "c.f. these search results on [https://huggingface.co/models?pipeline_tag=fill-mask&language=en&sort=trending](https://huggingface.co/models?pipeline_tag=fill-mask&language=en&sort=trending) 🤗HF. You should see [`distilbert-base-uncased`](https://huggingface.co/distilbert/distilbert-base-uncased) placed highly in the search results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "73f6ef8b-3286-461c-a70f-4b0f10522005",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForMaskedLM\n",
    "\n",
    "model_checkpoint = \"distilbert-base-uncased\"\n",
    "model = AutoModelForMaskedLM.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1db81795-3928-4ccc-afae-10d39ac33fce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'>>> DistilBERT number of parameters: 67M'\n",
      "'>>> BERT number of parameters: 110M'\n"
     ]
    }
   ],
   "source": [
    "distilbert_num_parameters = model.num_parameters() / 1_000_000\n",
    "\n",
    "print(f\"'>>> DistilBERT number of parameters: {round(distilbert_num_parameters)}M'\")\n",
    "print(f\"'>>> BERT number of parameters: 110M'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "54a90fee-d4d8-4316-9fd9-8555ec525fa4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DistilBertConfig {\n",
       "  \"_attn_implementation_autoset\": true,\n",
       "  \"_name_or_path\": \"distilbert-base-uncased\",\n",
       "  \"activation\": \"gelu\",\n",
       "  \"architectures\": [\n",
       "    \"DistilBertForMaskedLM\"\n",
       "  ],\n",
       "  \"attention_dropout\": 0.1,\n",
       "  \"dim\": 768,\n",
       "  \"dropout\": 0.1,\n",
       "  \"hidden_dim\": 3072,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"max_position_embeddings\": 512,\n",
       "  \"model_type\": \"distilbert\",\n",
       "  \"n_heads\": 12,\n",
       "  \"n_layers\": 6,\n",
       "  \"pad_token_id\": 0,\n",
       "  \"qa_dropout\": 0.1,\n",
       "  \"seq_classif_dropout\": 0.2,\n",
       "  \"sinusoidal_pos_embds\": false,\n",
       "  \"tie_weights_\": true,\n",
       "  \"torch_dtype\": \"float32\",\n",
       "  \"transformers_version\": \"4.49.0\",\n",
       "  \"vocab_size\": 30522\n",
       "}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5604c46-6186-4918-af35-2499a8951dbe",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "256a4cf7-4d17-4618-ae9f-7dc943a2ae14",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"This is a great [MASK].\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5fad3278-916c-4329-85dc-28c70e255a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18c72c40-b1bd-499f-a5a2-9ab18a88ed49",
   "metadata": {},
   "source": [
    "> With a tokenizer and a model, we can now pass our text example to the model, extract the logits, and print out the top 5 candidates:\n",
    "\n",
    "##### Let's unpack this...\n",
    "\n",
    "1. First, we tokenize the example `text` as `inputs` to our model\n",
    "2. Passing the tokenized `inputs` into our model, we obtain the corresponding logits to the tokenized `inputs`\n",
    "3. Use [`torch.where`](https://pytorch.org/docs/stable/generated/torch.where.html#torch-where) to locate the index of the token that corresponds to the `[MASK]` token (`tokenizer.mask_token_id`)\n",
    "4. Obtain the logits at the position of the `[MASK]` token\n",
    "5. Finally, use [`torch.topk`](https://pytorch.org/docs/stable/generated/torch.where.html#torch-where) to find the indices for the top 5 word/token candidates that the models suggests could replace the `[MASK]` token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "11221248-6527-4f0f-ba56-411e48695382",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'>>> This is a great deal.'\n",
      "'>>> This is a great success.'\n",
      "'>>> This is a great adventure.'\n",
      "'>>> This is a great idea.'\n",
      "'>>> This is a great feat.'\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "token_logits = model(**inputs).logits\n",
    "\n",
    "# Find the location of [MASK] and extract its logits\n",
    "mask_token_index = torch.where(inputs[\"input_ids\"] == tokenizer.mask_token_id)[1]\n",
    "mask_token_logits = token_logits[0, mask_token_index, :]\n",
    "\n",
    "# Pick the [MASK] candidates with the highest logits\n",
    "top_5_tokens = torch.topk(mask_token_logits, 5, dim=1).indices[0].tolist()\n",
    "\n",
    "for token in top_5_tokens:\n",
    "    print(f\"'>>> {text.replace(tokenizer.mask_token, tokenizer.decode([token]))}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76b51316-5a50-4d00-bc65-ddd4da4295b8",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5492858-67a1-4b2f-8c4c-d6b1f049917f",
   "metadata": {},
   "source": [
    "### The dataset\n",
    "\n",
    "> To showcase domain adaptation, we’ll use the famous [Large Movie Review Dataset](https://huggingface.co/datasets/stanfordnlp/imdb) (or IMDb for short), which is a corpus of movie reviews that is often used to benchmark sentiment analysis models. By fine-tuning DistilBERT on this corpus, we expect the language model will adapt its vocabulary from the factual data of Wikipedia that it was pretrained on to the more subjective elements of movie reviews....\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e90a8e3e-40c8-4f6c-b30c-d2ce5e01a23c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 25000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 25000\n",
       "    })\n",
       "    unsupervised: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 50000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "imdb_dataset = load_dataset(\"imdb\")\n",
    "imdb_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "08f53626-8f30-483a-8849-45b58ff9af3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "'>>> Review: There is no relation at all between Fortier and Profiler but the fact that both are police series about violent crimes. Profiler looks crispy, Fortier looks classic. Profiler plots are quite simple. Fortier's plot are far more complicated... Fortier looks more like Prime Suspect, if we have to spot similarities... The main character is weak and weirdo, but have \"clairvoyance\". People like to compare, to judge, to evaluate. How about just enjoying? Funny thing too, people writing Fortier looks American but, on the other hand, arguing they prefer American series (!!!). Maybe it's the language, or the spirit, but I think this series is more English than American. By the way, the actors are really good and funny. The acting is not superficial at all...'\n",
      "'>>> Label: 1'\n",
      "\n",
      "'>>> Review: This movie is a great. The plot is very true to the book which is a classic written by Mark Twain. The movie starts of with a scene where Hank sings a song with a bunch of kids called \"when you stub your toe on the moon\" It reminds me of Sinatra's song High Hopes, it is fun and inspirational. The Music is great throughout and my favorite song is sung by the King, Hank (bing Crosby) and Sir \"Saggy\" Sagamore. OVerall a great family movie or even a great Date movie. This is a movie you can watch over and over again. The princess played by Rhonda Fleming is gorgeous. I love this movie!! If you liked Danny Kaye in the Court Jester then you will definitely like this movie.'\n",
      "'>>> Label: 1'\n",
      "\n",
      "'>>> Review: George P. Cosmatos' \"Rambo: First Blood Part II\" is pure wish-fulfillment. The United States clearly didn't win the war in Vietnam. They caused damage to this country beyond the imaginable and this movie continues the fairy story of the oh-so innocent soldiers. The only bad guys were the leaders of the nation, who made this war happen. The character of Rambo is perfect to notice this. He is extremely patriotic, bemoans that US-Americans didn't appreciate and celebrate the achievements of the single soldier, but has nothing but distrust for leading officers and politicians. Like every film that defends the war (e.g. \"We Were Soldiers\") also this one avoids the need to give a comprehensible reason for the engagement in South Asia. And for that matter also the reason for every single US-American soldier that was there. Instead, Rambo gets to take revenge for the wounds of a whole nation. It would have been better to work on how to deal with the memories, rather than suppressing them. \"Do we get to win this time?\" Yes, you do.'\n",
      "'>>> Label: 0'\n"
     ]
    }
   ],
   "source": [
    "sample = imdb_dataset[\"train\"].shuffle(seed=42).select(range(3))\n",
    "\n",
    "for row in sample:\n",
    "    print(f\"\\n'>>> Review: {row['text']}'\")\n",
    "    print(f\"'>>> Label: {row['label']}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "098eeb0c-5d53-4066-aa06-3ea5a9b5ed87",
   "metadata": {},
   "source": [
    "> ✏️ Try it out! Create a random sample of the unsupervised split and verify that the labels are neither 0 nor 1. While you’re at it, you could also check that the labels in the train and test splits are indeed 0 or 1 — this is a useful sanity check that every NLP practitioner should perform at the start of a new project!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "247680a7-d9f2-4a07-b746-7c12b8a672f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels in 'train': [0 1]\n",
      "labels in 'test': [0 1]\n",
      "labels in 'unsupervised': [-1]\n"
     ]
    }
   ],
   "source": [
    "# why not simply check each dataset?\n",
    "check_df = imdb_dataset[\"train\"].to_pandas()\n",
    "print(f\"labels in 'train': {check_df.label.unique()}\")\n",
    "\n",
    "check_df = imdb_dataset[\"test\"].to_pandas()\n",
    "print(f\"labels in 'test': {check_df.label.unique()}\")\n",
    "\n",
    "check_df = imdb_dataset[\"unsupervised\"].to_pandas()\n",
    "print(f\"labels in 'unsupervised': {check_df.label.unique()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17ba93a2-bd3e-4996-a743-74521357efed",
   "metadata": {},
   "source": [
    "_**NOTE** that the Dataset Viewer view of the [`stanfordnlp/imdb`](https://huggingface.co/datasets/stanfordnlp/imdb) dataset at 🤗 HF will also let you explore the data easily._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d4796fc-af6a-42e7-9d59-3e864b039a48",
   "metadata": {},
   "source": [
    "### Preprocessing the data\n",
    "\n",
    "> <span style=\"background-color:#33FFFF\">For both auto-regressive and masked language modeling, a common preprocessing step is to concatenate all the examples and then split the whole corpus into chunks of equal size.</span> This is quite different from our usual approach, where we simply tokenize individual examples. <p/>Why concatenate everything together?<br/> <span style=\"background-color:#33FFFF\">The reason is that individual examples might get truncated if they’re too long, and that would result in losing information that might be useful for the language modeling task!</span>\n",
    "> </p>\n",
    "> ... we’ll first tokenize our corpus as usual, but without setting the <code>truncation=True</code> option in our tokenizer. We’ll also grab the word IDs if they are available (which they will be if we’re using a fast tokenizer, as described in Chapter 6), as we will need them later on to do whole word masking...\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "60e0fd38-ac13-4628-821a-e2066a879f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    result = tokenizer(examples[\"text\"])\n",
    "    if tokenizer.is_fast:\n",
    "        result[\"word_ids\"] = [result.word_ids(i) for i in range(len(result[\"input_ids\"]))]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b2fb53bd-f6fe-4895-aaa8-d15a27935627",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'word_ids'],\n",
       "        num_rows: 25000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'word_ids'],\n",
       "        num_rows: 25000\n",
       "    })\n",
       "    unsupervised: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'word_ids'],\n",
       "        num_rows: 50000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use batched=True to activate fast multithreading!\n",
    "tokenized_datasets = imdb_dataset.map(\n",
    "    tokenize_function, batched=True, remove_columns=[\"text\", \"label\"]\n",
    ")\n",
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9299f94-f1fd-4fed-9f19-2918df1140b1",
   "metadata": {},
   "source": [
    "> Now that we’ve tokenized our movie reviews, the next step is to group them all together and split the result into chunks. But how big should these chunks be? <br/>\n",
    "> ... a good starting point is to see what the model’s maximum context size is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "41f6520c-9b49-491e-8cfc-3ae53e6fd17c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "512"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.model_max_length"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cb3b50d-7705-4a3d-913e-2be2d836fe17",
   "metadata": {},
   "source": [
    "> ✏️ Try it out! Some Transformer models, like BigBird and Longformer, have a much longer context length than BERT and other early Transformer models. Instantiate the tokenizer for one of these checkpoints and verify that the `model_max_length` agrees with what’s quoted on its model card.\n",
    "\n",
    "Why not simply just have a look at `tokenizer_config.json` for each model?\n",
    "* [`tokenizer_config.json` for `distilbert/distilbert-base-uncased`](https://huggingface.co/distilbert/distilbert-base-uncased/blob/main/tokenizer_config.json): `model_max_length=512`\n",
    "* [`tokenizer_config.json` for `google/bigbird-roberta-base`](https://huggingface.co/google/bigbird-roberta-base/blob/main/tokenizer_config.json): `model_max_length=4096`\n",
    "* [`tokenizer_config.json` for `allenai/longformer-base-4096`](https://huggingface.co/google/bigbird-roberta-base/blob/main/tokenizer_config.json): `model_max_length=4096`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "501189ce-4a60-4393-9ec6-ac2b0cc4555f",
   "metadata": {},
   "source": [
    "##### Checking `google/bigbird-roberta-base\n",
    "\n",
    "Please see [`google/bigbird-roberta-base`](https://huggingface.co/google/bigbird-roberta-base)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4577bb1f-682f-4410-b5d4-ac2120095380",
   "metadata": {},
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "553b7e52-d100-445a-bb2d-c9300566ff39",
   "metadata": {},
   "source": [
    "!pip install tiktoken protobuf sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6162badf-888e-4fb5-ab72-20311f26633b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "google/bigbird-roberta-base, model_max_length: 4096\n"
     ]
    }
   ],
   "source": [
    "_check_tokenizer = AutoTokenizer.from_pretrained(\"google/bigbird-roberta-base\")\n",
    "\n",
    "print(f\"google/bigbird-roberta-base, model_max_length: {_check_tokenizer.model_max_length}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b8ccd85-03d1-4864-9b0c-588ecf209ca5",
   "metadata": {},
   "source": [
    "<hr width=40%/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ad78512-f795-4588-a1f8-815a7281b3d7",
   "metadata": {},
   "source": [
    "##### Checking `allenai/longformer-base-4096`\n",
    "\n",
    "Please see [`allenai/longformer-base-4096`](https://huggingface.co/allenai/longformer-base-4096)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4431a205-57cd-4029-a3ed-7b8aacb4e9a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "allenai/longformer-base-4096, model_max_length: 1000000000000000019884624838656\n",
      "wtf?!\n"
     ]
    }
   ],
   "source": [
    "_check_tokenizer = AutoTokenizer.from_pretrained(\"allenai/longformer-base-4096\")\n",
    "\n",
    "print(f\"allenai/longformer-base-4096, model_max_length: {_check_tokenizer.model_max_length}\")\n",
    "\n",
    "print(\"wtf?!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b564a20b-7c95-4f22-ba83-ee31c2f5bcfd",
   "metadata": {},
   "source": [
    "... hm...\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "636d27b1-98a8-4ae1-b997-11127c35b076",
   "metadata": {},
   "source": [
    "> ...in order to run our experiments on GPUs like those found on Google Colab, we’ll pick something a bit smaller that can fit in memory\n",
    "> <p/>\n",
    "> Note that using a small chunk size can be detrimental in real-world scenarios, so you should use a size that corresponds to the use case you will apply your model to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6aa3d1fa-c3f3-4e1a-9826-3e7c051195c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_size = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b973d6c-90c6-4be7-885c-755da2c55fb4",
   "metadata": {},
   "source": [
    "Now comes the fun part. To show how the concatenation works, let’s take a few reviews from our tokenized training set and print out the number of tokens per review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "553044fd-b828-46c9-ac88-165d0e8b4520",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'>>> Review 0 length: 363'\n",
      "'>>> Review 1 length: 304'\n",
      "'>>> Review 2 length: 133'\n"
     ]
    }
   ],
   "source": [
    "# Slicing produces a list of lists for each feature\n",
    "tokenized_samples = tokenized_datasets[\"train\"][:3]\n",
    "\n",
    "for idx, sample in enumerate(tokenized_samples[\"input_ids\"]):\n",
    "    print(f\"'>>> Review {idx} length: {len(sample)}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "317df0a8-cf76-441b-ad28-ac834010121b",
   "metadata": {},
   "source": [
    "##### Interesting use of Python `sum` to concatenate lists..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e038441f-2bd2-4dbb-91cb-1653ab812679",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'>>> Concatenated reviews length: 800'\n"
     ]
    }
   ],
   "source": [
    "concatenated_examples = {\n",
    "    k: sum(tokenized_samples[k], []) for k in tokenized_samples.keys()\n",
    "}\n",
    "total_length = len(concatenated_examples[\"input_ids\"])\n",
    "print(f\"'>>> Concatenated reviews length: {total_length}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5258b0cc-2982-4e76-b30c-733587166f45",
   "metadata": {},
   "source": [
    "##### Need to unpack this logic, too...\n",
    "\n",
    "* Iterating through the key,value pairs in `concatenated_examples` (keys are: `input_ids`, `attention_mask`, `word_ids`)\n",
    "* Create a new mapping of key `k`\n",
    "* ... to a list of the corresponding values `v`\n",
    "* ... but in slices of size `chunk_size` or less"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "faa69fba-6381-4463-8077-acbb1247be16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 32'\n"
     ]
    }
   ],
   "source": [
    "chunks = {\n",
    "    k: [v[i : i + chunk_size] for i in range(0, total_length, chunk_size)]\n",
    "    for k, v in concatenated_examples.items()\n",
    "}\n",
    "\n",
    "for chunk in chunks[\"input_ids\"]:\n",
    "    print(f\"'>>> Chunk length: {len(chunk)}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e255d6c-714d-4919-a0c9-c7565bf90a1f",
   "metadata": {},
   "source": [
    "> As you can see in this example, the last chunk will generally be smaller than the maximum chunk size. There are two main strategies for dealing with this:<br/>\n",
    "> * Drop the last chunk if it’s smaller than `chunk_size`.\n",
    "> * Pad the last chunk until its length equals `chunk_size`.\n",
    ">\n",
    "> We’ll take the first approach here, so let’s wrap all of the above logic in a single function that we can apply to our tokenized datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "eb1886ed-6014-4840-96b6-bcc5109bd59b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_texts(examples):\n",
    "    # Concatenate all texts\n",
    "    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
    "\n",
    "    # Compute length of concatenated texts\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "\n",
    "    # We drop the last chunk if it's smaller than chunk_size\n",
    "    total_length = (total_length // chunk_size) * chunk_size\n",
    "\n",
    "    # Split by chunks of max_len\n",
    "    result = {\n",
    "        k: [t[i : i + chunk_size] for i in range(0, total_length, chunk_size)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "\n",
    "    # Create a new labels column\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd978c35-1560-4e37-8999-74d62361c272",
   "metadata": {},
   "source": [
    "> Note that in the last step of `group_texts()` we create a new `labels` column which is a copy of the `input_ids` one. As we’ll see shortly, that’s because <span style=\"background-color:#33FFFF\">in masked language modeling the objective is to predict randomly masked tokens in the input batch, and by creating a `labels` column we provide the ground truth for our language model to learn from</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7245bcd8-f565-4086-9439-c64f7f5f1d37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'word_ids', 'labels'],\n",
       "        num_rows: 61291\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'word_ids', 'labels'],\n",
       "        num_rows: 59904\n",
       "    })\n",
       "    unsupervised: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'word_ids', 'labels'],\n",
       "        num_rows: 122957\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm_datasets = tokenized_datasets.map(group_texts, batched=True)\n",
    "lm_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "66fff6b8-9607-449c-ae96-4b2b33469b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = lm_datasets[\"train\"][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a2d94521-14e1-4360-a0dd-d2814ac4a036",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"as the vietnam war and race issues in the united states. in between asking politicians and ordinary denizens of stockholm about their opinions on politics, she has sex with her drama teacher, classmates, and married men. < br / > < br / > what kills me about i am curious - yellow is that 40 years ago, this was considered pornographic. really, the sex and nudity scenes are few and far between, even then it ' s not shot like some cheaply made porno. while my countrymen mind find it shocking, in reality sex and nudity are a major staple in swedish cinema. even ingmar bergman,\""
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(sample[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8f402b0a-55db-45a8-ad0f-47b1312cd902",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"as the vietnam war and race issues in the united states. in between asking politicians and ordinary denizens of stockholm about their opinions on politics, she has sex with her drama teacher, classmates, and married men. < br / > < br / > what kills me about i am curious - yellow is that 40 years ago, this was considered pornographic. really, the sex and nudity scenes are few and far between, even then it ' s not shot like some cheaply made porno. while my countrymen mind find it shocking, in reality sex and nudity are a major staple in swedish cinema. even ingmar bergman,\""
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(sample[\"labels\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7106ec78-1a29-4e22-8b29-9c63b44e26a6",
   "metadata": {},
   "source": [
    "### Fine-tuning DistilBERT with the Trainer API\n",
    "\n",
    "> Fine-tuning a masked language model is almost identical to fine-tuning a sequence classification model, like we did in Chapter 3. The only difference is that <span style=\"background-color:#33FFFF\">we need a special data collator that can randomly mask some of the tokens in each batch of texts</span>. Fortunately, 🤗 Transformers comes prepared with a dedicated [`DataCollatorForLanguageModeling`](https://huggingface.co/docs/transformers/main/main_classes/data_collator#transformers.DataCollatorForLanguageModeling) for just this task. We just have to pass it the tokenizer and an `mlm_probability` argument that specifies what fraction of the tokens to mask. We’ll pick 15%, which is the amount used for BERT and a common choice in the literature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8966cb67-bd3c-4cbe-9d1d-72d858913c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm_probability=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "494fec1d-0b19-4d88-9ba2-9d226ed07437",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "'>>> [CLS] floors rented i am curious - yellow from my video store because of all the controversy that surrounded it when it was first released in 1967. i also heard that at first it was seized by u. s. customs [MASK] it ever tried to enter this country, therefore [MASK] a fan of films considered \" controversial [MASK] i really had to see this for myself. < br / > [MASK] br / consists the [MASK] speculated centered around a young swedish drama student named lena who wants [MASK] learn everything she can about life. in particular she wants to focus her attentionimum to making some sort of documentary on what the average swede thought about certain political [MASK] such'\n",
      "\n",
      "'>>> as the vietnam war and race issues in the [MASK] states. in between asking politicians and ordinary den [MASK]ns of stockholm about their opinions on politics, she has sex with her drama teacher, classmates, and married men. < br [MASK] [MASK] < br [MASK] > [MASK] [MASK] me about i am curious - yellow is that 40 years ago, [MASK] was considered pornographic. really, the sex [MASK] nu [MASK] scenes are few and far [MASK], even then it ' s not shot like some cheaply made porno. while my countrymen mind find it shocking, in reality sex [MASK] nu [MASK] are a major staple [MASK] swedish cinema. even ingmar bergman,'\n"
     ]
    }
   ],
   "source": [
    "samples = [lm_datasets[\"train\"][i] for i in range(2)]\n",
    "for sample in samples:\n",
    "    _ = sample.pop(\"word_ids\")\n",
    "\n",
    "for chunk in data_collator(samples)[\"input_ids\"]:\n",
    "    print(f\"\\n'>>> {tokenizer.decode(chunk)}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaacf69a-f698-45c8-bf3b-ac13d095bac2",
   "metadata": {},
   "source": [
    "<hr width=40%/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d9ae51d-2b25-46e8-aa7d-4b52bebd10f6",
   "metadata": {},
   "source": [
    "> When training models for masked language modeling, one technique that can be used is to mask whole words together, not just individual tokens. This approach is called _whole word masking_. <span style=\"background-color:#33FFFF\">If we want to use whole word masking, we will need to build a data collator ourselves.</span> A data collator is just a function that takes a list of samples and converts them into a batch, so let’s do this now! We’ll use the word IDs computed earlier to make a map between word indices and the corresponding tokens, then randomly decide which words to mask and apply that mask on the inputs. Note that the labels are all `-100` except for the ones corresponding to mask words.\n",
    "\n",
    "##### NOT TRUE!\n",
    "\n",
    "Please see [`DataCollatorForWholeWordMask`](https://huggingface.co/docs/transformers/main/main_classes/data_collator#transformers.DataCollatorForWholeWordMask)\n",
    "\n",
    "But for laughs, let's first try doing things the hard way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5d2fdda0-9688-4799-8dd9-b1eea4b8ad31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import numpy as np\n",
    "\n",
    "from transformers import default_data_collator\n",
    "\n",
    "wwm_probability = 0.2\n",
    "\n",
    "\n",
    "def whole_word_masking_data_collator(features):\n",
    "    for feature in features:\n",
    "        word_ids = feature.pop(\"word_ids\")\n",
    "\n",
    "        # Create a map between words and corresponding token indices\n",
    "        mapping = collections.defaultdict(list)\n",
    "        current_word_index = -1\n",
    "        current_word = None\n",
    "        for idx, word_id in enumerate(word_ids):\n",
    "            if word_id is not None:\n",
    "                if word_id != current_word:\n",
    "                    current_word = word_id\n",
    "                    current_word_index += 1\n",
    "                mapping[current_word_index].append(idx)\n",
    "\n",
    "        # Randomly mask words\n",
    "        mask = np.random.binomial(1, wwm_probability, (len(mapping),))\n",
    "        input_ids = feature[\"input_ids\"]\n",
    "        labels = feature[\"labels\"]\n",
    "        new_labels = [-100] * len(labels)\n",
    "        for word_id in np.where(mask)[0]:\n",
    "            word_id = word_id.item()\n",
    "            for idx in mapping[word_id]:\n",
    "                new_labels[idx] = labels[idx]\n",
    "                input_ids[idx] = tokenizer.mask_token_id\n",
    "        feature[\"labels\"] = new_labels\n",
    "\n",
    "    return default_data_collator(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fa5ec079-a9c7-4cd1-9baa-a067ebe9766a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "'>>> [CLS] i rented [MASK] am curious - yellow from my video store because of all [MASK] controversy that [MASK] it when it was first released in 1967. [MASK] also heard that at first it was seized by u [MASK] [MASK]. customs if it ever tried to enter this country, therefore being a fan of films considered [MASK] [MASK] \" i really had to see this for myself. < br / [MASK] [MASK] [MASK] / > the plot is [MASK] around a young swedish drama student named lena who wants to learn everything she can [MASK] [MASK]. in particular she wants to focus her [MASK] [MASK] to making some sort of documentary on what [MASK] average swede thought about certain [MASK] issues such'\n",
      "\n",
      "'>>> as the vietnam war and [MASK] issues in the united [MASK]. [MASK] between [MASK] politicians [MASK] ordinary denizens of [MASK] about their [MASK] on politics, she has sex [MASK] her drama teacher, classmates, and married men. < br / > < br / > what kills [MASK] about i am curious - yellow [MASK] that [MASK] years ago, this was [MASK] pornographic [MASK] really, the sex and [MASK] [MASK] scenes are [MASK] and far between, [MASK] then [MASK] ' s not shot [MASK] [MASK] [MASK] [MASK] made porno [MASK] [MASK] my countrymen mind find it shocking, in reality sex and nudity are a major staple in swedish cinema. even ingmar bergman,'\n"
     ]
    }
   ],
   "source": [
    "samples = [lm_datasets[\"train\"][i] for i in range(2)]\n",
    "batch = whole_word_masking_data_collator(samples)\n",
    "\n",
    "for chunk in batch[\"input_ids\"]:\n",
    "    print(f\"\\n'>>> {tokenizer.decode(chunk)}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "461643d7-9add-4a39-b959-3c5e0bc65fc3",
   "metadata": {},
   "source": [
    "<hr width=40%/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "535af5c4-d5e5-4a50-929f-02e7cf80b049",
   "metadata": {},
   "source": [
    "##### And now using `DataCollatorForWholeWordMask`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a427b5fd-bcad-4e86-8403-6b1cf50a29d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForWholeWordMask\n",
    "\n",
    "data_collator_wholeword = DataCollatorForWholeWordMask(tokenizer=tokenizer, mlm_probability=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5322c517-499c-4f2a-b9af-ba18a60c186d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "'>>> [CLS] i rented i am curious - yellow from my [MASK] [MASK] 6000 of all the controversy that surrounded it [MASK] [MASK] was first released in 1967. i also heard karlsruhe at first it was [MASK] canadians u. s. customs if it ever tried to enter this country, [MASK] [MASK] a fan of films considered [MASK] controversial \" i really [MASK] to see this for [MASK]. < br / > < br / > the [MASK] is centered around a young swedish drama student named lena who wants to learn everything she can [MASK] life. in particular she wants to focus her attentions to making [MASK] sort of documentary on what the average swede thought about [MASK] political [MASK] such'\n",
      "\n",
      "'>>> as the vietnam [MASK] [MASK] race issues in the united states. in between asking politicians and ordinary [MASK] [MASK] [MASK] of stockholm about their opinions on politics, she has sex with her drama teacher, classmates [MASK] and married men. < br / > < [MASK] / > what kills me about i am curious - yellow is that 40 years ago, this was considered pornographic [MASK] really, [MASK] sex and nudity 1823 are few and far between [MASK] [MASK] then [MASK] ' s not shot like some cheaply made porno. while my countrymen mind find [MASK] [MASK], in reality sex and nudity are a major staple [MASK] [MASK] cinema. even ingmar bergman,'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/learn-hf/lib/python3.9/site-packages/transformers/data/data_collator.py:1189: UserWarning: DataCollatorForWholeWordMask is only suitable for BertTokenizer-like tokenizers. Please refer to the documentation for more information.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "samples = [lm_datasets[\"train\"][i] for i in range(2)]\n",
    "for sample in samples:\n",
    "    _ = sample.pop(\"word_ids\")\n",
    "\n",
    "for chunk in data_collator_wholeword(samples)[\"input_ids\"]:\n",
    "    print(f\"\\n'>>> {tokenizer.decode(chunk)}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "965b2faa-0217-4c6f-9631-45f180f51166",
   "metadata": {},
   "source": [
    "> Now that we have <strike>two</strike> _three_ data collators, the rest of the fine-tuning steps are standard. Training can take a while on Google Colab if you’re not lucky enough to score a mythical P100 GPU 😭, so we’ll first downsample the size of the training set to a few thousand examples. Don’t worry, we’ll still get a pretty decent language model! A quick way to downsample a dataset in 🤗 Datasets is via the `Dataset.train_test_split()` function that we saw in Chapter 5:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "aa2d1215-2c4b-4647-935b-d1569c1ab3b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'word_ids', 'labels'],\n",
       "        num_rows: 10000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'word_ids', 'labels'],\n",
       "        num_rows: 1000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_size = 10_000\n",
    "test_size = int(0.1 * train_size)\n",
    "\n",
    "downsampled_dataset = lm_datasets[\"train\"].train_test_split(\n",
    "    train_size=train_size, test_size=test_size, seed=42\n",
    ")\n",
    "downsampled_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "27796906-8f9b-41e0-84b3-07c2fe1d76d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81e13bae447d473cad6594e1990fbbbd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9987e0b1-61a2-4b40-91cf-da03d7a8bf89",
   "metadata": {},
   "source": [
    "#### Prep `TrainingArguments`\n",
    "\n",
    "> FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c7cf0eea-ccb3-4ae6-a542-a799703d5bc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "# Show the training loss with every epoch\n",
    "logging_steps = len(downsampled_dataset[\"train\"]) // batch_size\n",
    "\n",
    "model_name = model_checkpoint.split(\"/\")[-1]\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=f\"{model_name}-finetuned-imdb\",\n",
    "    overwrite_output_dir=True,\n",
    "    eval_strategy=\"epoch\", #evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    push_to_hub=True,\n",
    "    fp16=True,\n",
    "    logging_steps=logging_steps,\n",
    ")\n",
    "#training_args"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8f2fb6b-2064-4c6f-b7ec-74a8e48598dc",
   "metadata": {},
   "source": [
    "> Here we tweaked a few of the default options...<p/>\n",
    "> * `logging_steps` to ensure we track the training loss with each epoch<br/>\n",
    "> * `fp16=True` to enable mixed-precision training, which gives us another boost in speed<br/>\n",
    "> * `Trainer` will remove any columns that are not part of the model’s `forward()` method... if you’re using the whole word masking collator, you’ll also need to set `remove_unused_columns=False` to ensure we don’t lose the `word_ids` column during training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71ac50f0-3e02-497a-9059-c232f538e8ba",
   "metadata": {},
   "source": [
    "#### Prep `Trainer`\n",
    "\n",
    "> FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "060db0c8-65ec-4dfe-91cc-e3a4365fdf92",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=downsampled_dataset[\"train\"],\n",
    "    eval_dataset=downsampled_dataset[\"test\"],\n",
    "    data_collator=data_collator,\n",
    "    processing_class=tokenizer #tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "27dd3e8f-ad62-44c1-bf62-e28817518845",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='32' max='16' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [16/16 00:51]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Perplexity: 21.94\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "eval_results = trainer.evaluate()\n",
    "print(f\">>> Perplexity: {math.exp(eval_results['eval_loss']):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ba3b4643-2f86-4cb2-bd36-db90787ea7d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='471' max='471' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [471/471 02:33, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Model Preparation Time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.683800</td>\n",
       "      <td>2.509436</td>\n",
       "      <td>0.002300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.587800</td>\n",
       "      <td>2.450192</td>\n",
       "      <td>0.002300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.527900</td>\n",
       "      <td>2.481931</td>\n",
       "      <td>0.002300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=471, training_loss=2.599245999775621, metrics={'train_runtime': 153.4678, 'train_samples_per_second': 195.481, 'train_steps_per_second': 3.069, 'total_flos': 994208670720000.0, 'train_loss': 2.599245999775621, 'epoch': 3.0})"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b460954d-34c0-4cb5-89cf-392dd392c223",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='16' max='16' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [16/16 00:01]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Perplexity: 12.05\n"
     ]
    }
   ],
   "source": [
    "eval_results = trainer.evaluate()\n",
    "print(f\">>> Perplexity: {math.exp(eval_results['eval_loss']):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9b781fe2-72ad-4a09-89fa-c8d1a24fb3a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/buruzaemon/distilbert-base-uncased-finetuned-imdb/commit/b11d5578a9ebe9f896215fd68f0ccdac5a358955', commit_message='End of training', commit_description='', oid='b11d5578a9ebe9f896215fd68f0ccdac5a358955', pr_url=None, repo_url=RepoUrl('https://huggingface.co/buruzaemon/distilbert-base-uncased-finetuned-imdb', endpoint='https://huggingface.co', repo_type='model', repo_id='buruzaemon/distilbert-base-uncased-finetuned-imdb'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.push_to_hub()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f48204-878c-4eb2-83e8-62f5ab7336fe",
   "metadata": {},
   "source": [
    "<hr width=40%/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22dd5ea9-2119-4228-99ad-d230f7bf42c2",
   "metadata": {},
   "source": [
    "> ✏️ Your turn! Run the training above after changing the data collator to the whole word masking collator. Do you get better results?\n",
    "\n",
    "\n",
    "##### OK, how about trying that with _whole word masking_? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "89463d7d-7487-404f-9dda-6e7d0fffbcb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "training_args_wwm = TrainingArguments(\n",
    "    output_dir=f\"{model_name}-whole-word-masking-finetuned-imdb\",\n",
    "    overwrite_output_dir=True,\n",
    "    eval_strategy=\"epoch\", #evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    push_to_hub=True,\n",
    "    fp16=True,\n",
    "    logging_steps=logging_steps,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2f99adc8-0b85-4583-8d65-f428500ece7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_wwm = Trainer(\n",
    "    model=model,\n",
    "    args=training_args_wwm,\n",
    "    train_dataset=downsampled_dataset[\"train\"],\n",
    "    eval_dataset=downsampled_dataset[\"test\"],\n",
    "    data_collator=data_collator_wholeword, # this is that DataCollatorForWholeWordMask API\n",
    "    processing_class=tokenizer #tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "429bad52-e2cb-44bc-bb8a-15b4cad1a3b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/learn-hf/lib/python3.9/site-packages/transformers/data/data_collator.py:1189: UserWarning: DataCollatorForWholeWordMask is only suitable for BertTokenizer-like tokenizers. Please refer to the documentation for more information.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='32' max='16' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [16/16 00:53]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Evaluating whole-word masking Perplexity: 16.58\n"
     ]
    }
   ],
   "source": [
    "eval_results = trainer_wwm.evaluate()\n",
    "print(f\">>> Evaluating whole-word masking Perplexity: {math.exp(eval_results['eval_loss']):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d4e755f1-35bc-4942-bb78-9173a0311247",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='471' max='471' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [471/471 02:35, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Model Preparation Time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.823000</td>\n",
       "      <td>2.685290</td>\n",
       "      <td>0.002000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.769200</td>\n",
       "      <td>2.662086</td>\n",
       "      <td>0.002000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.744500</td>\n",
       "      <td>2.639881</td>\n",
       "      <td>0.002000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/learn-hf/lib/python3.9/site-packages/transformers/data/data_collator.py:1189: UserWarning: DataCollatorForWholeWordMask is only suitable for BertTokenizer-like tokenizers. Please refer to the documentation for more information.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='16' max='16' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [16/16 00:01]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>>Fine-tuned whole-word masking Perplexity: 14.47\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/buruzaemon/distilbert-base-uncased-whole-word-masking-finetuned-imdb/commit/318117427a64ca52a2ffac57b2cf6706396d8dcb', commit_message='End of training', commit_description='', oid='318117427a64ca52a2ffac57b2cf6706396d8dcb', pr_url=None, repo_url=RepoUrl('https://huggingface.co/buruzaemon/distilbert-base-uncased-whole-word-masking-finetuned-imdb', endpoint='https://huggingface.co', repo_type='model', repo_id='buruzaemon/distilbert-base-uncased-whole-word-masking-finetuned-imdb'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer_wwm.train()\n",
    "\n",
    "eval_results = trainer_wwm.evaluate()\n",
    "print(f\">>>Fine-tuned whole-word masking Perplexity: {math.exp(eval_results['eval_loss']):.2f}\")\n",
    "\n",
    "trainer_wwm.push_to_hub()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea350c88-9bb7-432f-9716-6f73dcb3a19e",
   "metadata": {},
   "source": [
    "### Fine-tuning DistilBERT with 🤗 Accelerate\n",
    "\n",
    "> ... However, we saw that `DataCollatorForLanguageModeling` also applies random masking with each evaluation, so we’ll see some fluctuations in our perplexity scores with each training run. One way to eliminate this source of randomness is to apply the masking once on the whole test set, and then use the default data collator in 🤗 Transformers to collect the batches during evaluation. To see how this works, let’s implement a simple function that applies the masking on a batch, similar to our first encounter with `DataCollatorForLanguageModeling`\n",
    "\n",
    "Recall that `data_collator` is that instance of `DataCollatorForLanguageModeling` we set up first..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b2e98228-4b2a-48cf-bc5d-f876e573eccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_random_mask(batch):\n",
    "    features = [\n",
    "        dict(zip(batch, t)) \n",
    "        for t in zip(*batch.values())\n",
    "    ]\n",
    "    \n",
    "    masked_inputs = data_collator(features)\n",
    "    \n",
    "    # Create a new \"masked\" column for each column in the dataset\n",
    "    return {\"masked_\" + k: v.numpy() for k, v in masked_inputs.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "97a2696d-8345-4949-9118-717dba453e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "downsampled_dataset = downsampled_dataset.remove_columns([\"word_ids\"])\n",
    "\n",
    "eval_dataset = downsampled_dataset[\"test\"].map(\n",
    "    insert_random_mask,\n",
    "    batched=True,\n",
    "    remove_columns=downsampled_dataset[\"test\"].column_names,\n",
    ")\n",
    "\n",
    "eval_dataset = eval_dataset.rename_columns(\n",
    "    {\n",
    "        \"masked_input_ids\": \"input_ids\",\n",
    "        \"masked_attention_mask\": \"attention_mask\",\n",
    "        \"masked_labels\": \"labels\",\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "20d971f9-c6d2-4e74-86d3-7041c90d04f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 10000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 1000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "downsampled_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8502e140-39e3-4334-b9ec-d413f239955f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 10000\n",
       "})"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "downsampled_dataset['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "83defe2f-a67a-4a79-a063-8e5ff52d0559",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from transformers import default_data_collator\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    downsampled_dataset[\"train\"],\n",
    "    shuffle=True,\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=data_collator,\n",
    ")\n",
    "\n",
    "eval_dataloader = DataLoader(\n",
    "    eval_dataset, batch_size=batch_size, collate_fn=default_data_collator\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e627b5a-a8a7-4671-abcb-07852ca58f83",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "bd041e86-d7c6-4a64-8af4-e28b70cd4972",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForMaskedLM.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "178174f7-660d-4072-a7d9-f8c9f4112f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import AdamW\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "20ad13a1-3cbb-4fe9-8696-5659325994d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from accelerate import Accelerator\n",
    "\n",
    "accelerator = Accelerator()\n",
    "\n",
    "model, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(\n",
    "    model, optimizer, train_dataloader, eval_dataloader\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "35d491f5-7947-4c34-b72b-338da372cc9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import get_scheduler\n",
    "\n",
    "num_train_epochs = 3\n",
    "num_update_steps_per_epoch = len(train_dataloader)\n",
    "num_training_steps = num_train_epochs * num_update_steps_per_epoch\n",
    "\n",
    "lr_scheduler = get_scheduler(\n",
    "    \"linear\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=num_training_steps,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "51ccb585-99a4-4ca0-b5d5-4ef34fe766c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'buruzaemon/distilbert-base-uncased-finetuned-imdb-accelerate'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from huggingface_hub import get_full_repo_name\n",
    "\n",
    "model_name = \"distilbert-base-uncased-finetuned-imdb-accelerate\"\n",
    "repo_name = get_full_repo_name(model_name)\n",
    "repo_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74b5adc3-523b-4bd4-9a63-ce9525292834",
   "metadata": {},
   "source": [
    "> FutureWarning: 'Repository' (from 'huggingface_hub.repository') is deprecated and will be removed from version '1.0'. Please prefer the http-based alternatives instead. Given its large adoption in legacy code, the complete removal is only planned on next major release.<p/>\n",
    "> For more details, please read https://huggingface.co/docs/huggingface_hub/concepts/git_vs_http."
   ]
  },
  {
   "cell_type": "raw",
   "id": "ad82a951-7424-4c4a-8f8b-b4f2df5fdaae",
   "metadata": {},
   "source": [
    "from huggingface_hub import Repository\n",
    "\n",
    "output_dir = model_name\n",
    "repo = Repository(output_dir, clone_from=repo_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bd86d90-470a-4992-8771-22c6e293d3d4",
   "metadata": {},
   "source": [
    "##### Using `huggingface_hub.HfApi`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "2737203e-a81e-4ec8-8488-54802ee4f5a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import HfApi\n",
    "\n",
    "output_dir = model_name\n",
    "\n",
    "api = HfApi()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "74dc6501-1fc9-47e0-ac17-09e2ddb095ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a6dcd3bb6a74f9f8aa45a7b7407d08e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/471 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Epoch 0: Perplexity: 11.263709812307237\n",
      ">>> Epoch 1: Perplexity: 10.754227971200459\n",
      ">>> Epoch 2: Perplexity: 10.55502901619238\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "import torch\n",
    "import math\n",
    "\n",
    "progress_bar = tqdm(range(num_training_steps))\n",
    "\n",
    "for epoch in range(num_train_epochs):\n",
    "    # Training\n",
    "    model.train()\n",
    "    for batch in train_dataloader:\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        accelerator.backward(loss)\n",
    "\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        progress_bar.update(1)\n",
    "\n",
    "    # Evaluation\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    for step, batch in enumerate(eval_dataloader):\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**batch)\n",
    "\n",
    "        loss = outputs.loss\n",
    "        losses.append(accelerator.gather(loss.repeat(batch_size)))\n",
    "\n",
    "    losses = torch.cat(losses)\n",
    "    losses = losses[: len(eval_dataset)]\n",
    "    try:\n",
    "        perplexity = math.exp(torch.mean(losses))\n",
    "    except OverflowError:\n",
    "        perplexity = float(\"inf\")\n",
    "\n",
    "    print(f\">>> Epoch {epoch}: Perplexity: {perplexity}\")\n",
    "\n",
    "    # Save and upload\n",
    "    accelerator.wait_for_everyone()\n",
    "    unwrapped_model = accelerator.unwrap_model(model)\n",
    "    unwrapped_model.save_pretrained(output_dir, save_function=accelerator.save)\n",
    "    if accelerator.is_main_process:\n",
    "        tokenizer.save_pretrained(output_dir)\n",
    "        #repo.push_to_hub(\n",
    "        #    commit_message=f\"Training in progress epoch {epoch}\", blocking=False\n",
    "        #)\n",
    "        future = api.upload_folder( # Upload in the background (non-blocking action)\n",
    "            repo_id=repo_name,\n",
    "            folder_path=output_dir,\n",
    "            run_as_future=True,\n",
    "            commit_message=f\"Training in progress epoch {epoch}\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "75c1005f-b814-40fe-9d50-e900b372e474",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/buruzaemon/distilbert-base-uncased-finetuned-imdb-accelerate/commit/71cd5d36c8d6fa5e067e47b833fa89b4cc30e8fa', commit_message='Training completed!', commit_description='', oid='71cd5d36c8d6fa5e067e47b833fa89b4cc30e8fa', pr_url=None, repo_url=RepoUrl('https://huggingface.co/buruzaemon/distilbert-base-uncased-finetuned-imdb-accelerate', endpoint='https://huggingface.co', repo_type='model', repo_id='buruzaemon/distilbert-base-uncased-finetuned-imdb-accelerate'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "api.upload_folder(\n",
    "    repo_id=repo_name,\n",
    "    folder_path=output_dir,\n",
    "    commit_message=\"Training completed!\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd005d6d-d5b2-4838-9775-09e377ec4a7a",
   "metadata": {},
   "source": [
    "### Using our fine-tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "7123a98a-6b63-483d-9139-df5acc99fc5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "mask_filler = pipeline(\n",
    "    \"fill-mask\", \n",
    "    model=repo_name\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "1715b7c7-514a-4f43-abdc-cf875c0f66e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a great [MASK].\n"
     ]
    }
   ],
   "source": [
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "7cef5dbd-a525-43fb-9bc6-5fe7dc8de790",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> this is a great film.\n",
      ">>> this is a great movie.\n",
      ">>> this is a great idea.\n",
      ">>> this is a great one.\n",
      ">>> this is a great adventure.\n"
     ]
    }
   ],
   "source": [
    "preds = mask_filler(text)\n",
    "\n",
    "for pred in preds:\n",
    "    print(f\">>> {pred['sequence']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef0f8274-82b6-4750-9d73-51006b0fba19",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
