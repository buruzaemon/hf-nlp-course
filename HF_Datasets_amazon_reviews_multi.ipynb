{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea82c489-8201-4031-96e3-7e7f811e3d25",
   "metadata": {},
   "source": [
    "# My Very First Dataset on ðŸ¤— \n",
    "\n",
    "## Background\n",
    "\n",
    "From [Preparing a multilingual corpus](https://huggingface.co/learn/nlp-course/chapter7/5?fw=pt#preparing-a-multilingual-corpus), in Summarization of 7. Main NLP Tasks from the ðŸ¤— NLP Course, there is the following code:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e26196e3-0c16-4ab8-89cc-b628b8d79c1d",
   "metadata": {},
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "spanish_dataset = load_dataset(\"amazon_reviews_multi\", \"es\")\n",
    "english_dataset = load_dataset(\"amazon_reviews_multi\", \"en\")\n",
    "english_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84f34d91-4a93-4b81-a1b6-0f8c8c70f1ba",
   "metadata": {},
   "source": [
    "... which won't work as the [`amazon_reviews_multi`](https://huggingface.co/datasets/defunct-datasets/amazon_reviews_multi) on ðŸ¤— Datasets is now defunct.\n",
    "\n",
    "In order to continue on with this tutorial, and to try my hand at preparing and sharing a dataset, I decided to clone / reproduce the Amazon Reviews Multilingual dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d6cb940-18ff-4aa8-b07c-f679467b189b",
   "metadata": {},
   "source": [
    "## Download the original (?) CSV datafiles from Kaggle\n",
    "\n",
    "From the [Amazon Reviews Multi page at Kaggle](https://www.kaggle.com/datasets/mexwell/amazon-reviews-multi), I used the Data Explorer feature on the page to obtain the train/test/validation splits.\n",
    "\n",
    "The files downloaded are:\n",
    "* `train.csv.zip`\n",
    "* `validation.csv.zip`\n",
    "* `test.csv.zip`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31cd8f15-96ed-4849-bfda-6f337b9cece7",
   "metadata": {},
   "source": [
    "## Clone the existing `defunct-datasets/amazon_reviews_multi` ðŸ¤— Dataset repo\n",
    "\n",
    "Taking a clue from the preceding code snippet, as well as the repo folder structure of related repo `mteb/amazon_reviews_multi`, we clone `defunct-datasets/amazon_reviews_multi` with `git`.\n",
    "\n",
    "This will give us access to that `amazon_reviews_multi.py` file which is needed for doing `datasets` magic."
   ]
  },
  {
   "cell_type": "raw",
   "id": "9fb2f7c8-ddd9-41f7-898b-d3cc5b188c8e",
   "metadata": {},
   "source": [
    "git clone git@hf.co:datasets/defunct-datasets/amazon_reviews_multi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40185506-d90e-4bbe-845c-aadaf9b27df0",
   "metadata": {},
   "source": [
    "Create a separate folder for each supported language."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0555597-d82c-452d-813f-63639a78cc64",
   "metadata": {},
   "source": [
    "cd amazon_reviews_multi\n",
    "\n",
    "mkdir de\n",
    "mkdir en\n",
    "mkdir es\n",
    "mkdir fr\n",
    "mkdir ja\n",
    "mkdir zh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5813cc6-9721-41f8-9203-50727266bcdb",
   "metadata": {},
   "source": [
    "## Process the CSV files and then output in a format for use with ðŸ¤— `datasets`\n",
    "\n",
    "Using the ðŸ¤— `datasets` APi, we need to carry out the following processing.\n",
    "\n",
    "1. Read in the CSV file for each split, but bring them together in a single `DatasetDict`\n",
    "2. Drop that leading column for the row index in each file.\n",
    "3. For each supported language, output a separate JSONL file for each split."
   ]
  },
  {
   "cell_type": "raw",
   "id": "8c531fb1-aaf1-48d5-abef-6fce25e7cf39",
   "metadata": {},
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# load up the train, validation, and test splits\n",
    "dataset = load_dataset(\n",
    "    \"csv\", \n",
    "    data_files={\n",
    "        \"train\": \"data/train.csv.zip\",\n",
    "        \"validation\": \"data/validation.csv.zip\",\n",
    "        \"test\":\"data/test.csv.zip\"\n",
    "})\n",
    "\n",
    "# the CSV files all have an extraneous leading column\n",
    "# for the line/row number per file, so let's remove that!\n",
    "dataset = dataset.remove_columns(\"Unnamed: 0\")\n",
    "\n",
    "languages = [\"de\", \"en\", \"es\", \"fr\", \"ja\", \"zh\"]\n",
    "splits = [\"train\", \"validation\", \"test\"]\n",
    "\n",
    "# lastly, we will output a train.jsonl, validation.jsonl, and test.jsonl\n",
    "# in a separate folder for each language in the dataset\n",
    "for split in splits:\n",
    "    for lang in languages:\n",
    "        ds = dataset[split].filter(lambda e: e['language']==lang)\n",
    "        ds.to_json(f\"{lang}/{split}.jsonl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14fdd804-8b1b-49cf-9ef4-d389c5b732c2",
   "metadata": {},
   "source": [
    "## Upload the dataset to your ðŸ¤— Dataset repo\n",
    "\n",
    "We only cloned the `amazon_reviews_multi` locally, and now that we have prepared JSONL datafiles for each split and for each supported language, we can upload all of that to your own namespace under Datasets.\n",
    "\n",
    "Making sure you are currently in the root directory for the cloned `amazon_reviews_multi` repo, execute:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "bbb79ed7-2ff8-4df5-8a4f-cfb7d6d6fc09",
   "metadata": {},
   "source": [
    "huggingface-cli login\n",
    "\n",
    "huggingface-cli upload buruzaemon/amazon_reviews_multi . . --repo-type datase"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90eb1c96-915d-4705-8224-bf02c66e717f",
   "metadata": {},
   "source": [
    "This should upload all of the JSONL files to `amazon_reviews_multi` but under your namespace on ðŸ¤—, as long as your `git-lfs` is configured correctly. \n",
    "\n",
    "Please refer to [Getting Started with Repositories](https://huggingface.co/docs/hub/repositories-getting-started)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b0f5930-4bac-40f0-914c-fa5c878aba25",
   "metadata": {},
   "source": [
    "## Update `amazon_reviews_multi.py` to allow for ðŸ¤— `datasets` magic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2178a2b1-84c1-4054-89bc-c20139f13502",
   "metadata": {},
   "source": [
    "##### line 59"
   ]
  },
  {
   "cell_type": "raw",
   "id": "63cd7635-0e74-4153-ab8f-fef5d2961b06",
   "metadata": {},
   "source": [
    "# BEFORE\n",
    "_DOWNLOAD_URL = \"https://amazon-reviews-ml.s3-us-west-2.amazonaws.com/json/{split}/dataset_{lang}_{split}.json\"\n",
    "\n",
    "# AFTER\n",
    "_DOWNLOAD_URL = \"https://huggingface.co/datasets/[your ðŸ¤— namespace]/amazon_reviews_multi/resolve/main/{lang}/{split}.jsonl\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39a5ffe8-a21f-46f3-9e4e-c6f5500a0f74",
   "metadata": {},
   "source": [
    "##### line 116"
   ]
  },
  {
   "cell_type": "raw",
   "id": "35880c37-b213-405d-8d03-26714a244ae7",
   "metadata": {},
   "source": [
    "# BEFORE\n",
    "dev_urls = [_DOWNLOAD_URL.format(split=\"dev\", lang=lang) for lang in self.config.languages]\n",
    "\n",
    "# AFTER\n",
    "dev_urls = [_DOWNLOAD_URL.format(split=\"validation\", lang=lang) for lang in self.config.languages]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b13f67a-603c-453e-8620-72195dc02374",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "# TODO\n",
    "\n",
    "* Make appropriate changes to the model card."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5953a17b-2c0e-4923-8aca-87a7a188ded5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
